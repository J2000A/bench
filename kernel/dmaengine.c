/* Generated by CIL v. 1.3.6 */
/* print_CIL_Input is true */

typedef unsigned short __kernel_mode_t;
typedef int __kernel_pid_t;
typedef unsigned int __kernel_size_t;
typedef int __kernel_ssize_t;
typedef long __kernel_time_t;
typedef long __kernel_clock_t;
typedef int __kernel_timer_t;
typedef int __kernel_clockid_t;
typedef unsigned int __kernel_uid32_t;
typedef unsigned int __kernel_gid32_t;
typedef unsigned short __u16;
typedef unsigned int __u32;
typedef signed char s8;
typedef unsigned char u8;
typedef unsigned short u16;
typedef int s32;
typedef unsigned int u32;
typedef long long s64;
typedef unsigned long long u64;
typedef u32 dma_addr_t;
typedef __u32 __kernel_dev_t;
typedef __kernel_dev_t dev_t;
typedef __kernel_mode_t mode_t;
typedef __kernel_pid_t pid_t;
typedef __kernel_timer_t timer_t;
typedef __kernel_clockid_t clockid_t;
typedef __kernel_uid32_t uid_t;
typedef __kernel_gid32_t gid_t;
typedef __kernel_size_t size_t;
typedef __kernel_ssize_t ssize_t;
typedef __kernel_time_t time_t;
typedef __kernel_clock_t clock_t;
typedef unsigned int gfp_t;
struct task_struct;
struct task_struct;
struct mm_struct;
struct mm_struct;
struct vm86_regs {
   long ebx ;
   long ecx ;
   long edx ;
   long esi ;
   long edi ;
   long ebp ;
   long eax ;
   long __null_ds ;
   long __null_es ;
   long __null_fs ;
   long __null_gs ;
   long orig_eax ;
   long eip ;
   unsigned short cs ;
   unsigned short __csh ;
   long eflags ;
   long esp ;
   unsigned short ss ;
   unsigned short __ssh ;
   unsigned short es ;
   unsigned short __esh ;
   unsigned short ds ;
   unsigned short __dsh ;
   unsigned short fs ;
   unsigned short __fsh ;
   unsigned short gs ;
   unsigned short __gsh ;
};
struct revectored_struct {
   unsigned long __map[8] ;
};
struct vm86_struct {
   struct vm86_regs regs ;
   unsigned long flags ;
   unsigned long screen_bitmap ;
   unsigned long cpu_type ;
   struct revectored_struct int_revectored ;
   struct revectored_struct int21_revectored ;
};
struct task_struct;
struct task_struct;
struct info {
   long ___orig_eip ;
   long ___ebx ;
   long ___ecx ;
   long ___edx ;
   long ___esi ;
   long ___edi ;
   long ___ebp ;
   long ___eax ;
   long ___ds ;
   long ___es ;
   long ___fs ;
   long ___orig_eax ;
   long ___eip ;
   long ___cs ;
   long ___eflags ;
   long ___esp ;
   long ___ss ;
   long ___vm86_es ;
   long ___vm86_ds ;
   long ___vm86_fs ;
   long ___vm86_gs ;
};
struct task_struct;
struct module;
struct module;
typedef __builtin_va_list __gnuc_va_list;
typedef __gnuc_va_list va_list;
struct bug_entry {
   unsigned long bug_addr ;
   char const   *file ;
   unsigned short line ;
   unsigned short flags ;
};
struct completion;
struct completion;
struct pid;
struct pid;
typedef unsigned long pgdval_t;
typedef unsigned long pgprotval_t;
typedef unsigned long phys_addr_t;
struct page;
struct __anonstruct_pgd_t_11 {
   pgdval_t pgd ;
};
typedef struct __anonstruct_pgd_t_11 pgd_t;
struct __anonstruct_pgprot_t_12 {
   pgprotval_t pgprot ;
};
typedef struct __anonstruct_pgprot_t_12 pgprot_t;
struct page;
struct mm_struct;
struct __anonstruct_cpumask_t_15 {
   unsigned long bits[((64U + 8U * sizeof(long )) - 1U) / (8U * sizeof(long ))] ;
};
typedef struct __anonstruct_cpumask_t_15 cpumask_t;
struct __anonstruct____missing_field_name_17 {
   unsigned int a ;
   unsigned int b ;
};
struct __anonstruct____missing_field_name_18 {
   u16 limit0 ;
   u16 base0 ;
   unsigned int base1 : 8 ;
   unsigned int type : 4 ;
   unsigned int s : 1 ;
   unsigned int dpl : 2 ;
   unsigned int p : 1 ;
   unsigned int limit : 4 ;
   unsigned int avl : 1 ;
   unsigned int l : 1 ;
   unsigned int d : 1 ;
   unsigned int g : 1 ;
   unsigned int base2 : 8 ;
};
union __anonunion____missing_field_name_16 {
   struct __anonstruct____missing_field_name_17 __annonCompField1 ;
   struct __anonstruct____missing_field_name_18 __annonCompField2 ;
};
struct desc_struct {
   union __anonunion____missing_field_name_16 __annonCompField3 ;
} __attribute__((__packed__)) ;
struct page;
struct thread_struct;
struct thread_struct;
struct mm_struct;
struct desc_struct;
struct raw_spinlock;
struct raw_spinlock;
struct task_struct;
struct i387_fsave_struct {
   u32 cwd ;
   u32 swd ;
   u32 twd ;
   u32 fip ;
   u32 fcs ;
   u32 foo ;
   u32 fos ;
   u32 st_space[20] ;
   u32 status ;
};
struct __anonstruct____missing_field_name_24 {
   u64 rip ;
   u64 rdp ;
};
struct __anonstruct____missing_field_name_25 {
   u32 fip ;
   u32 fcs ;
   u32 foo ;
   u32 fos ;
};
union __anonunion____missing_field_name_23 {
   struct __anonstruct____missing_field_name_24 __annonCompField4 ;
   struct __anonstruct____missing_field_name_25 __annonCompField5 ;
};
struct i387_fxsave_struct {
   u16 cwd ;
   u16 swd ;
   u16 twd ;
   u16 fop ;
   union __anonunion____missing_field_name_23 __annonCompField6 ;
   u32 mxcsr ;
   u32 mxcsr_mask ;
   u32 st_space[32] ;
   u32 xmm_space[64] ;
   u32 padding[24] ;
} __attribute__((__aligned__(16))) ;
struct i387_soft_struct {
   u32 cwd ;
   u32 swd ;
   u32 twd ;
   u32 fip ;
   u32 fcs ;
   u32 foo ;
   u32 fos ;
   u32 st_space[20] ;
   u8 ftop ;
   u8 changed ;
   u8 lookahead ;
   u8 no_update ;
   u8 rm ;
   u8 alimit ;
   struct info *info ;
   u32 entry_eip ;
};
union thread_xstate {
   struct i387_fsave_struct fsave ;
   struct i387_fxsave_struct fxsave ;
   struct i387_soft_struct soft ;
};
struct kmem_cache;
struct thread_struct {
   struct desc_struct tls_array[3] ;
   unsigned long sp0 ;
   unsigned long sp ;
   unsigned long sysenter_cs ;
   unsigned long ip ;
   unsigned long fs ;
   unsigned long gs ;
   unsigned long debugreg0 ;
   unsigned long debugreg1 ;
   unsigned long debugreg2 ;
   unsigned long debugreg3 ;
   unsigned long debugreg6 ;
   unsigned long debugreg7 ;
   unsigned long cr2 ;
   unsigned long trap_no ;
   unsigned long error_code ;
   union thread_xstate *xstate ;
   struct vm86_struct *vm86_info ;
   unsigned long screen_bitmap ;
   unsigned long v86flags ;
   unsigned long v86mask ;
   unsigned long saved_sp0 ;
   unsigned int saved_fs ;
   unsigned int saved_gs ;
   unsigned long *io_bitmap_ptr ;
   unsigned long iopl ;
   unsigned int io_bitmap_max ;
   unsigned long debugctlmsr ;
   unsigned long ds_area_msr ;
};
struct list_head {
   struct list_head *next ;
   struct list_head *prev ;
};
struct hlist_node;
struct hlist_head {
   struct hlist_node *first ;
};
struct hlist_node {
   struct hlist_node *next ;
   struct hlist_node **pprev ;
};
struct timespec;
struct timespec;
struct task_struct;
struct raw_spinlock {
   unsigned int slock ;
};
typedef struct raw_spinlock raw_spinlock_t;
struct __anonstruct_raw_rwlock_t_31 {
   unsigned int lock ;
};
typedef struct __anonstruct_raw_rwlock_t_31 raw_rwlock_t;
struct task_struct;
struct lock_class_key {

};
struct __anonstruct_spinlock_t_32 {
   raw_spinlock_t raw_lock ;
};
typedef struct __anonstruct_spinlock_t_32 spinlock_t;
struct __anonstruct_rwlock_t_33 {
   raw_rwlock_t raw_lock ;
};
typedef struct __anonstruct_rwlock_t_33 rwlock_t;
struct __anonstruct_atomic_t_34 {
   int counter ;
};
typedef struct __anonstruct_atomic_t_34 atomic_t;
typedef atomic_t atomic_long_t;
struct timespec {
   time_t tv_sec ;
   long tv_nsec ;
};
struct __wait_queue_head {
   spinlock_t lock ;
   struct list_head task_list ;
};
typedef struct __wait_queue_head wait_queue_head_t;
struct task_struct;
struct __anonstruct_nodemask_t_36 {
   unsigned long bits[((1U + 8U * sizeof(long )) - 1U) / (8U * sizeof(long ))] ;
};
typedef struct __anonstruct_nodemask_t_36 nodemask_t;
struct page;
struct mutex {
   atomic_t count ;
   spinlock_t wait_lock ;
   struct list_head wait_list ;
};
struct rw_semaphore;
struct rw_semaphore;
struct rw_semaphore {
   long count ;
   spinlock_t wait_lock ;
   struct list_head wait_list ;
};
struct page;
struct file;
struct file;
struct device;
struct device;
struct pm_message {
   int event ;
};
typedef struct pm_message pm_message_t;
struct pm_ops {
   int (*prepare)(struct device *dev ) ;
   void (*complete)(struct device *dev ) ;
   int (*suspend)(struct device *dev ) ;
   int (*resume)(struct device *dev ) ;
   int (*freeze)(struct device *dev ) ;
   int (*thaw)(struct device *dev ) ;
   int (*poweroff)(struct device *dev ) ;
   int (*restore)(struct device *dev ) ;
};
struct pm_ext_ops {
   struct pm_ops base ;
   int (*suspend_noirq)(struct device *dev ) ;
   int (*resume_noirq)(struct device *dev ) ;
   int (*freeze_noirq)(struct device *dev ) ;
   int (*thaw_noirq)(struct device *dev ) ;
   int (*poweroff_noirq)(struct device *dev ) ;
   int (*restore_noirq)(struct device *dev ) ;
};
enum dpm_state {
    DPM_INVALID = 0,
    DPM_ON = 1,
    DPM_PREPARING = 2,
    DPM_RESUMING = 3,
    DPM_SUSPENDING = 4,
    DPM_OFF = 5,
    DPM_OFF_IRQ = 6
} ;
struct dev_pm_info {
   pm_message_t power_state ;
   unsigned int can_wakeup : 1 ;
   unsigned int should_wakeup : 1 ;
   enum dpm_state status ;
   struct list_head entry ;
};
struct __anonstruct_mm_context_t_38 {
   void *ldt ;
   int size ;
   struct mutex lock ;
   void *vdso ;
};
typedef struct __anonstruct_mm_context_t_38 mm_context_t;
struct vm_area_struct;
struct vm_area_struct;
struct key;
struct key;
struct file;
struct file;
struct task_struct;
struct file;
typedef __u32 Elf32_Addr;
typedef __u16 Elf32_Half;
typedef __u32 Elf32_Word;
struct elf32_sym {
   Elf32_Word st_name ;
   Elf32_Addr st_value ;
   Elf32_Word st_size ;
   unsigned char st_info ;
   unsigned char st_other ;
   Elf32_Half st_shndx ;
};
typedef struct elf32_sym Elf32_Sym;
struct kobject;
struct kobject;
struct module;
struct attribute {
   char const   *name ;
   struct module *owner ;
   mode_t mode ;
};
struct attribute_group {
   char const   *name ;
   mode_t (*is_visible)(struct kobject * , struct attribute * , int  ) ;
   struct attribute **attrs ;
};
struct vm_area_struct;
struct sysfs_ops {
   ssize_t (*show)(struct kobject * , struct attribute * , char * ) ;
   ssize_t (*store)(struct kobject * , struct attribute * , char const   * , size_t  ) ;
};
struct kref {
   atomic_t refcount ;
};
struct kset;
struct kobj_type;
struct sysfs_dirent;
struct kobject {
   char const   *name ;
   struct list_head entry ;
   struct kobject *parent ;
   struct kset *kset ;
   struct kobj_type *ktype ;
   struct sysfs_dirent *sd ;
   struct kref kref ;
   unsigned int state_initialized : 1 ;
   unsigned int state_in_sysfs : 1 ;
   unsigned int state_add_uevent_sent : 1 ;
   unsigned int state_remove_uevent_sent : 1 ;
};
struct kobj_type {
   void (*release)(struct kobject *kobj ) ;
   struct sysfs_ops *sysfs_ops ;
   struct attribute **default_attrs ;
};
struct kobj_uevent_env {
   char *envp[32] ;
   int envp_idx ;
   char buf[2048] ;
   int buflen ;
};
struct kset_uevent_ops {
   int (*filter)(struct kset *kset , struct kobject *kobj ) ;
   char const   *(*name)(struct kset *kset , struct kobject *kobj ) ;
   int (*uevent)(struct kset *kset , struct kobject *kobj , struct kobj_uevent_env *env ) ;
};
struct kset {
   struct list_head list ;
   spinlock_t list_lock ;
   struct kobject kobj ;
   struct kset_uevent_ops *uevent_ops ;
};
struct module;
struct module;
struct marker;
struct marker;
typedef void marker_probe_func(void *probe_private , void *call_private , char const   *fmt ,
                               va_list *args );
struct marker_probe_closure {
   marker_probe_func *func ;
   void *probe_private ;
};
struct marker {
   char const   *name ;
   char const   *format ;
   char state ;
   char ptype ;
   void (*call)(struct marker  const  *mdata , void *call_private  , ...) ;
   struct marker_probe_closure single ;
   struct marker_probe_closure *multi ;
} __attribute__((__aligned__(8))) ;
union ktime {
   s64 tv64 ;
};
typedef union ktime ktime_t;
struct tvec_base;
struct tvec_base;
struct timer_list {
   struct list_head entry ;
   unsigned long expires ;
   void (*function)(unsigned long  ) ;
   unsigned long data ;
   struct tvec_base *base ;
   void *start_site ;
   char start_comm[16] ;
   int start_pid ;
};
struct hrtimer;
struct hrtimer;
enum hrtimer_restart;
struct work_struct;
struct work_struct;
struct work_struct {
   atomic_long_t data ;
   struct list_head entry ;
   void (*func)(struct work_struct *work ) ;
};
struct delayed_work {
   struct work_struct work ;
   struct timer_list timer ;
};
struct kmem_cache_cpu {
   void **freelist ;
   struct page *page ;
   int node ;
   unsigned int offset ;
   unsigned int objsize ;
};
struct kmem_cache_node {
   spinlock_t list_lock ;
   unsigned long nr_partial ;
   unsigned long min_partial ;
   struct list_head partial ;
   atomic_long_t nr_slabs ;
   atomic_long_t total_objects ;
   struct list_head full ;
};
struct kmem_cache_order_objects {
   unsigned long x ;
};
struct kmem_cache {
   unsigned long flags ;
   int size ;
   int objsize ;
   int offset ;
   struct kmem_cache_order_objects oo ;
   struct kmem_cache_node local_node ;
   struct kmem_cache_order_objects max ;
   struct kmem_cache_order_objects min ;
   gfp_t allocflags ;
   int refcount ;
   void (*ctor)(void * ) ;
   int inuse ;
   int align ;
   char const   *name ;
   struct list_head list ;
   struct kobject kobj ;
   struct kmem_cache_cpu *cpu_slab[64] ;
};
struct percpu_data {
   void *ptrs[1] ;
};
struct __anonstruct_local_t_98 {
   atomic_long_t a ;
};
typedef struct __anonstruct_local_t_98 local_t;
struct mod_arch_specific {

};
struct kernel_symbol {
   unsigned long value ;
   char const   *name ;
};
struct module;
struct module_attribute {
   struct attribute attr ;
   ssize_t (*show)(struct module_attribute * , struct module * , char * ) ;
   ssize_t (*store)(struct module_attribute * , struct module * , char const   * ,
                    size_t count ) ;
   void (*setup)(struct module * , char const   * ) ;
   int (*test)(struct module * ) ;
   void (*free)(struct module * ) ;
};
struct module_kobject {
   struct kobject kobj ;
   struct module *mod ;
   struct kobject *drivers_dir ;
};
struct exception_table_entry;
struct exception_table_entry;
struct module_ref {
   local_t count ;
} __attribute__((__aligned__((1) <<  (7) ))) ;
enum module_state {
    MODULE_STATE_LIVE = 0,
    MODULE_STATE_COMING = 1,
    MODULE_STATE_GOING = 2
} ;
struct module_param_attrs;
struct module_sect_attrs;
struct module_notes_attrs;
struct module {
   enum module_state state ;
   struct list_head list ;
   char name[64U - sizeof(unsigned long )] ;
   struct module_kobject mkobj ;
   struct module_param_attrs *param_attrs ;
   struct module_attribute *modinfo_attrs ;
   char const   *version ;
   char const   *srcversion ;
   struct kobject *holders_dir ;
   struct kernel_symbol  const  *syms ;
   unsigned long const   *crcs ;
   unsigned int num_syms ;
   unsigned int num_gpl_syms ;
   struct kernel_symbol  const  *gpl_syms ;
   unsigned long const   *gpl_crcs ;
   struct kernel_symbol  const  *unused_syms ;
   unsigned long const   *unused_crcs ;
   unsigned int num_unused_syms ;
   unsigned int num_unused_gpl_syms ;
   struct kernel_symbol  const  *unused_gpl_syms ;
   unsigned long const   *unused_gpl_crcs ;
   struct kernel_symbol  const  *gpl_future_syms ;
   unsigned long const   *gpl_future_crcs ;
   unsigned int num_gpl_future_syms ;
   unsigned int num_exentries ;
   struct exception_table_entry  const  *extable ;
   int (*init)(void) ;
   void *module_init ;
   void *module_core ;
   unsigned int init_size ;
   unsigned int core_size ;
   unsigned int init_text_size ;
   unsigned int core_text_size ;
   void *unwind_info ;
   struct mod_arch_specific arch ;
   unsigned int taints ;
   unsigned int num_bugs ;
   struct list_head bug_list ;
   struct bug_entry *bug_table ;
   Elf32_Sym *symtab ;
   unsigned int num_symtab ;
   char *strtab ;
   struct module_sect_attrs *sect_attrs ;
   struct module_notes_attrs *notes_attrs ;
   void *percpu ;
   char *args ;
   struct marker *markers ;
   unsigned int num_markers ;
   struct list_head modules_which_use_me ;
   struct task_struct *waiter ;
   void (*exit)(void) ;
   struct module_ref ref[64] ;
};
struct device_driver;
struct device_driver;
struct module;
struct rb_node {
   unsigned long rb_parent_color ;
   struct rb_node *rb_right ;
   struct rb_node *rb_left ;
} __attribute__((__aligned__(sizeof(long )))) ;
struct rb_root {
   struct rb_node *rb_node ;
};
struct prio_tree_node;
struct raw_prio_tree_node {
   struct prio_tree_node *left ;
   struct prio_tree_node *right ;
   struct prio_tree_node *parent ;
};
struct prio_tree_node {
   struct prio_tree_node *left ;
   struct prio_tree_node *right ;
   struct prio_tree_node *parent ;
   unsigned long start ;
   unsigned long last ;
};
struct task_struct;
struct task_struct;
struct completion {
   unsigned int done ;
   wait_queue_head_t wait ;
};
struct address_space;
struct address_space;
typedef atomic_long_t mm_counter_t;
struct __anonstruct____missing_field_name_100 {
   u16 inuse ;
   u16 objects ;
};
union __anonunion____missing_field_name_99 {
   atomic_t _mapcount ;
   struct __anonstruct____missing_field_name_100 __annonCompField10 ;
};
struct __anonstruct____missing_field_name_102 {
   unsigned long private ;
   struct address_space *mapping ;
};
union __anonunion____missing_field_name_101 {
   struct __anonstruct____missing_field_name_102 __annonCompField12 ;
   spinlock_t ptl ;
   struct kmem_cache *slab ;
   struct page *first_page ;
};
union __anonunion____missing_field_name_103 {
   unsigned long index ;
   void *freelist ;
};
struct page {
   unsigned long flags ;
   atomic_t _count ;
   union __anonunion____missing_field_name_99 __annonCompField11 ;
   union __anonunion____missing_field_name_101 __annonCompField13 ;
   union __anonunion____missing_field_name_103 __annonCompField14 ;
   struct list_head lru ;
   unsigned long page_cgroup ;
};
struct __anonstruct_vm_set_105 {
   struct list_head list ;
   void *parent ;
   struct vm_area_struct *head ;
};
union __anonunion_shared_104 {
   struct __anonstruct_vm_set_105 vm_set ;
   struct raw_prio_tree_node prio_tree_node ;
};
struct anon_vma;
struct vm_operations_struct;
struct vm_area_struct {
   struct mm_struct *vm_mm ;
   unsigned long vm_start ;
   unsigned long vm_end ;
   struct vm_area_struct *vm_next ;
   pgprot_t vm_page_prot ;
   unsigned long vm_flags ;
   struct rb_node vm_rb ;
   union __anonunion_shared_104 shared ;
   struct list_head anon_vma_node ;
   struct anon_vma *anon_vma ;
   struct vm_operations_struct *vm_ops ;
   unsigned long vm_pgoff ;
   struct file *vm_file ;
   void *vm_private_data ;
   unsigned long vm_truncate_count ;
};
struct core_thread {
   struct task_struct *task ;
   struct core_thread *next ;
};
struct core_state {
   atomic_t nr_threads ;
   struct core_thread dumper ;
   struct completion startup ;
};
struct kioctx;
struct mmu_notifier_mm;
struct mm_struct {
   struct vm_area_struct *mmap ;
   struct rb_root mm_rb ;
   struct vm_area_struct *mmap_cache ;
   unsigned long (*get_unmapped_area)(struct file *filp , unsigned long addr , unsigned long len ,
                                      unsigned long pgoff , unsigned long flags ) ;
   void (*unmap_area)(struct mm_struct *mm , unsigned long addr ) ;
   unsigned long mmap_base ;
   unsigned long task_size ;
   unsigned long cached_hole_size ;
   unsigned long free_area_cache ;
   pgd_t *pgd ;
   atomic_t mm_users ;
   atomic_t mm_count ;
   int map_count ;
   struct rw_semaphore mmap_sem ;
   spinlock_t page_table_lock ;
   struct list_head mmlist ;
   mm_counter_t _file_rss ;
   mm_counter_t _anon_rss ;
   unsigned long hiwater_rss ;
   unsigned long hiwater_vm ;
   unsigned long total_vm ;
   unsigned long locked_vm ;
   unsigned long shared_vm ;
   unsigned long exec_vm ;
   unsigned long stack_vm ;
   unsigned long reserved_vm ;
   unsigned long def_flags ;
   unsigned long nr_ptes ;
   unsigned long start_code ;
   unsigned long end_code ;
   unsigned long start_data ;
   unsigned long end_data ;
   unsigned long start_brk ;
   unsigned long brk ;
   unsigned long start_stack ;
   unsigned long arg_start ;
   unsigned long arg_end ;
   unsigned long env_start ;
   unsigned long env_end ;
   unsigned long saved_auxv[40] ;
   cpumask_t cpu_vm_mask ;
   mm_context_t context ;
   unsigned int faultstamp ;
   unsigned int token_priority ;
   unsigned int last_interval ;
   unsigned long flags ;
   struct core_state *core_state ;
   rwlock_t ioctx_list_lock ;
   struct kioctx *ioctx_list ;
   struct task_struct *owner ;
   struct file *exe_file ;
   unsigned long num_exe_file_vmas ;
   struct mmu_notifier_mm *mmu_notifier_mm ;
};
struct anon_vma;
struct user_struct;
struct user_struct;
struct file;
struct mm_struct;
struct vm_area_struct;
struct vm_area_struct;
struct vm_fault {
   unsigned int flags ;
   unsigned long pgoff ;
   void *virtual_address ;
   struct page *page ;
};
struct vm_operations_struct {
   void (*open)(struct vm_area_struct *area ) ;
   void (*close)(struct vm_area_struct *area ) ;
   int (*fault)(struct vm_area_struct *vma , struct vm_fault *vmf ) ;
   int (*page_mkwrite)(struct vm_area_struct *vma , struct page *page ) ;
   int (*access)(struct vm_area_struct *vma , unsigned long addr , void *buf , int len ,
                 int write ) ;
};
struct page;
struct device;
struct klist_node;
struct klist_node;
struct klist {
   spinlock_t k_lock ;
   struct list_head k_list ;
   void (*get)(struct klist_node * ) ;
   void (*put)(struct klist_node * ) ;
};
struct klist_node {
   struct klist *n_klist ;
   struct list_head n_node ;
   struct kref n_ref ;
   struct completion n_removed ;
};
struct semaphore {
   spinlock_t lock ;
   unsigned int count ;
   struct list_head wait_list ;
};
struct dev_archdata {
   void *acpi_handle ;
};
struct device;
struct device_driver;
struct driver_private;
struct driver_private;
struct class;
struct class;
struct class_private;
struct class_private;
struct bus_type;
struct bus_type;
struct bus_type_private;
struct bus_type_private;
struct bus_attribute {
   struct attribute attr ;
   ssize_t (*show)(struct bus_type *bus , char *buf ) ;
   ssize_t (*store)(struct bus_type *bus , char const   *buf , size_t count ) ;
};
struct device_attribute;
struct driver_attribute;
struct bus_type {
   char const   *name ;
   struct bus_attribute *bus_attrs ;
   struct device_attribute *dev_attrs ;
   struct driver_attribute *drv_attrs ;
   int (*match)(struct device *dev , struct device_driver *drv ) ;
   int (*uevent)(struct device *dev , struct kobj_uevent_env *env ) ;
   int (*probe)(struct device *dev ) ;
   int (*remove)(struct device *dev ) ;
   void (*shutdown)(struct device *dev ) ;
   int (*suspend)(struct device *dev , pm_message_t state ) ;
   int (*suspend_late)(struct device *dev , pm_message_t state ) ;
   int (*resume_early)(struct device *dev ) ;
   int (*resume)(struct device *dev ) ;
   struct pm_ext_ops *pm ;
   struct bus_type_private *p ;
};
struct device_driver {
   char const   *name ;
   struct bus_type *bus ;
   struct module *owner ;
   char const   *mod_name ;
   int (*probe)(struct device *dev ) ;
   int (*remove)(struct device *dev ) ;
   void (*shutdown)(struct device *dev ) ;
   int (*suspend)(struct device *dev , pm_message_t state ) ;
   int (*resume)(struct device *dev ) ;
   struct attribute_group **groups ;
   struct pm_ops *pm ;
   struct driver_private *p ;
};
struct driver_attribute {
   struct attribute attr ;
   ssize_t (*show)(struct device_driver *driver , char *buf ) ;
   ssize_t (*store)(struct device_driver *driver , char const   *buf , size_t count ) ;
};
struct class_attribute;
struct class {
   char const   *name ;
   struct module *owner ;
   struct class_attribute *class_attrs ;
   struct device_attribute *dev_attrs ;
   struct kobject *dev_kobj ;
   int (*dev_uevent)(struct device *dev , struct kobj_uevent_env *env ) ;
   void (*class_release)(struct class *class ) ;
   void (*dev_release)(struct device *dev ) ;
   int (*suspend)(struct device *dev , pm_message_t state ) ;
   int (*resume)(struct device *dev ) ;
   struct pm_ops *pm ;
   struct class_private *p ;
};
struct class_attribute {
   struct attribute attr ;
   ssize_t (*show)(struct class *class , char *buf ) ;
   ssize_t (*store)(struct class *class , char const   *buf , size_t count ) ;
};
struct device_type {
   char const   *name ;
   struct attribute_group **groups ;
   int (*uevent)(struct device *dev , struct kobj_uevent_env *env ) ;
   void (*release)(struct device *dev ) ;
   int (*suspend)(struct device *dev , pm_message_t state ) ;
   int (*resume)(struct device *dev ) ;
   struct pm_ops *pm ;
};
struct device_attribute {
   struct attribute attr ;
   ssize_t (*show)(struct device *dev , struct device_attribute *attr , char *buf ) ;
   ssize_t (*store)(struct device *dev , struct device_attribute *attr , char const   *buf ,
                    size_t count ) ;
};
struct device_dma_parameters {
   unsigned int max_segment_size ;
   unsigned long segment_boundary_mask ;
};
struct dma_coherent_mem;
struct device {
   struct klist klist_children ;
   struct klist_node knode_parent ;
   struct klist_node knode_driver ;
   struct klist_node knode_bus ;
   struct device *parent ;
   struct kobject kobj ;
   char bus_id[20] ;
   char const   *init_name ;
   struct device_type *type ;
   unsigned int uevent_suppress : 1 ;
   struct semaphore sem ;
   struct bus_type *bus ;
   struct device_driver *driver ;
   void *driver_data ;
   void *platform_data ;
   struct dev_pm_info power ;
   u64 *dma_mask ;
   u64 coherent_dma_mask ;
   struct device_dma_parameters *dma_parms ;
   struct list_head dma_pools ;
   struct dma_coherent_mem *dma_mem ;
   struct dev_archdata archdata ;
   spinlock_t devres_lock ;
   struct list_head devres_head ;
   struct list_head node ;
   struct class *class ;
   dev_t devt ;
   struct attribute_group **groups ;
   void (*release)(struct device *dev ) ;
};
struct rcu_head {
   struct rcu_head *next ;
   void (*func)(struct rcu_head *head ) ;
};
enum dma_data_direction {
    DMA_BIDIRECTIONAL = 0,
    DMA_TO_DEVICE = 1,
    DMA_FROM_DEVICE = 2,
    DMA_NONE = 3
} ;
struct scatterlist {
   unsigned long page_link ;
   unsigned int offset ;
   unsigned int length ;
   dma_addr_t dma_address ;
   unsigned int dma_length ;
};
struct vm_area_struct;
struct dma_mapping_ops {
   int (*mapping_error)(struct device *dev , dma_addr_t dma_addr ) ;
   void *(*alloc_coherent)(struct device *dev , size_t size , dma_addr_t *dma_handle ,
                           gfp_t gfp ) ;
   void (*free_coherent)(struct device *dev , size_t size , void *vaddr , dma_addr_t dma_handle ) ;
   dma_addr_t (*map_single)(struct device *hwdev , phys_addr_t ptr , size_t size ,
                            int direction ) ;
   dma_addr_t (*map_simple)(struct device *hwdev , phys_addr_t ptr , size_t size ,
                            int direction ) ;
   void (*unmap_single)(struct device *dev , dma_addr_t addr , size_t size , int direction ) ;
   void (*sync_single_for_cpu)(struct device *hwdev , dma_addr_t dma_handle , size_t size ,
                               int direction ) ;
   void (*sync_single_for_device)(struct device *hwdev , dma_addr_t dma_handle , size_t size ,
                                  int direction ) ;
   void (*sync_single_range_for_cpu)(struct device *hwdev , dma_addr_t dma_handle ,
                                     unsigned long offset , size_t size , int direction ) ;
   void (*sync_single_range_for_device)(struct device *hwdev , dma_addr_t dma_handle ,
                                        unsigned long offset , size_t size , int direction ) ;
   void (*sync_sg_for_cpu)(struct device *hwdev , struct scatterlist *sg , int nelems ,
                           int direction ) ;
   void (*sync_sg_for_device)(struct device *hwdev , struct scatterlist *sg , int nelems ,
                              int direction ) ;
   int (*map_sg)(struct device *hwdev , struct scatterlist *sg , int nents , int direction ) ;
   void (*unmap_sg)(struct device *hwdev , struct scatterlist *sg , int nents , int direction ) ;
   int (*dma_supported)(struct device *hwdev , u64 mask ) ;
   int is_phys ;
};
enum dma_state {
    DMA_RESOURCE_SUSPEND = 0,
    DMA_RESOURCE_RESUME = 1,
    DMA_RESOURCE_AVAILABLE = 2,
    DMA_RESOURCE_REMOVED = 3
} ;
enum dma_state_client {
    DMA_ACK = 0,
    DMA_DUP = 1,
    DMA_NAK = 2
} ;
typedef s32 dma_cookie_t;
enum dma_status {
    DMA_SUCCESS = 0,
    DMA_IN_PROGRESS = 1,
    DMA_ERROR = 2
} ;
enum dma_transaction_type {
    DMA_MEMCPY = 0,
    DMA_XOR = 1,
    DMA_PQ_XOR = 2,
    DMA_DUAL_XOR = 3,
    DMA_PQ_UPDATE = 4,
    DMA_ZERO_SUM = 5,
    DMA_PQ_ZERO_SUM = 6,
    DMA_MEMSET = 7,
    DMA_MEMCPY_CRC32C = 8,
    DMA_INTERRUPT = 9,
    DMA_SLAVE = 10
} ;
enum dma_slave_width {
    DMA_SLAVE_WIDTH_8BIT = 0,
    DMA_SLAVE_WIDTH_16BIT = 1,
    DMA_SLAVE_WIDTH_32BIT = 2
} ;
enum dma_ctrl_flags {
    DMA_PREP_INTERRUPT = 1,
    DMA_CTRL_ACK = 2,
    DMA_COMPL_SKIP_SRC_UNMAP = 4,
    DMA_COMPL_SKIP_DEST_UNMAP = 8
} ;
struct __anonstruct_dma_cap_mask_t_107 {
   unsigned long bits[((11U + 8U * sizeof(long )) - 1U) / (8U * sizeof(long ))] ;
};
typedef struct __anonstruct_dma_cap_mask_t_107 dma_cap_mask_t;
struct dma_slave {
   struct device *dev ;
   struct device *dma_dev ;
   dma_addr_t tx_reg ;
   dma_addr_t rx_reg ;
   enum dma_slave_width reg_width ;
};
struct dma_chan_percpu {
   local_t refcount ;
   unsigned long memcpy_count ;
   unsigned long bytes_transferred ;
};
struct dma_device;
struct dma_chan {
   struct dma_device *device ;
   dma_cookie_t cookie ;
   int chan_id ;
   struct device dev ;
   struct kref refcount ;
   int slow_ref ;
   struct rcu_head rcu ;
   struct list_head device_node ;
   struct dma_chan_percpu *local ;
   int client_count ;
};
struct dma_client;
struct dma_client;
struct dma_client {
   enum dma_state_client (*event_callback)(struct dma_client *client , struct dma_chan *chan ,
                                           enum dma_state state ) ;
   dma_cap_mask_t cap_mask ;
   struct dma_slave *slave ;
   struct list_head global_node ;
};
struct dma_async_tx_descriptor {
   dma_cookie_t cookie ;
   enum dma_ctrl_flags flags ;
   dma_addr_t phys ;
   struct list_head tx_list ;
   struct dma_chan *chan ;
   dma_cookie_t (*tx_submit)(struct dma_async_tx_descriptor *tx ) ;
   void (*callback)(void *dma_async_param ) ;
   void *callback_param ;
   struct dma_async_tx_descriptor *next ;
   struct dma_async_tx_descriptor *parent ;
   spinlock_t lock ;
};
struct dma_device {
   unsigned int chancnt ;
   struct list_head channels ;
   struct list_head global_node ;
   dma_cap_mask_t cap_mask ;
   int max_xor ;
   struct kref refcount ;
   struct completion done ;
   int dev_id ;
   struct device *dev ;
   int (*device_alloc_chan_resources)(struct dma_chan *chan , struct dma_client *client ) ;
   void (*device_free_chan_resources)(struct dma_chan *chan ) ;
   struct dma_async_tx_descriptor *(*device_prep_dma_memcpy)(struct dma_chan *chan ,
                                                             dma_addr_t dest , dma_addr_t src ,
                                                             size_t len , unsigned long flags ) ;
   struct dma_async_tx_descriptor *(*device_prep_dma_xor)(struct dma_chan *chan ,
                                                          dma_addr_t dest , dma_addr_t *src ,
                                                          unsigned int src_cnt , size_t len ,
                                                          unsigned long flags ) ;
   struct dma_async_tx_descriptor *(*device_prep_dma_zero_sum)(struct dma_chan *chan ,
                                                               dma_addr_t *src , unsigned int src_cnt ,
                                                               size_t len , u32 *result ,
                                                               unsigned long flags ) ;
   struct dma_async_tx_descriptor *(*device_prep_dma_memset)(struct dma_chan *chan ,
                                                             dma_addr_t dest , int value ,
                                                             size_t len , unsigned long flags ) ;
   struct dma_async_tx_descriptor *(*device_prep_dma_interrupt)(struct dma_chan *chan ,
                                                                unsigned long flags ) ;
   struct dma_async_tx_descriptor *(*device_prep_slave_sg)(struct dma_chan *chan ,
                                                           struct scatterlist *sgl ,
                                                           unsigned int sg_len , enum dma_data_direction direction ,
                                                           unsigned long flags ) ;
   void (*device_terminate_all)(struct dma_chan *chan ) ;
   enum dma_status (*device_is_tx_complete)(struct dma_chan *chan , dma_cookie_t cookie ,
                                            dma_cookie_t *last , dma_cookie_t *used ) ;
   void (*device_issue_pending)(struct dma_chan *chan ) ;
};
struct task_struct;
struct kernel_cap_struct {
   __u32 cap[2] ;
};
typedef struct kernel_cap_struct kernel_cap_t;
typedef unsigned long cputime_t;
struct task_struct;
struct sem_undo_list;
struct sem_undo_list {
   atomic_t refcnt ;
   spinlock_t lock ;
   struct list_head list_proc ;
};
struct sysv_sem {
   struct sem_undo_list *undo_list ;
};
struct siginfo;
struct siginfo;
struct __anonstruct_sigset_t_109 {
   unsigned long sig[2] ;
};
typedef struct __anonstruct_sigset_t_109 sigset_t;
typedef void __signalfn_t(int  );
typedef __signalfn_t *__sighandler_t;
typedef void __restorefn_t(void);
typedef __restorefn_t *__sigrestore_t;
struct sigaction {
   __sighandler_t sa_handler ;
   unsigned long sa_flags ;
   __sigrestore_t sa_restorer ;
   sigset_t sa_mask ;
};
struct k_sigaction {
   struct sigaction sa ;
};
union sigval {
   int sival_int ;
   void *sival_ptr ;
};
typedef union sigval sigval_t;
struct __anonstruct__kill_111 {
   pid_t _pid ;
   uid_t _uid ;
};
struct __anonstruct__timer_112 {
   timer_t _tid ;
   int _overrun ;
   char _pad[sizeof(uid_t ) - sizeof(int )] ;
   sigval_t _sigval ;
   int _sys_private ;
};
struct __anonstruct__rt_113 {
   pid_t _pid ;
   uid_t _uid ;
   sigval_t _sigval ;
};
struct __anonstruct__sigchld_114 {
   pid_t _pid ;
   uid_t _uid ;
   int _status ;
   clock_t _utime ;
   clock_t _stime ;
};
struct __anonstruct__sigfault_115 {
   void *_addr ;
};
struct __anonstruct__sigpoll_116 {
   long _band ;
   int _fd ;
};
union __anonunion__sifields_110 {
   int _pad[(128U - 3U * sizeof(int )) / sizeof(int )] ;
   struct __anonstruct__kill_111 _kill ;
   struct __anonstruct__timer_112 _timer ;
   struct __anonstruct__rt_113 _rt ;
   struct __anonstruct__sigchld_114 _sigchld ;
   struct __anonstruct__sigfault_115 _sigfault ;
   struct __anonstruct__sigpoll_116 _sigpoll ;
};
struct siginfo {
   int si_signo ;
   int si_errno ;
   int si_code ;
   union __anonunion__sifields_110 _sifields ;
};
typedef struct siginfo siginfo_t;
struct siginfo;
struct sigpending {
   struct list_head list ;
   sigset_t signal ;
};
struct dentry;
struct dentry;
struct vfsmount;
struct vfsmount;
struct path {
   struct vfsmount *mnt ;
   struct dentry *dentry ;
};
struct fs_struct {
   atomic_t count ;
   rwlock_t lock ;
   int umask ;
   struct path root ;
   struct path pwd ;
};
struct pid_namespace;
struct upid {
   int nr ;
   struct pid_namespace *ns ;
   struct hlist_node pid_chain ;
};
struct pid {
   atomic_t count ;
   unsigned int level ;
   struct hlist_head tasks[3] ;
   struct rcu_head rcu ;
   struct upid numbers[1] ;
};
struct pid_link {
   struct hlist_node node ;
   struct pid *pid ;
};
struct pid_namespace;
struct prop_local_single {
   unsigned long events ;
   int shift ;
   unsigned long period ;
   spinlock_t lock ;
};
struct __anonstruct_seccomp_t_119 {
   int mode ;
};
typedef struct __anonstruct_seccomp_t_119 seccomp_t;
struct plist_head {
   struct list_head prio_list ;
   struct list_head node_list ;
};
struct rt_mutex_waiter;
struct rt_mutex_waiter;
struct task_struct;
struct rlimit {
   unsigned long rlim_cur ;
   unsigned long rlim_max ;
};
struct hrtimer_clock_base;
struct hrtimer_clock_base;
struct hrtimer_cpu_base;
struct hrtimer_cpu_base;
enum hrtimer_restart {
    HRTIMER_NORESTART = 0,
    HRTIMER_RESTART = 1
} ;
enum hrtimer_cb_mode {
    HRTIMER_CB_SOFTIRQ = 0,
    HRTIMER_CB_IRQSAFE = 1,
    HRTIMER_CB_IRQSAFE_NO_RESTART = 2,
    HRTIMER_CB_IRQSAFE_PERCPU = 3,
    HRTIMER_CB_IRQSAFE_UNLOCKED = 4
} ;
struct hrtimer {
   struct rb_node node ;
   ktime_t expires ;
   enum hrtimer_restart (*function)(struct hrtimer * ) ;
   struct hrtimer_clock_base *base ;
   unsigned long state ;
   enum hrtimer_cb_mode cb_mode ;
   struct list_head cb_entry ;
   void *start_site ;
   char start_comm[16] ;
   int start_pid ;
};
struct hrtimer_clock_base {
   struct hrtimer_cpu_base *cpu_base ;
   clockid_t index ;
   struct rb_root active ;
   struct rb_node *first ;
   ktime_t resolution ;
   ktime_t (*get_time)(void) ;
   ktime_t (*get_softirq_time)(void) ;
   ktime_t softirq_time ;
   ktime_t offset ;
   int (*reprogram)(struct hrtimer *t , struct hrtimer_clock_base *b , ktime_t n ) ;
};
struct hrtimer_cpu_base {
   spinlock_t lock ;
   struct hrtimer_clock_base clock_base[2] ;
   struct list_head cb_pending ;
   ktime_t expires_next ;
   int hres_active ;
   unsigned long nr_events ;
};
struct task_io_accounting {
   u64 rchar ;
   u64 wchar ;
   u64 syscr ;
   u64 syscw ;
   u64 read_bytes ;
   u64 write_bytes ;
   u64 cancelled_write_bytes ;
};
struct latency_record {
   unsigned long backtrace[12] ;
   unsigned int count ;
   unsigned long time ;
   unsigned long max ;
};
struct task_struct;
struct futex_pi_state;
struct futex_pi_state;
struct robust_list_head;
struct robust_list_head;
struct bio;
struct bio;
struct cfs_rq;
struct cfs_rq;
struct task_struct;
struct nsproxy;
struct nsproxy;
struct kioctx;
struct aio_ring_info {
   unsigned long mmap_base ;
   unsigned long mmap_size ;
   struct page **ring_pages ;
   spinlock_t ring_lock ;
   long nr_pages ;
   unsigned int nr ;
   unsigned int tail ;
   struct page *internal_pages[8] ;
};
struct kioctx {
   atomic_t users ;
   int dead ;
   struct mm_struct *mm ;
   unsigned long user_id ;
   struct kioctx *next ;
   wait_queue_head_t wait ;
   spinlock_t ctx_lock ;
   int reqs_active ;
   struct list_head active_reqs ;
   struct list_head run_list ;
   unsigned int max_reqs ;
   struct aio_ring_info ring_info ;
   struct delayed_work wq ;
};
struct mm_struct;
struct sighand_struct {
   atomic_t count ;
   struct k_sigaction action[64] ;
   spinlock_t siglock ;
   wait_queue_head_t signalfd_wqh ;
};
struct pacct_struct {
   int ac_flag ;
   long ac_exitcode ;
   unsigned long ac_mem ;
   cputime_t ac_utime ;
   cputime_t ac_stime ;
   unsigned long ac_minflt ;
   unsigned long ac_majflt ;
};
union __anonunion____missing_field_name_122 {
   pid_t pgrp ;
   pid_t __pgrp ;
};
union __anonunion____missing_field_name_123 {
   pid_t session ;
   pid_t __session ;
};
struct tty_struct;
struct taskstats;
struct tty_audit_buf;
struct signal_struct {
   atomic_t count ;
   atomic_t live ;
   wait_queue_head_t wait_chldexit ;
   struct task_struct *curr_target ;
   struct sigpending shared_pending ;
   int group_exit_code ;
   struct task_struct *group_exit_task ;
   int notify_count ;
   int group_stop_count ;
   unsigned int flags ;
   struct list_head posix_timers ;
   struct hrtimer real_timer ;
   struct pid *leader_pid ;
   ktime_t it_real_incr ;
   cputime_t it_prof_expires ;
   cputime_t it_virt_expires ;
   cputime_t it_prof_incr ;
   cputime_t it_virt_incr ;
   union __anonunion____missing_field_name_122 __annonCompField15 ;
   struct pid *tty_old_pgrp ;
   union __anonunion____missing_field_name_123 __annonCompField16 ;
   int leader ;
   struct tty_struct *tty ;
   cputime_t utime ;
   cputime_t stime ;
   cputime_t cutime ;
   cputime_t cstime ;
   cputime_t gtime ;
   cputime_t cgtime ;
   unsigned long nvcsw ;
   unsigned long nivcsw ;
   unsigned long cnvcsw ;
   unsigned long cnivcsw ;
   unsigned long min_flt ;
   unsigned long maj_flt ;
   unsigned long cmin_flt ;
   unsigned long cmaj_flt ;
   unsigned long inblock ;
   unsigned long oublock ;
   unsigned long cinblock ;
   unsigned long coublock ;
   struct task_io_accounting ioac ;
   unsigned long long sum_sched_runtime ;
   struct rlimit rlim[16] ;
   struct list_head cpu_timers[3] ;
   struct key *session_keyring ;
   struct key *process_keyring ;
   struct pacct_struct pacct ;
   struct taskstats *stats ;
   unsigned int audit_tty ;
   struct tty_audit_buf *tty_audit_buf ;
};
struct user_struct {
   atomic_t __count ;
   atomic_t processes ;
   atomic_t files ;
   atomic_t sigpending ;
   atomic_t inotify_watches ;
   atomic_t inotify_devs ;
   atomic_t epoll_devs ;
   atomic_t epoll_watches ;
   unsigned long mq_bytes ;
   unsigned long locked_shm ;
   struct key *uid_keyring ;
   struct key *session_keyring ;
   struct hlist_node uidhash_node ;
   uid_t uid ;
};
struct backing_dev_info;
struct backing_dev_info;
struct reclaim_state;
struct reclaim_state;
struct sched_info {
   unsigned long pcount ;
   unsigned long long cpu_time ;
   unsigned long long run_delay ;
   unsigned long long last_arrival ;
   unsigned long long last_queued ;
   unsigned int bkl_count ;
};
enum cpu_idle_type {
    CPU_IDLE = 0,
    CPU_NOT_IDLE = 1,
    CPU_NEWLY_IDLE = 2,
    CPU_MAX_IDLE_TYPES = 3
} ;
struct sched_group {
   struct sched_group *next ;
   cpumask_t cpumask ;
   unsigned int __cpu_power ;
   u32 reciprocal_cpu_power ;
};
enum sched_domain_level {
    SD_LV_NONE = 0,
    SD_LV_SIBLING = 1,
    SD_LV_MC = 2,
    SD_LV_CPU = 3,
    SD_LV_NODE = 4,
    SD_LV_ALLNODES = 5,
    SD_LV_MAX = 6
} ;
struct sched_domain {
   struct sched_domain *parent ;
   struct sched_domain *child ;
   struct sched_group *groups ;
   cpumask_t span ;
   unsigned long min_interval ;
   unsigned long max_interval ;
   unsigned int busy_factor ;
   unsigned int imbalance_pct ;
   unsigned int cache_nice_tries ;
   unsigned int busy_idx ;
   unsigned int idle_idx ;
   unsigned int newidle_idx ;
   unsigned int wake_idx ;
   unsigned int forkexec_idx ;
   int flags ;
   enum sched_domain_level level ;
   unsigned long last_balance ;
   unsigned int balance_interval ;
   unsigned int nr_balance_failed ;
   u64 last_update ;
   unsigned int lb_count[3] ;
   unsigned int lb_failed[3] ;
   unsigned int lb_balanced[3] ;
   unsigned int lb_imbalance[3] ;
   unsigned int lb_gained[3] ;
   unsigned int lb_hot_gained[3] ;
   unsigned int lb_nobusyg[3] ;
   unsigned int lb_nobusyq[3] ;
   unsigned int alb_count ;
   unsigned int alb_failed ;
   unsigned int alb_pushed ;
   unsigned int sbe_count ;
   unsigned int sbe_balanced ;
   unsigned int sbe_pushed ;
   unsigned int sbf_count ;
   unsigned int sbf_balanced ;
   unsigned int sbf_pushed ;
   unsigned int ttwu_wake_remote ;
   unsigned int ttwu_move_affine ;
   unsigned int ttwu_move_balance ;
};
struct io_context;
struct io_context;
struct group_info {
   int ngroups ;
   atomic_t usage ;
   gid_t small_block[32] ;
   int nblocks ;
   gid_t *blocks[0] ;
};
struct audit_context;
struct audit_context;
struct pipe_inode_info;
struct pipe_inode_info;
struct rq;
struct rq;
struct sched_domain;
struct sched_class {
   struct sched_class  const  *next ;
   void (*enqueue_task)(struct rq *rq , struct task_struct *p , int wakeup ) ;
   void (*dequeue_task)(struct rq *rq , struct task_struct *p , int sleep ) ;
   void (*yield_task)(struct rq *rq ) ;
   int (*select_task_rq)(struct task_struct *p , int sync ) ;
   void (*check_preempt_curr)(struct rq *rq , struct task_struct *p ) ;
   struct task_struct *(*pick_next_task)(struct rq *rq ) ;
   void (*put_prev_task)(struct rq *rq , struct task_struct *p ) ;
   unsigned long (*load_balance)(struct rq *this_rq , int this_cpu , struct rq *busiest ,
                                 unsigned long max_load_move , struct sched_domain *sd ,
                                 enum cpu_idle_type idle , int *all_pinned , int *this_best_prio ) ;
   int (*move_one_task)(struct rq *this_rq , int this_cpu , struct rq *busiest , struct sched_domain *sd ,
                        enum cpu_idle_type idle ) ;
   void (*pre_schedule)(struct rq *this_rq , struct task_struct *task ) ;
   void (*post_schedule)(struct rq *this_rq ) ;
   void (*task_wake_up)(struct rq *this_rq , struct task_struct *task ) ;
   void (*set_curr_task)(struct rq *rq ) ;
   void (*task_tick)(struct rq *rq , struct task_struct *p , int queued ) ;
   void (*task_new)(struct rq *rq , struct task_struct *p ) ;
   void (*set_cpus_allowed)(struct task_struct *p , cpumask_t const   *newmask ) ;
   void (*rq_online)(struct rq *rq ) ;
   void (*rq_offline)(struct rq *rq ) ;
   void (*switched_from)(struct rq *this_rq , struct task_struct *task , int running ) ;
   void (*switched_to)(struct rq *this_rq , struct task_struct *task , int running ) ;
   void (*prio_changed)(struct rq *this_rq , struct task_struct *task , int oldprio ,
                        int running ) ;
   void (*moved_group)(struct task_struct *p ) ;
};
struct load_weight {
   unsigned long weight ;
   unsigned long inv_weight ;
};
struct sched_entity {
   struct load_weight load ;
   struct rb_node run_node ;
   struct list_head group_node ;
   unsigned int on_rq ;
   u64 exec_start ;
   u64 sum_exec_runtime ;
   u64 vruntime ;
   u64 prev_sum_exec_runtime ;
   u64 last_wakeup ;
   u64 avg_overlap ;
   u64 wait_start ;
   u64 wait_max ;
   u64 wait_count ;
   u64 wait_sum ;
   u64 sleep_start ;
   u64 sleep_max ;
   s64 sum_sleep_runtime ;
   u64 block_start ;
   u64 block_max ;
   u64 exec_max ;
   u64 slice_max ;
   u64 nr_migrations ;
   u64 nr_migrations_cold ;
   u64 nr_failed_migrations_affine ;
   u64 nr_failed_migrations_running ;
   u64 nr_failed_migrations_hot ;
   u64 nr_forced_migrations ;
   u64 nr_forced2_migrations ;
   u64 nr_wakeups ;
   u64 nr_wakeups_sync ;
   u64 nr_wakeups_migrate ;
   u64 nr_wakeups_local ;
   u64 nr_wakeups_remote ;
   u64 nr_wakeups_affine ;
   u64 nr_wakeups_affine_attempts ;
   u64 nr_wakeups_passive ;
   u64 nr_wakeups_idle ;
   struct sched_entity *parent ;
   struct cfs_rq *cfs_rq ;
   struct cfs_rq *my_q ;
};
struct rt_rq;
struct sched_rt_entity {
   struct list_head run_list ;
   unsigned int time_slice ;
   unsigned long timeout ;
   int nr_cpus_allowed ;
   struct sched_rt_entity *back ;
   struct sched_rt_entity *parent ;
   struct rt_rq *rt_rq ;
   struct rt_rq *my_q ;
};
struct linux_binfmt;
struct files_struct;
struct css_set;
struct task_struct {
   long volatile   state ;
   void *stack ;
   atomic_t usage ;
   unsigned int flags ;
   unsigned int ptrace ;
   int lock_depth ;
   int prio ;
   int static_prio ;
   int normal_prio ;
   unsigned int rt_priority ;
   struct sched_class  const  *sched_class ;
   struct sched_entity se ;
   struct sched_rt_entity rt ;
   struct hlist_head preempt_notifiers ;
   unsigned char fpu_counter ;
   s8 oomkilladj ;
   unsigned int btrace_seq ;
   unsigned int policy ;
   cpumask_t cpus_allowed ;
   struct sched_info sched_info ;
   struct list_head tasks ;
   struct mm_struct *mm ;
   struct mm_struct *active_mm ;
   struct linux_binfmt *binfmt ;
   int exit_state ;
   int exit_code ;
   int exit_signal ;
   int pdeath_signal ;
   unsigned int personality ;
   unsigned int did_exec : 1 ;
   pid_t pid ;
   pid_t tgid ;
   struct task_struct *real_parent ;
   struct task_struct *parent ;
   struct list_head children ;
   struct list_head sibling ;
   struct task_struct *group_leader ;
   struct list_head ptraced ;
   struct list_head ptrace_entry ;
   struct pid_link pids[3] ;
   struct list_head thread_group ;
   struct completion *vfork_done ;
   int *set_child_tid ;
   int *clear_child_tid ;
   cputime_t utime ;
   cputime_t stime ;
   cputime_t utimescaled ;
   cputime_t stimescaled ;
   cputime_t gtime ;
   cputime_t prev_utime ;
   cputime_t prev_stime ;
   unsigned long nvcsw ;
   unsigned long nivcsw ;
   struct timespec start_time ;
   struct timespec real_start_time ;
   unsigned long min_flt ;
   unsigned long maj_flt ;
   cputime_t it_prof_expires ;
   cputime_t it_virt_expires ;
   unsigned long long it_sched_expires ;
   struct list_head cpu_timers[3] ;
   uid_t uid ;
   uid_t euid ;
   uid_t suid ;
   uid_t fsuid ;
   gid_t gid ;
   gid_t egid ;
   gid_t sgid ;
   gid_t fsgid ;
   struct group_info *group_info ;
   kernel_cap_t cap_effective ;
   kernel_cap_t cap_inheritable ;
   kernel_cap_t cap_permitted ;
   kernel_cap_t cap_bset ;
   struct user_struct *user ;
   unsigned int securebits ;
   unsigned char jit_keyring ;
   struct key *request_key_auth ;
   struct key *thread_keyring ;
   char comm[16] ;
   int link_count ;
   int total_link_count ;
   struct sysv_sem sysvsem ;
   unsigned long last_switch_timestamp ;
   unsigned long last_switch_count ;
   struct thread_struct thread ;
   struct fs_struct *fs ;
   struct files_struct *files ;
   struct nsproxy *nsproxy ;
   struct signal_struct *signal ;
   struct sighand_struct *sighand ;
   sigset_t blocked ;
   sigset_t real_blocked ;
   sigset_t saved_sigmask ;
   struct sigpending pending ;
   unsigned long sas_ss_sp ;
   size_t sas_ss_size ;
   int (*notifier)(void *priv ) ;
   void *notifier_data ;
   sigset_t *notifier_mask ;
   void *security ;
   struct audit_context *audit_context ;
   uid_t loginuid ;
   unsigned int sessionid ;
   seccomp_t seccomp ;
   u32 parent_exec_id ;
   u32 self_exec_id ;
   spinlock_t alloc_lock ;
   spinlock_t pi_lock ;
   struct plist_head pi_waiters ;
   struct rt_mutex_waiter *pi_blocked_on ;
   void *journal_info ;
   struct bio *bio_list ;
   struct bio **bio_tail ;
   struct reclaim_state *reclaim_state ;
   struct backing_dev_info *backing_dev_info ;
   struct io_context *io_context ;
   unsigned long ptrace_message ;
   siginfo_t *last_siginfo ;
   struct task_io_accounting ioac ;
   u64 acct_rss_mem1 ;
   u64 acct_vm_mem1 ;
   cputime_t acct_timexpd ;
   nodemask_t mems_allowed ;
   int cpuset_mems_generation ;
   int cpuset_mem_spread_rotor ;
   struct css_set *cgroups ;
   struct list_head cg_list ;
   struct robust_list_head *robust_list ;
   struct list_head pi_state_list ;
   struct futex_pi_state *pi_state_cache ;
   atomic_t fs_excl ;
   struct rcu_head rcu ;
   struct list_head *scm_work_list ;
   struct pipe_inode_info *splice_pipe ;
   struct prop_local_single dirties ;
   int latency_record_count ;
   struct latency_record latency_record[32] ;
};
struct pid_namespace;
struct task_struct;
struct mm_struct;
struct task_struct;
/* compiler builtin: 
   long __builtin_expect(long  , long  ) ;  */
extern void __bad_percpu_size(void) ;
__inline static int variable_test_bit(int nr , unsigned long const volatile   *addr ) 
{ int oldbit ;

  {
  {
  __asm__  volatile   ("bt %2,%1\n\t"
                       "sbb %0,%0": "=r" (oldbit): "m" (*((unsigned long *)addr)),
                       "Ir" (nr));
  }
  return (oldbit);
}
}
extern int ( /* format attribute */  sprintf)(char *buf , char const   *fmt  , ...) ;
extern int ( /* format attribute */  snprintf)(char *buf , size_t size , char const   *fmt 
                                               , ...) ;
extern int ( /* format attribute */ __attribute__((__regparm__(0))) printk)(char const   *fmt 
                                                                            , ...)  __attribute__((__cold__)) ;
extern int __bitmap_equal(unsigned long const   *bitmap1 , unsigned long const   *bitmap2 ,
                          int bits ) ;
extern void __bitmap_and(unsigned long *dst , unsigned long const   *bitmap1 , unsigned long const   *bitmap2 ,
                         int bits ) ;
__inline static void bitmap_and(unsigned long *dst , unsigned long const   *src1 ,
                                unsigned long const   *src2 , int nbits ) 
{ 

  {
  if (nbits <= 32) {
    {
    *dst = (unsigned long )(*src1 & *src2);
    }
  } else {
    {
    __bitmap_and(dst, src1, src2, nbits);
    }
  }
  return;
}
}
__inline static int bitmap_equal(unsigned long const   *src1 , unsigned long const   *src2 ,
                                 int nbits ) 
{ unsigned long tmp ;
  int tmp___0 ;
  int tmp___1 ;

  {
  if (nbits <= 32) {
    if (nbits % 32) {
      {
      tmp = (1UL << nbits % 32) - 1UL;
      }
    } else {
      {
      tmp = ~ 0UL;
      }
    }
    if ((*src1 ^ *src2) & (unsigned long const   )tmp) {
      {
      tmp___0 = 0;
      }
    } else {
      {
      tmp___0 = 1;
      }
    }
    return (tmp___0);
  } else {
    {
    tmp___1 = __bitmap_equal(src1, src2, nbits);
    }
    return (tmp___1);
  }
}
}
extern int __next_cpu(int n , cpumask_t const   *srcp ) ;
extern cpumask_t cpu_possible_map ;
__inline static void prefetch(void const   *x ) 
{ 

  {
  {
  __asm__  volatile   ("661:\n\t"
                       ".byte 0x8d,0x74,0x26,0x00\n"
                       "\n662:\n"
                       ".section .altinstructions,\"a\"\n"
                       " "
                       ".balign 4"
                       " "
                       "\n"
                       " "
                       ".long"
                       " "
                       "661b\n"
                       " "
                       ".long"
                       " "
                       "663f\n"
                       "\t .byte %c0\n"
                       "\t .byte 662b-661b\n"
                       "\t .byte 664f-663f\n"
                       ".previous\n"
                       ".section .altinstr_replacement,\"ax\"\n"
                       "663:\n\t"
                       "prefetchnta (%1)"
                       "\n664:\n"
                       ".previous": : "i" (25), "r" (x));
  }
  return;
}
}
__inline static void __list_add(struct list_head *new , struct list_head *prev , struct list_head *next ) 
{ 

  {
  {
  next->prev = new;
  new->next = next;
  new->prev = prev;
  prev->next = new;
  }
  return;
}
}
__inline static void list_add_tail(struct list_head *new , struct list_head *head ) 
{ 

  {
  {
  __list_add(new, head->prev, head);
  }
  return;
}
}
__inline static void __list_del(struct list_head *prev , struct list_head *next ) 
{ 

  {
  {
  next->prev = prev;
  prev->next = next;
  }
  return;
}
}
__inline static void list_del(struct list_head *entry ) 
{ 

  {
  {
  __list_del(entry->prev, entry->next);
  entry->next = (struct list_head *)((void *)1048832);
  entry->prev = (struct list_head *)((void *)2097664);
  }
  return;
}
}
register unsigned long current_stack_pointer  __asm__("esp") __attribute__((__used__))  ;
__inline static void atomic_add(int i , atomic_t *v ) 
{ 

  {
  {
  __asm__  volatile   (".section .smp_locks,\"a\"\n"
                       " "
                       ".balign 4"
                       " "
                       "\n"
                       " "
                       ".long"
                       " "
                       "661f\n"
                       ".previous\n"
                       "661:\n\tlock; "
                       "addl %1,%0": "+m" (v->counter): "ir" (i));
  }
  return;
}
}
__inline static void atomic_sub(int i , atomic_t *v ) 
{ 

  {
  {
  __asm__  volatile   (".section .smp_locks,\"a\"\n"
                       " "
                       ".balign 4"
                       " "
                       "\n"
                       " "
                       ".long"
                       " "
                       "661f\n"
                       ".previous\n"
                       "661:\n\tlock; "
                       "subl %1,%0": "+m" (v->counter): "ir" (i));
  }
  return;
}
}
__inline static long atomic_long_read(atomic_long_t *l ) 
{ atomic_t *v ;

  {
  {
  v = l;
  }
  return ((long )v->counter);
}
}
extern void init_waitqueue_head(wait_queue_head_t *q ) ;
extern struct page *mem_map ;
extern void __mutex_init(struct mutex *lock , char const   *name , struct lock_class_key *key ) ;
extern void mutex_lock(struct mutex *lock ) ;
extern void mutex_unlock(struct mutex *lock ) ;
extern int per_cpu__cpu_number ;
extern void kref_init(struct kref *kref ) ;
extern void kref_get(struct kref *kref ) ;
extern int kref_put(struct kref *kref , void (*release)(struct kref *kref ) ) ;
extern unsigned long volatile   jiffies  __attribute__((__section__(".data"))) ;
extern unsigned long msecs_to_jiffies(unsigned int m ) ;
extern void *__percpu_alloc_mask(size_t size , gfp_t gfp , cpumask_t *mask ) ;
extern void percpu_free(void *__pdata ) ;
__inline static void local_inc(local_t *l ) 
{ 

  {
  {
  __asm__  volatile   (" "
                       "incl"
                       " "
                       "%0": "+m" (l->a.counter));
  }
  return;
}
}
__inline static void local_dec(local_t *l ) 
{ 

  {
  {
  __asm__  volatile   (" "
                       "decl"
                       " "
                       "%0": "+m" (l->a.counter));
  }
  return;
}
}
__inline static void init_completion(struct completion *x ) 
{ 

  {
  {
  x->done = 0U;
  init_waitqueue_head(& x->wait);
  }
  return;
}
}
extern void wait_for_completion(struct completion * ) ;
extern void complete(struct completion * ) ;
extern int __class_register(struct class *class , struct lock_class_key *key ) ;
extern int device_register(struct device *dev ) ;
extern void device_unregister(struct device *dev ) ;
extern void call_rcu(struct rcu_head *head , void (*func)(struct rcu_head *head ) ) ;
__inline static int valid_dma_direction(int dma_direction ) 
{ int tmp ;

  {
  if (dma_direction == 0) {
    {
    tmp = 1;
    }
  } else {
    if (dma_direction == 1) {
      {
      tmp = 1;
      }
    } else {
      if (dma_direction == 2) {
        {
        tmp = 1;
        }
      } else {
        {
        tmp = 0;
        }
      }
    }
  }
  return (tmp);
}
}
__inline static unsigned long virt_to_phys(void volatile   *address ) 
{ 

  {
  return ((unsigned long )address - 3221225472UL);
}
}
extern struct dma_mapping_ops *dma_ops ;
__inline static struct dma_mapping_ops *get_dma_ops(struct device *dev ) 
{ 

  {
  return (dma_ops);
}
}
__inline static dma_addr_t dma_map_single(struct device *hwdev , void *ptr , size_t size ,
                                          int direction ) 
{ struct dma_mapping_ops *ops ;
  struct dma_mapping_ops *tmp ;
  int tmp___0 ;
  int tmp___1 ;
  long tmp___2 ;
  unsigned long tmp___3 ;
  dma_addr_t tmp___4 ;

  {
  {
  tmp = get_dma_ops(hwdev);
  ops = tmp;
  }
  {
  while (1) {
    while_0_continue: /* CIL Label */ ;
    {
    tmp___0 = valid_dma_direction(direction);
    }
    if (tmp___0) {
      {
      tmp___1 = 0;
      }
    } else {
      {
      tmp___1 = 1;
      }
    }
    {
    tmp___2 = __builtin_expect((long )tmp___1, 0L);
    }
    if (tmp___2) {
      {
      while (1) {
        while_1_continue: /* CIL Label */ ;
        {
        __asm__  volatile   ("1:\tud2\n"
                             ".pushsection __bug_table,\"a\"\n"
                             "2:\t.long 1b, %c0\n"
                             "\t.word %c1, 0\n"
                             "\t.org 2b+%c2\n"
                             ".popsection": : "i" ("/lib/modules/2.6.27-11-generic/build/include/asm/dma-mapping.h"),
                             "i" (107), "i" (sizeof(struct bug_entry )));
        }
        {
        while (1) {
          while_2_continue: /* CIL Label */ ;
        }
        while_2_break: /* CIL Label */ ;
        }
        goto while_1_break;
      }
      while_1_break: /* CIL Label */ ;
      }
    }
    goto while_0_break;
  }
  while_0_break: /* CIL Label */ ;
  }
  {
  tmp___3 = virt_to_phys((void volatile   *)ptr);
  tmp___4 = (*(ops->map_single))(hwdev, tmp___3, size, direction);
  }
  return (tmp___4);
}
}
__inline static void dma_unmap_single(struct device *dev , dma_addr_t addr , size_t size ,
                                      int direction ) 
{ struct dma_mapping_ops *ops ;
  struct dma_mapping_ops *tmp ;
  int tmp___0 ;
  int tmp___1 ;
  long tmp___2 ;

  {
  {
  tmp = get_dma_ops(dev);
  ops = tmp;
  }
  {
  while (1) {
    while_3_continue: /* CIL Label */ ;
    {
    tmp___0 = valid_dma_direction(direction);
    }
    if (tmp___0) {
      {
      tmp___1 = 0;
      }
    } else {
      {
      tmp___1 = 1;
      }
    }
    {
    tmp___2 = __builtin_expect((long )tmp___1, 0L);
    }
    if (tmp___2) {
      {
      while (1) {
        while_4_continue: /* CIL Label */ ;
        {
        __asm__  volatile   ("1:\tud2\n"
                             ".pushsection __bug_table,\"a\"\n"
                             "2:\t.long 1b, %c0\n"
                             "\t.word %c1, 0\n"
                             "\t.org 2b+%c2\n"
                             ".popsection": : "i" ("/lib/modules/2.6.27-11-generic/build/include/asm/dma-mapping.h"),
                             "i" (117), "i" (sizeof(struct bug_entry )));
        }
        {
        while (1) {
          while_5_continue: /* CIL Label */ ;
        }
        while_5_break: /* CIL Label */ ;
        }
        goto while_4_break;
      }
      while_4_break: /* CIL Label */ ;
      }
    }
    goto while_3_break;
  }
  while_3_break: /* CIL Label */ ;
  }
  if (ops->unmap_single) {
    {
    (*(ops->unmap_single))(dev, addr, size, direction);
    }
  }
  return;
}
}
__inline static dma_addr_t dma_map_page(struct device *dev , struct page *page , size_t offset ,
                                        size_t size , int direction ) 
{ struct dma_mapping_ops *ops ;
  struct dma_mapping_ops *tmp ;
  int tmp___0 ;
  int tmp___1 ;
  long tmp___2 ;
  dma_addr_t tmp___3 ;

  {
  {
  tmp = get_dma_ops(dev);
  ops = tmp;
  }
  {
  while (1) {
    while_6_continue: /* CIL Label */ ;
    {
    tmp___0 = valid_dma_direction(direction);
    }
    if (tmp___0) {
      {
      tmp___1 = 0;
      }
    } else {
      {
      tmp___1 = 1;
      }
    }
    {
    tmp___2 = __builtin_expect((long )tmp___1, 0L);
    }
    if (tmp___2) {
      {
      while (1) {
        while_7_continue: /* CIL Label */ ;
        {
        __asm__  volatile   ("1:\tud2\n"
                             ".pushsection __bug_table,\"a\"\n"
                             "2:\t.long 1b, %c0\n"
                             "\t.word %c1, 0\n"
                             "\t.org 2b+%c2\n"
                             ".popsection": : "i" ("/lib/modules/2.6.27-11-generic/build/include/asm/dma-mapping.h"),
                             "i" (225), "i" (sizeof(struct bug_entry )));
        }
        {
        while (1) {
          while_8_continue: /* CIL Label */ ;
        }
        while_8_break: /* CIL Label */ ;
        }
        goto while_7_break;
      }
      while_7_break: /* CIL Label */ ;
      }
    }
    goto while_6_break;
  }
  while_6_break: /* CIL Label */ ;
  }
  {
  tmp___3 = (*(ops->map_single))(dev, (unsigned long )(((unsigned int )((unsigned long )(page - mem_map)) << 12) + offset),
                                 size, direction);
  }
  return (tmp___3);
}
}
__inline static void dma_unmap_page(struct device *dev , dma_addr_t addr , size_t size ,
                                    int direction ) 
{ 

  {
  {
  dma_unmap_single(dev, addr, size, direction);
  }
  return;
}
}
void dma_chan_cleanup(struct kref *kref ) ;
__inline static void dma_chan_get(struct dma_chan *chan ) 
{ struct percpu_data *__p ;
  int ret__ ;
  long tmp ;

  {
  {
  tmp = __builtin_expect((long )(! (! chan->slow_ref)), 0L);
  }
  if (tmp) {
    {
    kref_get(& chan->refcount);
    }
  } else {
    {
    __p = (struct percpu_data *)(~ ((unsigned long )chan->local));
    }
    {
    while (1) {
      while_9_continue: /* CIL Label */ ;
      goto while_9_break;
    }
    while_9_break: /* CIL Label */ ;
    }
    if ((int )sizeof(per_cpu__cpu_number) == 1) {
      goto switch_10_1;
    } else {
      if ((int )sizeof(per_cpu__cpu_number) == 2) {
        goto switch_10_2;
      } else {
        if ((int )sizeof(per_cpu__cpu_number) == 4) {
          goto switch_10_4;
        } else {
          {
          goto switch_10_default;
          if (0) {
            switch_10_1: /* CIL Label */ 
            {
            __asm__  ("mov"
                      "b "
                      "%%fs:"
                      "%1,%0": "=r" (ret__): "m" (per_cpu__cpu_number));
            }
            goto switch_10_break;
            switch_10_2: /* CIL Label */ 
            {
            __asm__  ("mov"
                      "w "
                      "%%fs:"
                      "%1,%0": "=r" (ret__): "m" (per_cpu__cpu_number));
            }
            goto switch_10_break;
            switch_10_4: /* CIL Label */ 
            {
            __asm__  ("mov"
                      "l "
                      "%%fs:"
                      "%1,%0": "=r" (ret__): "m" (per_cpu__cpu_number));
            }
            goto switch_10_break;
            switch_10_default: /* CIL Label */ 
            {
            __bad_percpu_size();
            }
          } else {
            switch_10_break: /* CIL Label */ ;
          }
          }
        }
      }
    }
    {
    local_inc(& ((struct dma_chan_percpu *)__p->ptrs[ret__])->refcount);
    }
    {
    while (1) {
      while_11_continue: /* CIL Label */ ;
      goto while_11_break;
    }
    while_11_break: /* CIL Label */ ;
    }
  }
  return;
}
}
__inline static void dma_chan_put(struct dma_chan *chan ) 
{ struct percpu_data *__p ;
  int ret__ ;
  long tmp ;

  {
  {
  tmp = __builtin_expect((long )(! (! chan->slow_ref)), 0L);
  }
  if (tmp) {
    {
    kref_put(& chan->refcount, & dma_chan_cleanup);
    }
  } else {
    {
    __p = (struct percpu_data *)(~ ((unsigned long )chan->local));
    }
    {
    while (1) {
      while_12_continue: /* CIL Label */ ;
      goto while_12_break;
    }
    while_12_break: /* CIL Label */ ;
    }
    if ((int )sizeof(per_cpu__cpu_number) == 1) {
      goto switch_13_1;
    } else {
      if ((int )sizeof(per_cpu__cpu_number) == 2) {
        goto switch_13_2;
      } else {
        if ((int )sizeof(per_cpu__cpu_number) == 4) {
          goto switch_13_4;
        } else {
          {
          goto switch_13_default;
          if (0) {
            switch_13_1: /* CIL Label */ 
            {
            __asm__  ("mov"
                      "b "
                      "%%fs:"
                      "%1,%0": "=r" (ret__): "m" (per_cpu__cpu_number));
            }
            goto switch_13_break;
            switch_13_2: /* CIL Label */ 
            {
            __asm__  ("mov"
                      "w "
                      "%%fs:"
                      "%1,%0": "=r" (ret__): "m" (per_cpu__cpu_number));
            }
            goto switch_13_break;
            switch_13_4: /* CIL Label */ 
            {
            __asm__  ("mov"
                      "l "
                      "%%fs:"
                      "%1,%0": "=r" (ret__): "m" (per_cpu__cpu_number));
            }
            goto switch_13_break;
            switch_13_default: /* CIL Label */ 
            {
            __bad_percpu_size();
            }
          } else {
            switch_13_break: /* CIL Label */ ;
          }
          }
        }
      }
    }
    {
    local_dec(& ((struct dma_chan_percpu *)__p->ptrs[ret__])->refcount);
    }
    {
    while (1) {
      while_14_continue: /* CIL Label */ ;
      goto while_14_break;
    }
    while_14_break: /* CIL Label */ ;
    }
  }
  return;
}
}
void dma_async_client_register(struct dma_client *client ) ;
void dma_async_client_unregister(struct dma_client *client ) ;
void dma_async_client_chan_request(struct dma_client *client ) ;
dma_cookie_t dma_async_memcpy_buf_to_buf(struct dma_chan *chan , void *dest , void *src ,
                                         size_t len ) ;
dma_cookie_t dma_async_memcpy_buf_to_pg(struct dma_chan *chan , struct page *page ,
                                        unsigned int offset , void *kdata , size_t len ) ;
dma_cookie_t dma_async_memcpy_pg_to_pg(struct dma_chan *chan , struct page *dest_pg ,
                                       unsigned int dest_off , struct page *src_pg ,
                                       unsigned int src_off , size_t len ) ;
void dma_async_tx_descriptor_init(struct dma_async_tx_descriptor *tx , struct dma_chan *chan ) ;
__inline static int __dma_has_cap(enum dma_transaction_type tx_type , dma_cap_mask_t *srcp ) 
{ int tmp___0 ;

  {
  {
  tmp___0 = variable_test_bit((int )tx_type, (unsigned long const volatile   *)(srcp->bits));
  }
  return (tmp___0);
}
}
__inline static void dma_async_issue_pending(struct dma_chan *chan ) 
{ 

  {
  {
  (*((chan->device)->device_issue_pending))(chan);
  }
  return;
}
}
__inline static enum dma_status dma_async_is_tx_complete(struct dma_chan *chan , dma_cookie_t cookie ,
                                                         dma_cookie_t *last , dma_cookie_t *used ) 
{ enum dma_status tmp ;

  {
  {
  tmp = (*((chan->device)->device_is_tx_complete))(chan, cookie, last, used);
  }
  return (tmp);
}
}
enum dma_status dma_sync_wait(struct dma_chan *chan , dma_cookie_t cookie ) ;
int dma_async_device_register(struct dma_device *device ) ;
void dma_async_device_unregister(struct dma_device *device ) ;
static struct mutex dma_list_mutex  =    {{1}, {{0U}}, {& dma_list_mutex.wait_list, & dma_list_mutex.wait_list}};
static struct list_head dma_device_list  =    {& dma_device_list, & dma_device_list};
static struct list_head dma_client_list  =    {& dma_client_list, & dma_client_list};
static ssize_t show_memcpy_count(struct device *dev , struct device_attribute *attr ,
                                 char *buf ) 
{ struct dma_chan *chan ;
  struct device  const  *__mptr ;
  unsigned long count ;
  int i ;
  struct percpu_data *__p ;
  int tmp ;

  {
  {
  __mptr = (struct device  const  *)dev;
  chan = (struct dma_chan *)((char *)__mptr - (unsigned int )(& ((struct dma_chan *)0)->dev));
  count = 0UL;
  i = -1;
  }
  {
  while (1) {
    while_15_continue: /* CIL Label */ ;
    {
    i = __next_cpu(i, (cpumask_t const   *)(& cpu_possible_map));
    }
    if (! (i < 64)) {
      goto while_15_break;
    }
    {
    __p = (struct percpu_data *)(~ ((unsigned long )chan->local));
    count += ((struct dma_chan_percpu *)__p->ptrs[i])->memcpy_count;
    }
  }
  while_15_break: /* CIL Label */ ;
  }
  {
  tmp = sprintf(buf, "%lu\n", count);
  }
  return (tmp);
}
}
static ssize_t show_bytes_transferred(struct device *dev , struct device_attribute *attr ,
                                      char *buf ) 
{ struct dma_chan *chan ;
  struct device  const  *__mptr ;
  unsigned long count ;
  int i ;
  struct percpu_data *__p ;
  int tmp ;

  {
  {
  __mptr = (struct device  const  *)dev;
  chan = (struct dma_chan *)((char *)__mptr - (unsigned int )(& ((struct dma_chan *)0)->dev));
  count = 0UL;
  i = -1;
  }
  {
  while (1) {
    while_16_continue: /* CIL Label */ ;
    {
    i = __next_cpu(i, (cpumask_t const   *)(& cpu_possible_map));
    }
    if (! (i < 64)) {
      goto while_16_break;
    }
    {
    __p = (struct percpu_data *)(~ ((unsigned long )chan->local));
    count += ((struct dma_chan_percpu *)__p->ptrs[i])->bytes_transferred;
    }
  }
  while_16_break: /* CIL Label */ ;
  }
  {
  tmp = sprintf(buf, "%lu\n", count);
  }
  return (tmp);
}
}
static ssize_t show_in_use(struct device *dev , struct device_attribute *attr , char *buf ) 
{ struct dma_chan *chan ;
  struct device  const  *__mptr ;
  int in_use ;
  struct percpu_data *__p ;
  int ret__ ;
  long tmp ;
  long tmp___0 ;
  int tmp___1 ;

  {
  {
  __mptr = (struct device  const  *)dev;
  chan = (struct dma_chan *)((char *)__mptr - (unsigned int )(& ((struct dma_chan *)0)->dev));
  in_use = 0;
  tmp___0 = __builtin_expect((long )(! (! chan->slow_ref)), 0L);
  }
  if (tmp___0) {
    if (chan->refcount.refcount.counter > 1) {
      {
      in_use = 1;
      }
    } else {
      goto _L;
    }
  } else {
    _L: /* CIL Label */ 
    {
    __p = (struct percpu_data *)(~ ((unsigned long )chan->local));
    }
    {
    while (1) {
      while_17_continue: /* CIL Label */ ;
      goto while_17_break;
    }
    while_17_break: /* CIL Label */ ;
    }
    if ((int )sizeof(per_cpu__cpu_number) == 1) {
      goto switch_18_1;
    } else {
      if ((int )sizeof(per_cpu__cpu_number) == 2) {
        goto switch_18_2;
      } else {
        if ((int )sizeof(per_cpu__cpu_number) == 4) {
          goto switch_18_4;
        } else {
          {
          goto switch_18_default;
          if (0) {
            switch_18_1: /* CIL Label */ 
            {
            __asm__  ("mov"
                      "b "
                      "%%fs:"
                      "%1,%0": "=r" (ret__): "m" (per_cpu__cpu_number));
            }
            goto switch_18_break;
            switch_18_2: /* CIL Label */ 
            {
            __asm__  ("mov"
                      "w "
                      "%%fs:"
                      "%1,%0": "=r" (ret__): "m" (per_cpu__cpu_number));
            }
            goto switch_18_break;
            switch_18_4: /* CIL Label */ 
            {
            __asm__  ("mov"
                      "l "
                      "%%fs:"
                      "%1,%0": "=r" (ret__): "m" (per_cpu__cpu_number));
            }
            goto switch_18_break;
            switch_18_default: /* CIL Label */ 
            {
            __bad_percpu_size();
            }
          } else {
            switch_18_break: /* CIL Label */ ;
          }
          }
        }
      }
    }
    {
    tmp = atomic_long_read(& ((struct dma_chan_percpu *)__p->ptrs[ret__])->refcount.a);
    }
    if (tmp > 0L) {
      {
      in_use = 1;
      }
    }
    {
    while (1) {
      while_19_continue: /* CIL Label */ ;
      goto while_19_break;
    }
    while_19_break: /* CIL Label */ ;
    }
  }
  {
  tmp___1 = sprintf(buf, "%d\n", in_use);
  }
  return (tmp___1);
}
}
static struct device_attribute dma_attrs[4]  = {      {{"memcpy_count", (struct module *)0, (mode_t )292}, & show_memcpy_count, (ssize_t (*)(struct device *dev ,
                                                                                            struct device_attribute *attr ,
                                                                                            char const   *buf ,
                                                                                            size_t count ))((void *)0)}, 
        {{"bytes_transferred",
       (struct module *)0, (mode_t )292}, & show_bytes_transferred, (ssize_t (*)(struct device *dev ,
                                                                                 struct device_attribute *attr ,
                                                                                 char const   *buf ,
                                                                                 size_t count ))((void *)0)}, 
        {{"in_use",
       (struct module *)0, (mode_t )292}, & show_in_use, (ssize_t (*)(struct device *dev ,
                                                                      struct device_attribute *attr ,
                                                                      char const   *buf ,
                                                                      size_t count ))((void *)0)}, 
        {{(char const   *)((void *)0),
       (struct module *)0, (unsigned short)0}, (ssize_t (*)(struct device *dev , struct device_attribute *attr ,
                                                            char *buf ))0, (ssize_t (*)(struct device *dev ,
                                                                                        struct device_attribute *attr ,
                                                                                        char const   *buf ,
                                                                                        size_t count ))0}};
static void dma_async_device_cleanup(struct kref *kref ) ;
static void dma_dev_release(struct device *dev ) 
{ struct dma_chan *chan ;
  struct device  const  *__mptr ;

  {
  {
  __mptr = (struct device  const  *)dev;
  chan = (struct dma_chan *)((char *)__mptr - (unsigned int )(& ((struct dma_chan *)0)->dev));
  kref_put(& (chan->device)->refcount, & dma_async_device_cleanup);
  }
  return;
}
}
static struct class dma_devclass  = 
     {"dma", (struct module *)0, (struct class_attribute *)0, dma_attrs, (struct kobject *)0,
    (int (*)(struct device *dev , struct kobj_uevent_env *env ))0, (void (*)(struct class *class ))0,
    & dma_dev_release, (int (*)(struct device *dev , pm_message_t state ))0, (int (*)(struct device *dev ))0,
    (struct pm_ops *)0, (struct class_private *)0};
static int __dma_chan_satisfies_mask(struct dma_chan *chan , dma_cap_mask_t *want ) 
{ dma_cap_mask_t has ;
  int tmp ;

  {
  {
  bitmap_and(has.bits, (unsigned long const   *)(want->bits), (unsigned long const   *)((chan->device)->cap_mask.bits),
             11);
  tmp = bitmap_equal((unsigned long const   *)(want->bits), (unsigned long const   *)(has.bits),
                     11);
  }
  return (tmp);
}
}
static void dma_client_chan_alloc(struct dma_client *client ) 
{ struct dma_device *device ;
  struct dma_chan *chan ;
  int desc ;
  enum dma_state_client ack ;
  struct list_head  const  *__mptr ;
  struct list_head  const  *__mptr___0 ;
  struct list_head  const  *__mptr___1 ;
  struct list_head  const  *__mptr___2 ;
  int tmp ;

  {
  {
  __mptr = (struct list_head  const  *)dma_device_list.next;
  device = (struct dma_device *)((char *)__mptr - (unsigned int )(& ((struct dma_device *)0)->global_node));
  }
  {
  while (1) {
    while_20_continue: /* CIL Label */ ;
    {
    prefetch((void const   *)device->global_node.next);
    }
    if (! ((unsigned int )(& device->global_node) != (unsigned int )(& dma_device_list))) {
      goto while_20_break;
    }
    if (client->slave) {
      if ((client->slave)->dma_dev) {
        if ((unsigned int )(client->slave)->dma_dev != (unsigned int )device->dev) {
          goto __Cont;
        }
      }
    }
    {
    __mptr___1 = (struct list_head  const  *)device->channels.next;
    chan = (struct dma_chan *)((char *)__mptr___1 - (unsigned int )(& ((struct dma_chan *)0)->device_node));
    }
    {
    while (1) {
      while_21_continue: /* CIL Label */ ;
      {
      prefetch((void const   *)chan->device_node.next);
      }
      if (! ((unsigned int )(& chan->device_node) != (unsigned int )(& device->channels))) {
        goto while_21_break;
      }
      {
      tmp = __dma_chan_satisfies_mask(chan, & client->cap_mask);
      }
      if (! tmp) {
        goto __Cont___0;
      }
      {
      desc = (*((chan->device)->device_alloc_chan_resources))(chan, client);
      }
      if (desc >= 0) {
        {
        ack = (*(client->event_callback))(client, chan, 2);
        }
        if ((int )ack == 0) {
          {
          dma_chan_get(chan);
          (chan->client_count) ++;
          }
        } else {
          if ((int )ack == 2) {
            return;
          }
        }
      }
      __Cont___0: /* CIL Label */ 
      {
      __mptr___2 = (struct list_head  const  *)chan->device_node.next;
      chan = (struct dma_chan *)((char *)__mptr___2 - (unsigned int )(& ((struct dma_chan *)0)->device_node));
      }
    }
    while_21_break: /* CIL Label */ ;
    }
    __Cont: /* CIL Label */ 
    {
    __mptr___0 = (struct list_head  const  *)device->global_node.next;
    device = (struct dma_device *)((char *)__mptr___0 - (unsigned int )(& ((struct dma_device *)0)->global_node));
    }
  }
  while_20_break: /* CIL Label */ ;
  }
  return;
}
}
enum dma_status dma_sync_wait(struct dma_chan *chan , dma_cookie_t cookie ) 
{ enum dma_status status ;
  unsigned long dma_sync_wait_timeout ;
  unsigned long tmp ;

  {
  {
  tmp = msecs_to_jiffies(5000U);
  dma_sync_wait_timeout = (unsigned long )(jiffies + (unsigned long volatile   )tmp);
  dma_async_issue_pending(chan);
  }
  {
  while (1) {
    while_22_continue: /* CIL Label */ ;
    {
    status = dma_async_is_tx_complete(chan, cookie, (dma_cookie_t *)((void *)0), (dma_cookie_t *)((void *)0));
    }
    if ((long )jiffies - (long )dma_sync_wait_timeout >= 0L) {
      {
      printk("<3>dma_sync_wait_timeout!\n");
      }
      return (2);
    }
    if (! ((int )status == 1)) {
      goto while_22_break;
    }
  }
  while_22_break: /* CIL Label */ ;
  }
  return (status);
}
}
extern void *__crc_dma_sync_wait  __attribute__((__weak__)) ;
static unsigned long const   __kcrctab_dma_sync_wait  __attribute__((__used__, __unused__,
__section__("__kcrctab")))  =    (unsigned long const   )((unsigned long )(& __crc_dma_sync_wait));
static char const   __kstrtab_dma_sync_wait[14]  __attribute__((__section__("__ksymtab_strings"),
__aligned__(1)))  = 
  {      (char const   )'d',      (char const   )'m',      (char const   )'a',      (char const   )'_', 
        (char const   )'s',      (char const   )'y',      (char const   )'n',      (char const   )'c', 
        (char const   )'_',      (char const   )'w',      (char const   )'a',      (char const   )'i', 
        (char const   )'t',      (char const   )'\000'};
static struct kernel_symbol  const  __ksymtab_dma_sync_wait  __attribute__((__used__,
__unused__, __section__("__ksymtab")))  =    {(unsigned long )(& dma_sync_wait), __kstrtab_dma_sync_wait};
void dma_chan_cleanup(struct kref *kref ) 
{ struct dma_chan *chan ;
  struct kref  const  *__mptr ;

  {
  {
  __mptr = (struct kref  const  *)kref;
  chan = (struct dma_chan *)((char *)__mptr - (unsigned int )(& ((struct dma_chan *)0)->refcount));
  (*((chan->device)->device_free_chan_resources))(chan);
  kref_put(& (chan->device)->refcount, & dma_async_device_cleanup);
  }
  return;
}
}
extern void *__crc_dma_chan_cleanup  __attribute__((__weak__)) ;
static unsigned long const   __kcrctab_dma_chan_cleanup  __attribute__((__used__,
__unused__, __section__("__kcrctab")))  =    (unsigned long const   )((unsigned long )(& __crc_dma_chan_cleanup));
static char const   __kstrtab_dma_chan_cleanup[17]  __attribute__((__section__("__ksymtab_strings"),
__aligned__(1)))  = 
  {      (char const   )'d',      (char const   )'m',      (char const   )'a',      (char const   )'_', 
        (char const   )'c',      (char const   )'h',      (char const   )'a',      (char const   )'n', 
        (char const   )'_',      (char const   )'c',      (char const   )'l',      (char const   )'e', 
        (char const   )'a',      (char const   )'n',      (char const   )'u',      (char const   )'p', 
        (char const   )'\000'};
static struct kernel_symbol  const  __ksymtab_dma_chan_cleanup  __attribute__((__used__,
__unused__, __section__("__ksymtab")))  =    {(unsigned long )(& dma_chan_cleanup), __kstrtab_dma_chan_cleanup};
static void dma_chan_free_rcu(struct rcu_head *rcu ) 
{ struct dma_chan *chan ;
  struct rcu_head  const  *__mptr ;
  int bias ;
  int i ;
  struct percpu_data *__p ;
  long tmp ;

  {
  {
  __mptr = (struct rcu_head  const  *)rcu;
  chan = (struct dma_chan *)((char *)__mptr - (unsigned int )(& ((struct dma_chan *)0)->rcu));
  bias = 2147483647;
  i = -1;
  }
  {
  while (1) {
    while_23_continue: /* CIL Label */ ;
    {
    i = __next_cpu(i, (cpumask_t const   *)(& cpu_possible_map));
    }
    if (! (i < 64)) {
      goto while_23_break;
    }
    {
    __p = (struct percpu_data *)(~ ((unsigned long )chan->local));
    tmp = atomic_long_read(& ((struct dma_chan_percpu *)__p->ptrs[i])->refcount.a);
    bias = (int )((long )bias - tmp);
    }
  }
  while_23_break: /* CIL Label */ ;
  }
  {
  atomic_sub(bias, & chan->refcount.refcount);
  kref_put(& chan->refcount, & dma_chan_cleanup);
  }
  return;
}
}
static void dma_chan_release(struct dma_chan *chan ) 
{ 

  {
  {
  atomic_add(2147483647, & chan->refcount.refcount);
  chan->slow_ref = 1;
  call_rcu(& chan->rcu, & dma_chan_free_rcu);
  }
  return;
}
}
static void dma_clients_notify_available(void) 
{ struct dma_client *client ;
  struct list_head  const  *__mptr ;
  struct list_head  const  *__mptr___0 ;

  {
  {
  mutex_lock(& dma_list_mutex);
  __mptr = (struct list_head  const  *)dma_client_list.next;
  client = (struct dma_client *)((char *)__mptr - (unsigned int )(& ((struct dma_client *)0)->global_node));
  }
  {
  while (1) {
    while_24_continue: /* CIL Label */ ;
    {
    prefetch((void const   *)client->global_node.next);
    }
    if (! ((unsigned int )(& client->global_node) != (unsigned int )(& dma_client_list))) {
      goto while_24_break;
    }
    {
    dma_client_chan_alloc(client);
    __mptr___0 = (struct list_head  const  *)client->global_node.next;
    client = (struct dma_client *)((char *)__mptr___0 - (unsigned int )(& ((struct dma_client *)0)->global_node));
    }
  }
  while_24_break: /* CIL Label */ ;
  }
  {
  mutex_unlock(& dma_list_mutex);
  }
  return;
}
}
static void dma_clients_notify_removed(struct dma_chan *chan ) 
{ struct dma_client *client ;
  enum dma_state_client ack ;
  struct list_head  const  *__mptr ;
  struct list_head  const  *__mptr___0 ;

  {
  {
  mutex_lock(& dma_list_mutex);
  __mptr = (struct list_head  const  *)dma_client_list.next;
  client = (struct dma_client *)((char *)__mptr - (unsigned int )(& ((struct dma_client *)0)->global_node));
  }
  {
  while (1) {
    while_25_continue: /* CIL Label */ ;
    {
    prefetch((void const   *)client->global_node.next);
    }
    if (! ((unsigned int )(& client->global_node) != (unsigned int )(& dma_client_list))) {
      goto while_25_break;
    }
    {
    ack = (*(client->event_callback))(client, chan, 3);
    }
    if ((int )ack == 0) {
      {
      dma_chan_put(chan);
      (chan->client_count) --;
      }
    }
    {
    __mptr___0 = (struct list_head  const  *)client->global_node.next;
    client = (struct dma_client *)((char *)__mptr___0 - (unsigned int )(& ((struct dma_client *)0)->global_node));
    }
  }
  while_25_break: /* CIL Label */ ;
  }
  {
  mutex_unlock(& dma_list_mutex);
  }
  return;
}
}
void dma_async_client_register(struct dma_client *client ) 
{ int tmp ;
  int tmp___0 ;
  long tmp___1 ;

  {
  {
  while (1) {
    while_26_continue: /* CIL Label */ ;
    {
    tmp = __dma_has_cap(10, & client->cap_mask);
    }
    if (tmp) {
      if (! client->slave) {
        {
        tmp___0 = 1;
        }
      } else {
        {
        tmp___0 = 0;
        }
      }
    } else {
      {
      tmp___0 = 0;
      }
    }
    {
    tmp___1 = __builtin_expect((long )tmp___0, 0L);
    }
    if (tmp___1) {
      {
      while (1) {
        while_27_continue: /* CIL Label */ ;
        {
        __asm__  volatile   ("1:\tud2\n"
                             ".pushsection __bug_table,\"a\"\n"
                             "2:\t.long 1b, %c0\n"
                             "\t.word %c1, 0\n"
                             "\t.org 2b+%c2\n"
                             ".popsection": : "i" ("./dma/dmaengine.c"), "i" (300),
                             "i" (sizeof(struct bug_entry )));
        }
        {
        while (1) {
          while_28_continue: /* CIL Label */ ;
        }
        while_28_break: /* CIL Label */ ;
        }
        goto while_27_break;
      }
      while_27_break: /* CIL Label */ ;
      }
    }
    goto while_26_break;
  }
  while_26_break: /* CIL Label */ ;
  }
  {
  mutex_lock(& dma_list_mutex);
  list_add_tail(& client->global_node, & dma_client_list);
  mutex_unlock(& dma_list_mutex);
  }
  return;
}
}
extern void *__crc_dma_async_client_register  __attribute__((__weak__)) ;
static unsigned long const   __kcrctab_dma_async_client_register  __attribute__((__used__,
__unused__, __section__("__kcrctab")))  =    (unsigned long const   )((unsigned long )(& __crc_dma_async_client_register));
static char const   __kstrtab_dma_async_client_register[26]  __attribute__((__section__("__ksymtab_strings"),
__aligned__(1)))  = 
  {      (char const   )'d',      (char const   )'m',      (char const   )'a',      (char const   )'_', 
        (char const   )'a',      (char const   )'s',      (char const   )'y',      (char const   )'n', 
        (char const   )'c',      (char const   )'_',      (char const   )'c',      (char const   )'l', 
        (char const   )'i',      (char const   )'e',      (char const   )'n',      (char const   )'t', 
        (char const   )'_',      (char const   )'r',      (char const   )'e',      (char const   )'g', 
        (char const   )'i',      (char const   )'s',      (char const   )'t',      (char const   )'e', 
        (char const   )'r',      (char const   )'\000'};
static struct kernel_symbol  const  __ksymtab_dma_async_client_register  __attribute__((__used__,
__unused__, __section__("__ksymtab")))  =    {(unsigned long )(& dma_async_client_register), __kstrtab_dma_async_client_register};
void dma_async_client_unregister(struct dma_client *client ) 
{ struct dma_device *device ;
  struct dma_chan *chan ;
  enum dma_state_client ack ;
  struct list_head  const  *__mptr ;
  struct list_head  const  *__mptr___0 ;
  struct list_head  const  *__mptr___1 ;
  struct list_head  const  *__mptr___2 ;

  {
  if (! client) {
    return;
  }
  {
  mutex_lock(& dma_list_mutex);
  __mptr = (struct list_head  const  *)dma_device_list.next;
  device = (struct dma_device *)((char *)__mptr - (unsigned int )(& ((struct dma_device *)0)->global_node));
  }
  {
  while (1) {
    while_29_continue: /* CIL Label */ ;
    {
    prefetch((void const   *)device->global_node.next);
    }
    if (! ((unsigned int )(& device->global_node) != (unsigned int )(& dma_device_list))) {
      goto while_29_break;
    }
    {
    __mptr___1 = (struct list_head  const  *)device->channels.next;
    chan = (struct dma_chan *)((char *)__mptr___1 - (unsigned int )(& ((struct dma_chan *)0)->device_node));
    }
    {
    while (1) {
      while_30_continue: /* CIL Label */ ;
      {
      prefetch((void const   *)chan->device_node.next);
      }
      if (! ((unsigned int )(& chan->device_node) != (unsigned int )(& device->channels))) {
        goto while_30_break;
      }
      {
      ack = (*(client->event_callback))(client, chan, 3);
      }
      if ((int )ack == 0) {
        {
        dma_chan_put(chan);
        (chan->client_count) --;
        }
      }
      {
      __mptr___2 = (struct list_head  const  *)chan->device_node.next;
      chan = (struct dma_chan *)((char *)__mptr___2 - (unsigned int )(& ((struct dma_chan *)0)->device_node));
      }
    }
    while_30_break: /* CIL Label */ ;
    }
    {
    __mptr___0 = (struct list_head  const  *)device->global_node.next;
    device = (struct dma_device *)((char *)__mptr___0 - (unsigned int )(& ((struct dma_device *)0)->global_node));
    }
  }
  while_29_break: /* CIL Label */ ;
  }
  {
  list_del(& client->global_node);
  mutex_unlock(& dma_list_mutex);
  }
  return;
}
}
extern void *__crc_dma_async_client_unregister  __attribute__((__weak__)) ;
static unsigned long const   __kcrctab_dma_async_client_unregister  __attribute__((__used__,
__unused__, __section__("__kcrctab")))  =    (unsigned long const   )((unsigned long )(& __crc_dma_async_client_unregister));
static char const   __kstrtab_dma_async_client_unregister[28]  __attribute__((__section__("__ksymtab_strings"),
__aligned__(1)))  = 
  {      (char const   )'d',      (char const   )'m',      (char const   )'a',      (char const   )'_', 
        (char const   )'a',      (char const   )'s',      (char const   )'y',      (char const   )'n', 
        (char const   )'c',      (char const   )'_',      (char const   )'c',      (char const   )'l', 
        (char const   )'i',      (char const   )'e',      (char const   )'n',      (char const   )'t', 
        (char const   )'_',      (char const   )'u',      (char const   )'n',      (char const   )'r', 
        (char const   )'e',      (char const   )'g',      (char const   )'i',      (char const   )'s', 
        (char const   )'t',      (char const   )'e',      (char const   )'r',      (char const   )'\000'};
static struct kernel_symbol  const  __ksymtab_dma_async_client_unregister  __attribute__((__used__,
__unused__, __section__("__ksymtab")))  =    {(unsigned long )(& dma_async_client_unregister), __kstrtab_dma_async_client_unregister};
void dma_async_client_chan_request(struct dma_client *client ) 
{ 

  {
  {
  mutex_lock(& dma_list_mutex);
  dma_client_chan_alloc(client);
  mutex_unlock(& dma_list_mutex);
  }
  return;
}
}
extern void *__crc_dma_async_client_chan_request  __attribute__((__weak__)) ;
static unsigned long const   __kcrctab_dma_async_client_chan_request  __attribute__((__used__,
__unused__, __section__("__kcrctab")))  =    (unsigned long const   )((unsigned long )(& __crc_dma_async_client_chan_request));
static char const   __kstrtab_dma_async_client_chan_request[30]  __attribute__((__section__("__ksymtab_strings"),
__aligned__(1)))  = 
  {      (char const   )'d',      (char const   )'m',      (char const   )'a',      (char const   )'_', 
        (char const   )'a',      (char const   )'s',      (char const   )'y',      (char const   )'n', 
        (char const   )'c',      (char const   )'_',      (char const   )'c',      (char const   )'l', 
        (char const   )'i',      (char const   )'e',      (char const   )'n',      (char const   )'t', 
        (char const   )'_',      (char const   )'c',      (char const   )'h',      (char const   )'a', 
        (char const   )'n',      (char const   )'_',      (char const   )'r',      (char const   )'e', 
        (char const   )'q',      (char const   )'u',      (char const   )'e',      (char const   )'s', 
        (char const   )'t',      (char const   )'\000'};
static struct kernel_symbol  const  __ksymtab_dma_async_client_chan_request  __attribute__((__used__,
__unused__, __section__("__ksymtab")))  =    {(unsigned long )(& dma_async_client_chan_request), __kstrtab_dma_async_client_chan_request};
static int id  ;
int dma_async_device_register(struct dma_device *device ) 
{ int chancnt ;
  int rc ;
  struct dma_chan *chan ;
  int tmp ;
  int tmp___0 ;
  long tmp___1 ;
  int tmp___2 ;
  int tmp___3 ;
  long tmp___4 ;
  int tmp___5 ;
  int tmp___6 ;
  long tmp___7 ;
  int tmp___8 ;
  int tmp___9 ;
  long tmp___10 ;
  int tmp___11 ;
  int tmp___12 ;
  long tmp___13 ;
  int tmp___14 ;
  int tmp___15 ;
  long tmp___16 ;
  int tmp___17 ;
  int tmp___18 ;
  long tmp___19 ;
  long tmp___20 ;
  long tmp___21 ;
  long tmp___22 ;
  long tmp___23 ;
  long tmp___24 ;
  int tmp___25 ;
  struct list_head  const  *__mptr ;
  struct list_head  const  *__mptr___0 ;
  void *tmp___26 ;
  int tmp___27 ;
  struct list_head  const  *__mptr___1 ;
  struct list_head  const  *__mptr___2 ;

  {
  {
  chancnt = 0;
  }
  if (! device) {
    return (-19);
  }
  {
  while (1) {
    while_31_continue: /* CIL Label */ ;
    {
    tmp = __dma_has_cap(0, & device->cap_mask);
    }
    if (tmp) {
      if (! device->device_prep_dma_memcpy) {
        {
        tmp___0 = 1;
        }
      } else {
        {
        tmp___0 = 0;
        }
      }
    } else {
      {
      tmp___0 = 0;
      }
    }
    {
    tmp___1 = __builtin_expect((long )tmp___0, 0L);
    }
    if (tmp___1) {
      {
      while (1) {
        while_32_continue: /* CIL Label */ ;
        {
        __asm__  volatile   ("1:\tud2\n"
                             ".pushsection __bug_table,\"a\"\n"
                             "2:\t.long 1b, %c0\n"
                             "\t.word %c1, 0\n"
                             "\t.org 2b+%c2\n"
                             ".popsection": : "i" ("./dma/dmaengine.c"), "i" (369),
                             "i" (sizeof(struct bug_entry )));
        }
        {
        while (1) {
          while_33_continue: /* CIL Label */ ;
        }
        while_33_break: /* CIL Label */ ;
        }
        goto while_32_break;
      }
      while_32_break: /* CIL Label */ ;
      }
    }
    goto while_31_break;
  }
  while_31_break: /* CIL Label */ ;
  }
  {
  while (1) {
    while_34_continue: /* CIL Label */ ;
    {
    tmp___2 = __dma_has_cap(1, & device->cap_mask);
    }
    if (tmp___2) {
      if (! device->device_prep_dma_xor) {
        {
        tmp___3 = 1;
        }
      } else {
        {
        tmp___3 = 0;
        }
      }
    } else {
      {
      tmp___3 = 0;
      }
    }
    {
    tmp___4 = __builtin_expect((long )tmp___3, 0L);
    }
    if (tmp___4) {
      {
      while (1) {
        while_35_continue: /* CIL Label */ ;
        {
        __asm__  volatile   ("1:\tud2\n"
                             ".pushsection __bug_table,\"a\"\n"
                             "2:\t.long 1b, %c0\n"
                             "\t.word %c1, 0\n"
                             "\t.org 2b+%c2\n"
                             ".popsection": : "i" ("./dma/dmaengine.c"), "i" (371),
                             "i" (sizeof(struct bug_entry )));
        }
        {
        while (1) {
          while_36_continue: /* CIL Label */ ;
        }
        while_36_break: /* CIL Label */ ;
        }
        goto while_35_break;
      }
      while_35_break: /* CIL Label */ ;
      }
    }
    goto while_34_break;
  }
  while_34_break: /* CIL Label */ ;
  }
  {
  while (1) {
    while_37_continue: /* CIL Label */ ;
    {
    tmp___5 = __dma_has_cap(5, & device->cap_mask);
    }
    if (tmp___5) {
      if (! device->device_prep_dma_zero_sum) {
        {
        tmp___6 = 1;
        }
      } else {
        {
        tmp___6 = 0;
        }
      }
    } else {
      {
      tmp___6 = 0;
      }
    }
    {
    tmp___7 = __builtin_expect((long )tmp___6, 0L);
    }
    if (tmp___7) {
      {
      while (1) {
        while_38_continue: /* CIL Label */ ;
        {
        __asm__  volatile   ("1:\tud2\n"
                             ".pushsection __bug_table,\"a\"\n"
                             "2:\t.long 1b, %c0\n"
                             "\t.word %c1, 0\n"
                             "\t.org 2b+%c2\n"
                             ".popsection": : "i" ("./dma/dmaengine.c"), "i" (373),
                             "i" (sizeof(struct bug_entry )));
        }
        {
        while (1) {
          while_39_continue: /* CIL Label */ ;
        }
        while_39_break: /* CIL Label */ ;
        }
        goto while_38_break;
      }
      while_38_break: /* CIL Label */ ;
      }
    }
    goto while_37_break;
  }
  while_37_break: /* CIL Label */ ;
  }
  {
  while (1) {
    while_40_continue: /* CIL Label */ ;
    {
    tmp___8 = __dma_has_cap(7, & device->cap_mask);
    }
    if (tmp___8) {
      if (! device->device_prep_dma_memset) {
        {
        tmp___9 = 1;
        }
      } else {
        {
        tmp___9 = 0;
        }
      }
    } else {
      {
      tmp___9 = 0;
      }
    }
    {
    tmp___10 = __builtin_expect((long )tmp___9, 0L);
    }
    if (tmp___10) {
      {
      while (1) {
        while_41_continue: /* CIL Label */ ;
        {
        __asm__  volatile   ("1:\tud2\n"
                             ".pushsection __bug_table,\"a\"\n"
                             "2:\t.long 1b, %c0\n"
                             "\t.word %c1, 0\n"
                             "\t.org 2b+%c2\n"
                             ".popsection": : "i" ("./dma/dmaengine.c"), "i" (375),
                             "i" (sizeof(struct bug_entry )));
        }
        {
        while (1) {
          while_42_continue: /* CIL Label */ ;
        }
        while_42_break: /* CIL Label */ ;
        }
        goto while_41_break;
      }
      while_41_break: /* CIL Label */ ;
      }
    }
    goto while_40_break;
  }
  while_40_break: /* CIL Label */ ;
  }
  {
  while (1) {
    while_43_continue: /* CIL Label */ ;
    {
    tmp___11 = __dma_has_cap(9, & device->cap_mask);
    }
    if (tmp___11) {
      if (! device->device_prep_dma_interrupt) {
        {
        tmp___12 = 1;
        }
      } else {
        {
        tmp___12 = 0;
        }
      }
    } else {
      {
      tmp___12 = 0;
      }
    }
    {
    tmp___13 = __builtin_expect((long )tmp___12, 0L);
    }
    if (tmp___13) {
      {
      while (1) {
        while_44_continue: /* CIL Label */ ;
        {
        __asm__  volatile   ("1:\tud2\n"
                             ".pushsection __bug_table,\"a\"\n"
                             "2:\t.long 1b, %c0\n"
                             "\t.word %c1, 0\n"
                             "\t.org 2b+%c2\n"
                             ".popsection": : "i" ("./dma/dmaengine.c"), "i" (377),
                             "i" (sizeof(struct bug_entry )));
        }
        {
        while (1) {
          while_45_continue: /* CIL Label */ ;
        }
        while_45_break: /* CIL Label */ ;
        }
        goto while_44_break;
      }
      while_44_break: /* CIL Label */ ;
      }
    }
    goto while_43_break;
  }
  while_43_break: /* CIL Label */ ;
  }
  {
  while (1) {
    while_46_continue: /* CIL Label */ ;
    {
    tmp___14 = __dma_has_cap(10, & device->cap_mask);
    }
    if (tmp___14) {
      if (! device->device_prep_slave_sg) {
        {
        tmp___15 = 1;
        }
      } else {
        {
        tmp___15 = 0;
        }
      }
    } else {
      {
      tmp___15 = 0;
      }
    }
    {
    tmp___16 = __builtin_expect((long )tmp___15, 0L);
    }
    if (tmp___16) {
      {
      while (1) {
        while_47_continue: /* CIL Label */ ;
        {
        __asm__  volatile   ("1:\tud2\n"
                             ".pushsection __bug_table,\"a\"\n"
                             "2:\t.long 1b, %c0\n"
                             "\t.word %c1, 0\n"
                             "\t.org 2b+%c2\n"
                             ".popsection": : "i" ("./dma/dmaengine.c"), "i" (379),
                             "i" (sizeof(struct bug_entry )));
        }
        {
        while (1) {
          while_48_continue: /* CIL Label */ ;
        }
        while_48_break: /* CIL Label */ ;
        }
        goto while_47_break;
      }
      while_47_break: /* CIL Label */ ;
      }
    }
    goto while_46_break;
  }
  while_46_break: /* CIL Label */ ;
  }
  {
  while (1) {
    while_49_continue: /* CIL Label */ ;
    {
    tmp___17 = __dma_has_cap(10, & device->cap_mask);
    }
    if (tmp___17) {
      if (! device->device_terminate_all) {
        {
        tmp___18 = 1;
        }
      } else {
        {
        tmp___18 = 0;
        }
      }
    } else {
      {
      tmp___18 = 0;
      }
    }
    {
    tmp___19 = __builtin_expect((long )tmp___18, 0L);
    }
    if (tmp___19) {
      {
      while (1) {
        while_50_continue: /* CIL Label */ ;
        {
        __asm__  volatile   ("1:\tud2\n"
                             ".pushsection __bug_table,\"a\"\n"
                             "2:\t.long 1b, %c0\n"
                             "\t.word %c1, 0\n"
                             "\t.org 2b+%c2\n"
                             ".popsection": : "i" ("./dma/dmaengine.c"), "i" (381),
                             "i" (sizeof(struct bug_entry )));
        }
        {
        while (1) {
          while_51_continue: /* CIL Label */ ;
        }
        while_51_break: /* CIL Label */ ;
        }
        goto while_50_break;
      }
      while_50_break: /* CIL Label */ ;
      }
    }
    goto while_49_break;
  }
  while_49_break: /* CIL Label */ ;
  }
  {
  while (1) {
    while_52_continue: /* CIL Label */ ;
    {
    tmp___20 = __builtin_expect((long )(! (! (! device->device_alloc_chan_resources))),
                                0L);
    }
    if (tmp___20) {
      {
      while (1) {
        while_53_continue: /* CIL Label */ ;
        {
        __asm__  volatile   ("1:\tud2\n"
                             ".pushsection __bug_table,\"a\"\n"
                             "2:\t.long 1b, %c0\n"
                             "\t.word %c1, 0\n"
                             "\t.org 2b+%c2\n"
                             ".popsection": : "i" ("./dma/dmaengine.c"), "i" (383),
                             "i" (sizeof(struct bug_entry )));
        }
        {
        while (1) {
          while_54_continue: /* CIL Label */ ;
        }
        while_54_break: /* CIL Label */ ;
        }
        goto while_53_break;
      }
      while_53_break: /* CIL Label */ ;
      }
    }
    goto while_52_break;
  }
  while_52_break: /* CIL Label */ ;
  }
  {
  while (1) {
    while_55_continue: /* CIL Label */ ;
    {
    tmp___21 = __builtin_expect((long )(! (! (! device->device_free_chan_resources))),
                                0L);
    }
    if (tmp___21) {
      {
      while (1) {
        while_56_continue: /* CIL Label */ ;
        {
        __asm__  volatile   ("1:\tud2\n"
                             ".pushsection __bug_table,\"a\"\n"
                             "2:\t.long 1b, %c0\n"
                             "\t.word %c1, 0\n"
                             "\t.org 2b+%c2\n"
                             ".popsection": : "i" ("./dma/dmaengine.c"), "i" (384),
                             "i" (sizeof(struct bug_entry )));
        }
        {
        while (1) {
          while_57_continue: /* CIL Label */ ;
        }
        while_57_break: /* CIL Label */ ;
        }
        goto while_56_break;
      }
      while_56_break: /* CIL Label */ ;
      }
    }
    goto while_55_break;
  }
  while_55_break: /* CIL Label */ ;
  }
  {
  while (1) {
    while_58_continue: /* CIL Label */ ;
    {
    tmp___22 = __builtin_expect((long )(! (! (! device->device_is_tx_complete))),
                                0L);
    }
    if (tmp___22) {
      {
      while (1) {
        while_59_continue: /* CIL Label */ ;
        {
        __asm__  volatile   ("1:\tud2\n"
                             ".pushsection __bug_table,\"a\"\n"
                             "2:\t.long 1b, %c0\n"
                             "\t.word %c1, 0\n"
                             "\t.org 2b+%c2\n"
                             ".popsection": : "i" ("./dma/dmaengine.c"), "i" (385),
                             "i" (sizeof(struct bug_entry )));
        }
        {
        while (1) {
          while_60_continue: /* CIL Label */ ;
        }
        while_60_break: /* CIL Label */ ;
        }
        goto while_59_break;
      }
      while_59_break: /* CIL Label */ ;
      }
    }
    goto while_58_break;
  }
  while_58_break: /* CIL Label */ ;
  }
  {
  while (1) {
    while_61_continue: /* CIL Label */ ;
    {
    tmp___23 = __builtin_expect((long )(! (! (! device->device_issue_pending))), 0L);
    }
    if (tmp___23) {
      {
      while (1) {
        while_62_continue: /* CIL Label */ ;
        {
        __asm__  volatile   ("1:\tud2\n"
                             ".pushsection __bug_table,\"a\"\n"
                             "2:\t.long 1b, %c0\n"
                             "\t.word %c1, 0\n"
                             "\t.org 2b+%c2\n"
                             ".popsection": : "i" ("./dma/dmaengine.c"), "i" (386),
                             "i" (sizeof(struct bug_entry )));
        }
        {
        while (1) {
          while_63_continue: /* CIL Label */ ;
        }
        while_63_break: /* CIL Label */ ;
        }
        goto while_62_break;
      }
      while_62_break: /* CIL Label */ ;
      }
    }
    goto while_61_break;
  }
  while_61_break: /* CIL Label */ ;
  }
  {
  while (1) {
    while_64_continue: /* CIL Label */ ;
    {
    tmp___24 = __builtin_expect((long )(! (! (! device->dev))), 0L);
    }
    if (tmp___24) {
      {
      while (1) {
        while_65_continue: /* CIL Label */ ;
        {
        __asm__  volatile   ("1:\tud2\n"
                             ".pushsection __bug_table,\"a\"\n"
                             "2:\t.long 1b, %c0\n"
                             "\t.word %c1, 0\n"
                             "\t.org 2b+%c2\n"
                             ".popsection": : "i" ("./dma/dmaengine.c"), "i" (387),
                             "i" (sizeof(struct bug_entry )));
        }
        {
        while (1) {
          while_66_continue: /* CIL Label */ ;
        }
        while_66_break: /* CIL Label */ ;
        }
        goto while_65_break;
      }
      while_65_break: /* CIL Label */ ;
      }
    }
    goto while_64_break;
  }
  while_64_break: /* CIL Label */ ;
  }
  {
  init_completion(& device->done);
  kref_init(& device->refcount);
  mutex_lock(& dma_list_mutex);
  tmp___25 = id;
  id ++;
  device->dev_id = tmp___25;
  mutex_unlock(& dma_list_mutex);
  __mptr = (struct list_head  const  *)device->channels.next;
  chan = (struct dma_chan *)((char *)__mptr - (unsigned int )(& ((struct dma_chan *)0)->device_node));
  }
  {
  while (1) {
    while_67_continue: /* CIL Label */ ;
    {
    prefetch((void const   *)chan->device_node.next);
    }
    if (! ((unsigned int )(& chan->device_node) != (unsigned int )(& device->channels))) {
      goto while_67_break;
    }
    {
    tmp___26 = __percpu_alloc_mask(sizeof(struct dma_chan_percpu ), 208U, & cpu_possible_map);
    chan->local = (struct dma_chan_percpu *)tmp___26;
    }
    if ((unsigned int )chan->local == (unsigned int )((void *)0)) {
      goto __Cont;
    }
    {
    tmp___27 = chancnt;
    chancnt ++;
    chan->chan_id = tmp___27;
    chan->dev.class = & dma_devclass;
    chan->dev.parent = device->dev;
    snprintf(chan->dev.bus_id, 20U, "dma%dchan%d", device->dev_id, chan->chan_id);
    rc = device_register(& chan->dev);
    }
    if (rc) {
      {
      chancnt --;
      percpu_free((void *)chan->local);
      chan->local = (struct dma_chan_percpu *)((void *)0);
      }
      goto err_out;
    }
    {
    kref_get(& device->refcount);
    kref_get(& device->refcount);
    kref_init(& chan->refcount);
    chan->client_count = 0;
    chan->slow_ref = 0;
    }
    {
    while (1) {
      while_68_continue: /* CIL Label */ ;
      {
      chan->rcu.next = (struct rcu_head *)((void *)0);
      chan->rcu.func = (void (*)(struct rcu_head *head ))((void *)0);
      }
      goto while_68_break;
    }
    while_68_break: /* CIL Label */ ;
    }
    __Cont: /* CIL Label */ 
    {
    __mptr___0 = (struct list_head  const  *)chan->device_node.next;
    chan = (struct dma_chan *)((char *)__mptr___0 - (unsigned int )(& ((struct dma_chan *)0)->device_node));
    }
  }
  while_67_break: /* CIL Label */ ;
  }
  {
  mutex_lock(& dma_list_mutex);
  list_add_tail(& device->global_node, & dma_device_list);
  mutex_unlock(& dma_list_mutex);
  dma_clients_notify_available();
  }
  return (0);
  err_out: 
  {
  __mptr___1 = (struct list_head  const  *)device->channels.next;
  chan = (struct dma_chan *)((char *)__mptr___1 - (unsigned int )(& ((struct dma_chan *)0)->device_node));
  }
  {
  while (1) {
    while_69_continue: /* CIL Label */ ;
    {
    prefetch((void const   *)chan->device_node.next);
    }
    if (! ((unsigned int )(& chan->device_node) != (unsigned int )(& device->channels))) {
      goto while_69_break;
    }
    if ((unsigned int )chan->local == (unsigned int )((void *)0)) {
      goto __Cont___0;
    }
    {
    kref_put(& device->refcount, & dma_async_device_cleanup);
    device_unregister(& chan->dev);
    chancnt --;
    percpu_free((void *)chan->local);
    }
    __Cont___0: /* CIL Label */ 
    {
    __mptr___2 = (struct list_head  const  *)chan->device_node.next;
    chan = (struct dma_chan *)((char *)__mptr___2 - (unsigned int )(& ((struct dma_chan *)0)->device_node));
    }
  }
  while_69_break: /* CIL Label */ ;
  }
  return (rc);
}
}
extern void *__crc_dma_async_device_register  __attribute__((__weak__)) ;
static unsigned long const   __kcrctab_dma_async_device_register  __attribute__((__used__,
__unused__, __section__("__kcrctab")))  =    (unsigned long const   )((unsigned long )(& __crc_dma_async_device_register));
static char const   __kstrtab_dma_async_device_register[26]  __attribute__((__section__("__ksymtab_strings"),
__aligned__(1)))  = 
  {      (char const   )'d',      (char const   )'m',      (char const   )'a',      (char const   )'_', 
        (char const   )'a',      (char const   )'s',      (char const   )'y',      (char const   )'n', 
        (char const   )'c',      (char const   )'_',      (char const   )'d',      (char const   )'e', 
        (char const   )'v',      (char const   )'i',      (char const   )'c',      (char const   )'e', 
        (char const   )'_',      (char const   )'r',      (char const   )'e',      (char const   )'g', 
        (char const   )'i',      (char const   )'s',      (char const   )'t',      (char const   )'e', 
        (char const   )'r',      (char const   )'\000'};
static struct kernel_symbol  const  __ksymtab_dma_async_device_register  __attribute__((__used__,
__unused__, __section__("__ksymtab")))  =    {(unsigned long )(& dma_async_device_register), __kstrtab_dma_async_device_register};
static void dma_async_device_cleanup(struct kref *kref ) 
{ struct dma_device *device ;
  struct kref  const  *__mptr ;

  {
  {
  __mptr = (struct kref  const  *)kref;
  device = (struct dma_device *)((char *)__mptr - (unsigned int )(& ((struct dma_device *)0)->refcount));
  complete(& device->done);
  }
  return;
}
}
void dma_async_device_unregister(struct dma_device *device ) 
{ struct dma_chan *chan ;
  struct list_head  const  *__mptr ;
  struct list_head  const  *__mptr___0 ;

  {
  {
  mutex_lock(& dma_list_mutex);
  list_del(& device->global_node);
  mutex_unlock(& dma_list_mutex);
  __mptr = (struct list_head  const  *)device->channels.next;
  chan = (struct dma_chan *)((char *)__mptr - (unsigned int )(& ((struct dma_chan *)0)->device_node));
  }
  {
  while (1) {
    while_70_continue: /* CIL Label */ ;
    {
    prefetch((void const   *)chan->device_node.next);
    }
    if (! ((unsigned int )(& chan->device_node) != (unsigned int )(& device->channels))) {
      goto while_70_break;
    }
    {
    dma_clients_notify_removed(chan);
    device_unregister(& chan->dev);
    dma_chan_release(chan);
    __mptr___0 = (struct list_head  const  *)chan->device_node.next;
    chan = (struct dma_chan *)((char *)__mptr___0 - (unsigned int )(& ((struct dma_chan *)0)->device_node));
    }
  }
  while_70_break: /* CIL Label */ ;
  }
  {
  kref_put(& device->refcount, & dma_async_device_cleanup);
  wait_for_completion(& device->done);
  }
  return;
}
}
extern void *__crc_dma_async_device_unregister  __attribute__((__weak__)) ;
static unsigned long const   __kcrctab_dma_async_device_unregister  __attribute__((__used__,
__unused__, __section__("__kcrctab")))  =    (unsigned long const   )((unsigned long )(& __crc_dma_async_device_unregister));
static char const   __kstrtab_dma_async_device_unregister[28]  __attribute__((__section__("__ksymtab_strings"),
__aligned__(1)))  = 
  {      (char const   )'d',      (char const   )'m',      (char const   )'a',      (char const   )'_', 
        (char const   )'a',      (char const   )'s',      (char const   )'y',      (char const   )'n', 
        (char const   )'c',      (char const   )'_',      (char const   )'d',      (char const   )'e', 
        (char const   )'v',      (char const   )'i',      (char const   )'c',      (char const   )'e', 
        (char const   )'_',      (char const   )'u',      (char const   )'n',      (char const   )'r', 
        (char const   )'e',      (char const   )'g',      (char const   )'i',      (char const   )'s', 
        (char const   )'t',      (char const   )'e',      (char const   )'r',      (char const   )'\000'};
static struct kernel_symbol  const  __ksymtab_dma_async_device_unregister  __attribute__((__used__,
__unused__, __section__("__ksymtab")))  =    {(unsigned long )(& dma_async_device_unregister), __kstrtab_dma_async_device_unregister};
dma_cookie_t dma_async_memcpy_buf_to_buf(struct dma_chan *chan ,
                                         void *dest , void *src , size_t len ) 
{ struct dma_device *dev ;
  struct dma_async_tx_descriptor *tx ;
  dma_addr_t dma_dest ;
  dma_addr_t dma_src ;
  dma_cookie_t cookie ;
  int cpu ;
  int ret__ ;
  struct percpu_data *__p ;
  struct percpu_data *__p___0 ;

  {
  {
  dev = chan->device;
  dma_src = dma_map_single(dev->dev, src, len, 1);
  dma_dest = dma_map_single(dev->dev, dest, len, 2);
  tx = (*(dev->device_prep_dma_memcpy))(chan, dma_dest, dma_src, len, 2UL);
  }
  if (! tx) {
    {
    dma_unmap_single(dev->dev, dma_src, len, 1);
    dma_unmap_single(dev->dev, dma_dest, len, 2);
    }
    return (-12);
  }
  {
  tx->callback = (void (*)(void *dma_async_param ))((void *)0);
  cookie = (*(tx->tx_submit))(tx);
  }
  {
  while (1) {
    while_71_continue: /* CIL Label */ ;
    goto while_71_break;
  }
  while_71_break: /* CIL Label */ ;
  }
  if ((int )sizeof(per_cpu__cpu_number) == 1) {
    goto switch_72_1;
  } else {
    if ((int )sizeof(per_cpu__cpu_number) == 2) {
      goto switch_72_2;
    } else {
      if ((int )sizeof(per_cpu__cpu_number) == 4) {
        goto switch_72_4;
      } else {
        {
        goto switch_72_default;
        if (0) {
          switch_72_1: /* CIL Label */ 
          {
          __asm__  ("mov"
                    "b "
                    "%%fs:"
                    "%1,%0": "=r" (ret__): "m" (per_cpu__cpu_number));
          }
          goto switch_72_break;
          switch_72_2: /* CIL Label */ 
          {
          __asm__  ("mov"
                    "w "
                    "%%fs:"
                    "%1,%0": "=r" (ret__): "m" (per_cpu__cpu_number));
          }
          goto switch_72_break;
          switch_72_4: /* CIL Label */ 
          {
          __asm__  ("mov"
                    "l "
                    "%%fs:"
                    "%1,%0": "=r" (ret__): "m" (per_cpu__cpu_number));
          }
          goto switch_72_break;
          switch_72_default: /* CIL Label */ 
          {
          __bad_percpu_size();
          }
        } else {
          switch_72_break: /* CIL Label */ ;
        }
        }
      }
    }
  }
  {
  cpu = ret__;
  __p = (struct percpu_data *)(~ ((unsigned long )chan->local));
  ((struct dma_chan_percpu *)__p->ptrs[cpu])->bytes_transferred += (unsigned long )len;
  __p___0 = (struct percpu_data *)(~ ((unsigned long )chan->local));
  (((struct dma_chan_percpu *)__p___0->ptrs[cpu])->memcpy_count) ++;
  }
  {
  while (1) {
    while_73_continue: /* CIL Label */ ;
    goto while_73_break;
  }
  while_73_break: /* CIL Label */ ;
  }
  return (cookie);
}
}
extern void *__crc_dma_async_memcpy_buf_to_buf  __attribute__((__weak__)) ;
static unsigned long const   __kcrctab_dma_async_memcpy_buf_to_buf  __attribute__((__used__,
__unused__, __section__("__kcrctab")))  =    (unsigned long const   )((unsigned long )(& __crc_dma_async_memcpy_buf_to_buf));
static char const   __kstrtab_dma_async_memcpy_buf_to_buf[28]  __attribute__((__section__("__ksymtab_strings"),
__aligned__(1)))  = 
  {      (char const   )'d',      (char const   )'m',      (char const   )'a',      (char const   )'_', 
        (char const   )'a',      (char const   )'s',      (char const   )'y',      (char const   )'n', 
        (char const   )'c',      (char const   )'_',      (char const   )'m',      (char const   )'e', 
        (char const   )'m',      (char const   )'c',      (char const   )'p',      (char const   )'y', 
        (char const   )'_',      (char const   )'b',      (char const   )'u',      (char const   )'f', 
        (char const   )'_',      (char const   )'t',      (char const   )'o',      (char const   )'_', 
        (char const   )'b',      (char const   )'u',      (char const   )'f',      (char const   )'\000'};
static struct kernel_symbol  const  __ksymtab_dma_async_memcpy_buf_to_buf  __attribute__((__used__,
__unused__, __section__("__ksymtab")))  =    {(unsigned long )(& dma_async_memcpy_buf_to_buf), __kstrtab_dma_async_memcpy_buf_to_buf};
dma_cookie_t dma_async_memcpy_buf_to_pg(struct dma_chan *chan ,
                                        struct page *page , unsigned int offset ,
                                        void *kdata , size_t len ) 
{ struct dma_device *dev ;
  struct dma_async_tx_descriptor *tx ;
  dma_addr_t dma_dest ;
  dma_addr_t dma_src ;
  dma_cookie_t cookie ;
  int cpu ;
  int ret__ ;
  struct percpu_data *__p ;
  struct percpu_data *__p___0 ;

  {
  {
  dev = chan->device;
  dma_src = dma_map_single(dev->dev, kdata, len, 1);
  dma_dest = dma_map_page(dev->dev, page, offset, len, 2);
  tx = (*(dev->device_prep_dma_memcpy))(chan, dma_dest, dma_src, len, 2UL);
  }
  if (! tx) {
    {
    dma_unmap_single(dev->dev, dma_src, len, 1);
    dma_unmap_page(dev->dev, dma_dest, len, 2);
    }
    return (-12);
  }
  {
  tx->callback = (void (*)(void *dma_async_param ))((void *)0);
  cookie = (*(tx->tx_submit))(tx);
  }
  {
  while (1) {
    while_74_continue: /* CIL Label */ ;
    goto while_74_break;
  }
  while_74_break: /* CIL Label */ ;
  }
  if ((int )sizeof(per_cpu__cpu_number) == 1) {
    goto switch_75_1;
  } else {
    if ((int )sizeof(per_cpu__cpu_number) == 2) {
      goto switch_75_2;
    } else {
      if ((int )sizeof(per_cpu__cpu_number) == 4) {
        goto switch_75_4;
      } else {
        {
        goto switch_75_default;
        if (0) {
          switch_75_1: /* CIL Label */ 
          {
          __asm__  ("mov"
                    "b "
                    "%%fs:"
                    "%1,%0": "=r" (ret__): "m" (per_cpu__cpu_number));
          }
          goto switch_75_break;
          switch_75_2: /* CIL Label */ 
          {
          __asm__  ("mov"
                    "w "
                    "%%fs:"
                    "%1,%0": "=r" (ret__): "m" (per_cpu__cpu_number));
          }
          goto switch_75_break;
          switch_75_4: /* CIL Label */ 
          {
          __asm__  ("mov"
                    "l "
                    "%%fs:"
                    "%1,%0": "=r" (ret__): "m" (per_cpu__cpu_number));
          }
          goto switch_75_break;
          switch_75_default: /* CIL Label */ 
          {
          __bad_percpu_size();
          }
        } else {
          switch_75_break: /* CIL Label */ ;
        }
        }
      }
    }
  }
  {
  cpu = ret__;
  __p = (struct percpu_data *)(~ ((unsigned long )chan->local));
  ((struct dma_chan_percpu *)__p->ptrs[cpu])->bytes_transferred += (unsigned long )len;
  __p___0 = (struct percpu_data *)(~ ((unsigned long )chan->local));
  (((struct dma_chan_percpu *)__p___0->ptrs[cpu])->memcpy_count) ++;
  }
  {
  while (1) {
    while_76_continue: /* CIL Label */ ;
    goto while_76_break;
  }
  while_76_break: /* CIL Label */ ;
  }
  return (cookie);
}
}
extern void *__crc_dma_async_memcpy_buf_to_pg  __attribute__((__weak__)) ;
static unsigned long const   __kcrctab_dma_async_memcpy_buf_to_pg  __attribute__((__used__,
__unused__, __section__("__kcrctab")))  =    (unsigned long const   )((unsigned long )(& __crc_dma_async_memcpy_buf_to_pg));
static char const   __kstrtab_dma_async_memcpy_buf_to_pg[27]  __attribute__((__section__("__ksymtab_strings"),
__aligned__(1)))  = 
  {      (char const   )'d',      (char const   )'m',      (char const   )'a',      (char const   )'_', 
        (char const   )'a',      (char const   )'s',      (char const   )'y',      (char const   )'n', 
        (char const   )'c',      (char const   )'_',      (char const   )'m',      (char const   )'e', 
        (char const   )'m',      (char const   )'c',      (char const   )'p',      (char const   )'y', 
        (char const   )'_',      (char const   )'b',      (char const   )'u',      (char const   )'f', 
        (char const   )'_',      (char const   )'t',      (char const   )'o',      (char const   )'_', 
        (char const   )'p',      (char const   )'g',      (char const   )'\000'};
static struct kernel_symbol  const  __ksymtab_dma_async_memcpy_buf_to_pg  __attribute__((__used__,
__unused__, __section__("__ksymtab")))  =    {(unsigned long )(& dma_async_memcpy_buf_to_pg), __kstrtab_dma_async_memcpy_buf_to_pg};
dma_cookie_t dma_async_memcpy_pg_to_pg(struct dma_chan *chan ,
                                       struct page *dest_pg , unsigned int dest_off ,
                                       struct page *src_pg , unsigned int src_off ,
                                       size_t len ) 
{ struct dma_device *dev ;
  struct dma_async_tx_descriptor *tx ;
  dma_addr_t dma_dest ;
  dma_addr_t dma_src ;
  dma_cookie_t cookie ;
  int cpu ;
  int ret__ ;
  struct percpu_data *__p ;
  struct percpu_data *__p___0 ;

  {
  {
  dev = chan->device;
  dma_src = dma_map_page(dev->dev, src_pg, src_off, len, 1);
  dma_dest = dma_map_page(dev->dev, dest_pg, dest_off, len, 2);
  tx = (*(dev->device_prep_dma_memcpy))(chan, dma_dest, dma_src, len, 2UL);
  }
  if (! tx) {
    {
    dma_unmap_page(dev->dev, dma_src, len, 1);
    dma_unmap_page(dev->dev, dma_dest, len, 2);
    }
    return (-12);
  }
  {
  tx->callback = (void (*)(void *dma_async_param ))((void *)0);
  cookie = (*(tx->tx_submit))(tx);
  }
  {
  while (1) {
    while_77_continue: /* CIL Label */ ;
    goto while_77_break;
  }
  while_77_break: /* CIL Label */ ;
  }
  if ((int )sizeof(per_cpu__cpu_number) == 1) {
    goto switch_78_1;
  } else {
    if ((int )sizeof(per_cpu__cpu_number) == 2) {
      goto switch_78_2;
    } else {
      if ((int )sizeof(per_cpu__cpu_number) == 4) {
        goto switch_78_4;
      } else {
        {
        goto switch_78_default;
        if (0) {
          switch_78_1: /* CIL Label */ 
          {
          __asm__  ("mov"
                    "b "
                    "%%fs:"
                    "%1,%0": "=r" (ret__): "m" (per_cpu__cpu_number));
          }
          goto switch_78_break;
          switch_78_2: /* CIL Label */ 
          {
          __asm__  ("mov"
                    "w "
                    "%%fs:"
                    "%1,%0": "=r" (ret__): "m" (per_cpu__cpu_number));
          }
          goto switch_78_break;
          switch_78_4: /* CIL Label */ 
          {
          __asm__  ("mov"
                    "l "
                    "%%fs:"
                    "%1,%0": "=r" (ret__): "m" (per_cpu__cpu_number));
          }
          goto switch_78_break;
          switch_78_default: /* CIL Label */ 
          {
          __bad_percpu_size();
          }
        } else {
          switch_78_break: /* CIL Label */ ;
        }
        }
      }
    }
  }
  {
  cpu = ret__;
  __p = (struct percpu_data *)(~ ((unsigned long )chan->local));
  ((struct dma_chan_percpu *)__p->ptrs[cpu])->bytes_transferred += (unsigned long )len;
  __p___0 = (struct percpu_data *)(~ ((unsigned long )chan->local));
  (((struct dma_chan_percpu *)__p___0->ptrs[cpu])->memcpy_count) ++;
  }
  {
  while (1) {
    while_79_continue: /* CIL Label */ ;
    goto while_79_break;
  }
  while_79_break: /* CIL Label */ ;
  }
  return (cookie);
}
}
extern void *__crc_dma_async_memcpy_pg_to_pg  __attribute__((__weak__)) ;
static unsigned long const   __kcrctab_dma_async_memcpy_pg_to_pg  __attribute__((__used__,
__unused__, __section__("__kcrctab")))  =    (unsigned long const   )((unsigned long )(& __crc_dma_async_memcpy_pg_to_pg));
static char const   __kstrtab_dma_async_memcpy_pg_to_pg[26]  __attribute__((__section__("__ksymtab_strings"),
__aligned__(1)))  = 
  {      (char const   )'d',      (char const   )'m',      (char const   )'a',      (char const   )'_', 
        (char const   )'a',      (char const   )'s',      (char const   )'y',      (char const   )'n', 
        (char const   )'c',      (char const   )'_',      (char const   )'m',      (char const   )'e', 
        (char const   )'m',      (char const   )'c',      (char const   )'p',      (char const   )'y', 
        (char const   )'_',      (char const   )'p',      (char const   )'g',      (char const   )'_', 
        (char const   )'t',      (char const   )'o',      (char const   )'_',      (char const   )'p', 
        (char const   )'g',      (char const   )'\000'};
static struct kernel_symbol  const  __ksymtab_dma_async_memcpy_pg_to_pg  __attribute__((__used__,
__unused__, __section__("__ksymtab")))  =    {(unsigned long )(& dma_async_memcpy_pg_to_pg), __kstrtab_dma_async_memcpy_pg_to_pg};
void dma_async_tx_descriptor_init(struct dma_async_tx_descriptor *tx ,
                                  struct dma_chan *chan ) 
{ spinlock_t __constr_expr_0 ;

  {
  {
  tx->chan = chan;
  }
  {
  while (1) {
    while_80_continue: /* CIL Label */ ;
    {
    __constr_expr_0.raw_lock.slock = 0U;
    tx->lock = __constr_expr_0;
    }
    goto while_80_break;
  }
  while_80_break: /* CIL Label */ ;
  }
  return;
}
}
extern void *__crc_dma_async_tx_descriptor_init  __attribute__((__weak__)) ;
static unsigned long const   __kcrctab_dma_async_tx_descriptor_init  __attribute__((__used__,
__unused__, __section__("__kcrctab")))  =    (unsigned long const   )((unsigned long )(& __crc_dma_async_tx_descriptor_init));
static char const   __kstrtab_dma_async_tx_descriptor_init[29]  __attribute__((__section__("__ksymtab_strings"),
__aligned__(1)))  = 
  {      (char const   )'d',      (char const   )'m',      (char const   )'a',      (char const   )'_', 
        (char const   )'a',      (char const   )'s',      (char const   )'y',      (char const   )'n', 
        (char const   )'c',      (char const   )'_',      (char const   )'t',      (char const   )'x', 
        (char const   )'_',      (char const   )'d',      (char const   )'e',      (char const   )'s', 
        (char const   )'c',      (char const   )'r',      (char const   )'i',      (char const   )'p', 
        (char const   )'t',      (char const   )'o',      (char const   )'r',      (char const   )'_', 
        (char const   )'i',      (char const   )'n',      (char const   )'i',      (char const   )'t', 
        (char const   )'\000'};
static struct kernel_symbol  const  __ksymtab_dma_async_tx_descriptor_init  __attribute__((__used__,
__unused__, __section__("__ksymtab")))  =    {(unsigned long )(& dma_async_tx_descriptor_init), __kstrtab_dma_async_tx_descriptor_init};
static struct lock_class_key __key___0  ;
static struct lock_class_key __key___1  ;
static int __attribute__((__cold__))  dma_bus_init(void)  __attribute__((__section__(".init.text"))) ;
static int __attribute__((__cold__))  dma_bus_init(void) 
{ int tmp ;

  {
  {
  while (1) {
    while_81_continue: /* CIL Label */ ;
    {
    __mutex_init(& dma_list_mutex, "&dma_list_mutex", & __key___0);
    }
    goto while_81_break;
  }
  while_81_break: /* CIL Label */ ;
  }
  {
  tmp = __class_register(& dma_devclass, & __key___1);
  }
  return ((int __attribute__((__cold__))  )tmp);
}
}
static int (*__initcall_dma_bus_init4)(void)  __attribute__((__used__, __section__(".initcall4.init")))  =    (int (*)(void))(& dma_bus_init);
