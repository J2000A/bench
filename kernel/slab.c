/* Generated by CIL v. 1.3.6 */
/* print_CIL_Input is true */

typedef unsigned short __kernel_mode_t;
typedef int __kernel_pid_t;
typedef unsigned int __kernel_size_t;
typedef int __kernel_ssize_t;
typedef long __kernel_time_t;
typedef long __kernel_clock_t;
typedef int __kernel_timer_t;
typedef int __kernel_clockid_t;
typedef unsigned int __kernel_uid32_t;
typedef unsigned int __kernel_gid32_t;
typedef long long __kernel_loff_t;
typedef signed char __s8;
typedef unsigned char __u8;
typedef short __s16;
typedef unsigned short __u16;
typedef int __s32;
typedef unsigned int __u32;
typedef long long __s64;
typedef unsigned long long __u64;
typedef signed char s8;
typedef unsigned char u8;
typedef unsigned short u16;
typedef unsigned int u32;
typedef long long s64;
typedef unsigned long long u64;
typedef unsigned short umode_t;
typedef __u32 __kernel_dev_t;
typedef __kernel_dev_t dev_t;
typedef __kernel_mode_t mode_t;
typedef __kernel_pid_t pid_t;
typedef __kernel_timer_t timer_t;
typedef __kernel_clockid_t clockid_t;
typedef __kernel_uid32_t uid_t;
typedef __kernel_gid32_t gid_t;
typedef __kernel_loff_t loff_t;
typedef __kernel_size_t size_t;
typedef __kernel_ssize_t ssize_t;
typedef __kernel_time_t time_t;
typedef __kernel_clock_t clock_t;
typedef u64 sector_t;
typedef u64 blkcnt_t;
typedef unsigned int gfp_t;
struct timespec;
struct timespec;
struct module;
struct module;
typedef unsigned long pteval_t;
typedef unsigned long pgdval_t;
typedef unsigned long pgprotval_t;
union __anonunion_pte_t_10 {
   pteval_t pte ;
   pteval_t pte_low ;
};
typedef union __anonunion_pte_t_10 pte_t;
struct page;
struct __anonstruct_pgd_t_11 {
   pgdval_t pgd ;
};
typedef struct __anonstruct_pgd_t_11 pgd_t;
struct __anonstruct_pgprot_t_12 {
   pgprotval_t pgprot ;
};
typedef struct __anonstruct_pgprot_t_12 pgprot_t;
struct page;
struct __anonstruct_pud_t_13 {
   pgd_t pgd ;
};
typedef struct __anonstruct_pud_t_13 pud_t;
struct mm_struct;
struct mm_struct;
struct __anonstruct_pmd_t_14 {
   pud_t pud ;
};
typedef struct __anonstruct_pmd_t_14 pmd_t;
typedef __builtin_va_list __gnuc_va_list;
typedef __gnuc_va_list va_list;
struct bug_entry {
   unsigned long bug_addr ;
   char const   *file ;
   unsigned short line ;
   unsigned short flags ;
};
struct completion;
struct completion;
struct pid;
struct pid;
struct __anonstruct_cpumask_t_17 {
   unsigned long bits[((64U + 8U * sizeof(long )) - 1U) / (8U * sizeof(long ))] ;
};
typedef struct __anonstruct_cpumask_t_17 cpumask_t;
enum km_type {
    KM_BOUNCE_READ = 0,
    KM_SKB_SUNRPC_DATA = 1,
    KM_SKB_DATA_SOFTIRQ = 2,
    KM_USER0 = 3,
    KM_USER1 = 4,
    KM_BIO_SRC_IRQ = 5,
    KM_BIO_DST_IRQ = 6,
    KM_PTE0 = 7,
    KM_PTE1 = 8,
    KM_IRQ0 = 9,
    KM_IRQ1 = 10,
    KM_SOFTIRQ0 = 11,
    KM_SOFTIRQ1 = 12,
    KM_TYPE_NR = 13
} ;
struct __anonstruct____missing_field_name_19 {
   unsigned int a ;
   unsigned int b ;
};
struct __anonstruct____missing_field_name_20 {
   u16 limit0 ;
   u16 base0 ;
   unsigned int base1 : 8 ;
   unsigned int type : 4 ;
   unsigned int s : 1 ;
   unsigned int dpl : 2 ;
   unsigned int p : 1 ;
   unsigned int limit : 4 ;
   unsigned int avl : 1 ;
   unsigned int l : 1 ;
   unsigned int d : 1 ;
   unsigned int g : 1 ;
   unsigned int base2 : 8 ;
};
union __anonunion____missing_field_name_18 {
   struct __anonstruct____missing_field_name_19 __annonCompField3 ;
   struct __anonstruct____missing_field_name_20 __annonCompField4 ;
};
struct desc_struct {
   union __anonunion____missing_field_name_18 __annonCompField5 ;
} __attribute__((__packed__)) ;
typedef struct desc_struct gate_desc;
struct desc_ptr {
   unsigned short size ;
   unsigned long address ;
} __attribute__((__packed__)) ;
struct page;
struct thread_struct;
struct thread_struct;
struct desc_ptr;
struct tss_struct;
struct tss_struct;
struct mm_struct;
struct desc_struct;
struct pv_init_ops {
   unsigned int (*patch)(u8 type , u16 clobber , void *insnbuf , unsigned long addr ,
                         unsigned int len ) ;
   void (*arch_setup)(void) ;
   char *(*memory_setup)(void) ;
   void (*post_allocator_init)(void) ;
   void (*banner)(void) ;
};
struct pv_lazy_ops {
   void (*enter)(void) ;
   void (*leave)(void) ;
};
struct pv_time_ops {
   void (*time_init)(void) ;
   unsigned long (*get_wallclock)(void) ;
   int (*set_wallclock)(unsigned long  ) ;
   unsigned long long (*sched_clock)(void) ;
   unsigned long (*get_tsc_khz)(void) ;
};
struct pv_cpu_ops {
   unsigned long (*get_debugreg)(int regno ) ;
   void (*set_debugreg)(int regno , unsigned long value ) ;
   void (*clts)(void) ;
   unsigned long (*read_cr0)(void) ;
   void (*write_cr0)(unsigned long  ) ;
   unsigned long (*read_cr4_safe)(void) ;
   unsigned long (*read_cr4)(void) ;
   void (*write_cr4)(unsigned long  ) ;
   void (*load_tr_desc)(void) ;
   void (*load_gdt)(struct desc_ptr  const  * ) ;
   void (*load_idt)(struct desc_ptr  const  * ) ;
   void (*store_gdt)(struct desc_ptr * ) ;
   void (*store_idt)(struct desc_ptr * ) ;
   void (*set_ldt)(void const   *desc , unsigned int entries ) ;
   unsigned long (*store_tr)(void) ;
   void (*load_tls)(struct thread_struct *t , unsigned int cpu ) ;
   void (*write_ldt_entry)(struct desc_struct *ldt , int entrynum , void const   *desc ) ;
   void (*write_gdt_entry)(struct desc_struct * , int entrynum , void const   *desc ,
                           int size ) ;
   void (*write_idt_entry)(gate_desc * , int entrynum , gate_desc const   *gate ) ;
   void (*load_sp0)(struct tss_struct *tss , struct thread_struct *t ) ;
   void (*set_iopl_mask)(unsigned int mask ) ;
   void (*wbinvd)(void) ;
   void (*io_delay)(void) ;
   void (*cpuid)(unsigned int *eax , unsigned int *ebx , unsigned int *ecx , unsigned int *edx ) ;
   u64 (*read_msr)(unsigned int msr , int *err ) ;
   int (*write_msr)(unsigned int msr , unsigned int low , unsigned int high ) ;
   u64 (*read_tsc)(void) ;
   u64 (*read_pmc)(int counter ) ;
   unsigned long long (*read_tscp)(unsigned int *aux ) ;
   void (*irq_enable_sysexit)(void) ;
   void (*usergs_sysret64)(void) ;
   void (*usergs_sysret32)(void) ;
   void (*iret)(void) ;
   void (*swapgs)(void) ;
   struct pv_lazy_ops lazy_mode ;
};
struct pv_irq_ops {
   void (*init_IRQ)(void) ;
   unsigned long (*save_fl)(void) ;
   void (*restore_fl)(unsigned long  ) ;
   void (*irq_disable)(void) ;
   void (*irq_enable)(void) ;
   void (*safe_halt)(void) ;
   void (*halt)(void) ;
};
struct pv_apic_ops {
   void (*apic_write)(unsigned long reg , u32 v ) ;
   u32 (*apic_read)(unsigned long reg ) ;
   void (*setup_boot_clock)(void) ;
   void (*setup_secondary_clock)(void) ;
   void (*startup_ipi_hook)(int phys_apicid , unsigned long start_eip , unsigned long start_esp ) ;
};
struct pv_mmu_ops {
   void (*pagetable_setup_start)(pgd_t *pgd_base ) ;
   void (*pagetable_setup_done)(pgd_t *pgd_base ) ;
   unsigned long (*read_cr2)(void) ;
   void (*write_cr2)(unsigned long  ) ;
   unsigned long (*read_cr3)(void) ;
   void (*write_cr3)(unsigned long  ) ;
   void (*activate_mm)(struct mm_struct *prev , struct mm_struct *next ) ;
   void (*dup_mmap)(struct mm_struct *oldmm , struct mm_struct *mm ) ;
   void (*exit_mmap)(struct mm_struct *mm ) ;
   void (*flush_tlb_user)(void) ;
   void (*flush_tlb_kernel)(void) ;
   void (*flush_tlb_single)(unsigned long addr ) ;
   void (*flush_tlb_others)(cpumask_t const   *cpus , struct mm_struct *mm , unsigned long va ) ;
   int (*pgd_alloc)(struct mm_struct *mm ) ;
   void (*pgd_free)(struct mm_struct *mm , pgd_t *pgd ) ;
   void (*alloc_pte)(struct mm_struct *mm , u32 pfn ) ;
   void (*alloc_pmd)(struct mm_struct *mm , u32 pfn ) ;
   void (*alloc_pmd_clone)(u32 pfn , u32 clonepfn , u32 start , u32 count ) ;
   void (*alloc_pud)(struct mm_struct *mm , u32 pfn ) ;
   void (*release_pte)(u32 pfn ) ;
   void (*release_pmd)(u32 pfn ) ;
   void (*release_pud)(u32 pfn ) ;
   void (*set_pte)(pte_t *ptep , pte_t pteval ) ;
   void (*set_pte_at)(struct mm_struct *mm , unsigned long addr , pte_t *ptep , pte_t pteval ) ;
   void (*set_pmd)(pmd_t *pmdp , pmd_t pmdval ) ;
   void (*pte_update)(struct mm_struct *mm , unsigned long addr , pte_t *ptep ) ;
   void (*pte_update_defer)(struct mm_struct *mm , unsigned long addr , pte_t *ptep ) ;
   pte_t (*ptep_modify_prot_start)(struct mm_struct *mm , unsigned long addr , pte_t *ptep ) ;
   void (*ptep_modify_prot_commit)(struct mm_struct *mm , unsigned long addr , pte_t *ptep ,
                                   pte_t pte ) ;
   pteval_t (*pte_val)(pte_t  ) ;
   pteval_t (*pte_flags)(pte_t  ) ;
   pte_t (*make_pte)(pteval_t pte ) ;
   pgdval_t (*pgd_val)(pgd_t  ) ;
   pgd_t (*make_pgd)(pgdval_t pgd ) ;
   void *(*kmap_atomic_pte)(struct page *page , enum km_type type ) ;
   struct pv_lazy_ops lazy_mode ;
   void (*set_fixmap)(unsigned int idx , unsigned long phys , pgprot_t flags ) ;
};
struct raw_spinlock;
struct raw_spinlock;
struct pv_lock_ops {
   int (*spin_is_locked)(struct raw_spinlock *lock ) ;
   int (*spin_is_contended)(struct raw_spinlock *lock ) ;
   void (*spin_lock)(struct raw_spinlock *lock ) ;
   int (*spin_trylock)(struct raw_spinlock *lock ) ;
   void (*spin_unlock)(struct raw_spinlock *lock ) ;
};
struct paravirt_patch_template {
   struct pv_init_ops pv_init_ops ;
   struct pv_time_ops pv_time_ops ;
   struct pv_cpu_ops pv_cpu_ops ;
   struct pv_irq_ops pv_irq_ops ;
   struct pv_apic_ops pv_apic_ops ;
   struct pv_mmu_ops pv_mmu_ops ;
   struct pv_lock_ops pv_lock_ops ;
};
struct task_struct;
struct task_struct;
struct task_struct;
struct mm_struct;
struct vm86_regs {
   long ebx ;
   long ecx ;
   long edx ;
   long esi ;
   long edi ;
   long ebp ;
   long eax ;
   long __null_ds ;
   long __null_es ;
   long __null_fs ;
   long __null_gs ;
   long orig_eax ;
   long eip ;
   unsigned short cs ;
   unsigned short __csh ;
   long eflags ;
   long esp ;
   unsigned short ss ;
   unsigned short __ssh ;
   unsigned short es ;
   unsigned short __esh ;
   unsigned short ds ;
   unsigned short __dsh ;
   unsigned short fs ;
   unsigned short __fsh ;
   unsigned short gs ;
   unsigned short __gsh ;
};
struct revectored_struct {
   unsigned long __map[8] ;
};
struct vm86_struct {
   struct vm86_regs regs ;
   unsigned long flags ;
   unsigned long screen_bitmap ;
   unsigned long cpu_type ;
   struct revectored_struct int_revectored ;
   struct revectored_struct int21_revectored ;
};
struct obs_kernel_param {
   char const   *str ;
   int (*setup_func)(char * ) ;
   int early ;
};
struct cpuinfo_x86;
struct cpuinfo_x86;
struct task_struct;
struct task_struct;
struct info {
   long ___orig_eip ;
   long ___ebx ;
   long ___ecx ;
   long ___edx ;
   long ___esi ;
   long ___edi ;
   long ___ebp ;
   long ___eax ;
   long ___ds ;
   long ___es ;
   long ___fs ;
   long ___orig_eax ;
   long ___eip ;
   long ___cs ;
   long ___eflags ;
   long ___esp ;
   long ___ss ;
   long ___vm86_es ;
   long ___vm86_ds ;
   long ___vm86_fs ;
   long ___vm86_gs ;
};
struct task_struct;
struct task_struct;
struct cpuinfo_x86 {
   __u8 x86 ;
   __u8 x86_vendor ;
   __u8 x86_model ;
   __u8 x86_mask ;
   char wp_works_ok ;
   char hlt_works_ok ;
   char hard_math ;
   char rfu ;
   char fdiv_bug ;
   char f00f_bug ;
   char coma_bug ;
   char pad0 ;
   int cpuid_level ;
   __u32 x86_capability[8] ;
   char x86_vendor_id[16] ;
   char x86_model_id[64] ;
   int x86_cache_size ;
   int x86_cache_alignment ;
   int x86_power ;
   unsigned long loops_per_jiffy ;
   cpumask_t llc_shared_map ;
   u16 x86_max_cores ;
   u16 apicid ;
   u16 initial_apicid ;
   u16 x86_clflush_size ;
   u16 booted_cores ;
   u16 phys_proc_id ;
   u16 cpu_core_id ;
   u16 cpu_index ;
} __attribute__((__aligned__((1) <<  (7) ))) ;
struct x86_hw_tss {
   unsigned short back_link ;
   unsigned short __blh ;
   unsigned long sp0 ;
   unsigned short ss0 ;
   unsigned short __ss0h ;
   unsigned long sp1 ;
   unsigned short ss1 ;
   unsigned short __ss1h ;
   unsigned long sp2 ;
   unsigned short ss2 ;
   unsigned short __ss2h ;
   unsigned long __cr3 ;
   unsigned long ip ;
   unsigned long flags ;
   unsigned long ax ;
   unsigned long cx ;
   unsigned long dx ;
   unsigned long bx ;
   unsigned long sp ;
   unsigned long bp ;
   unsigned long si ;
   unsigned long di ;
   unsigned short es ;
   unsigned short __esh ;
   unsigned short cs ;
   unsigned short __csh ;
   unsigned short ss ;
   unsigned short __ssh ;
   unsigned short ds ;
   unsigned short __dsh ;
   unsigned short fs ;
   unsigned short __fsh ;
   unsigned short gs ;
   unsigned short __gsh ;
   unsigned short ldt ;
   unsigned short __ldth ;
   unsigned short trace ;
   unsigned short io_bitmap_base ;
} __attribute__((__packed__)) ;
struct tss_struct {
   struct x86_hw_tss x86_tss ;
   unsigned long io_bitmap[8192U / sizeof(long ) + 1U] ;
   unsigned long io_bitmap_max ;
   struct thread_struct *io_bitmap_owner ;
   unsigned long stack[64] ;
} __attribute__((__aligned__((1) <<  (7) ))) ;
struct i387_fsave_struct {
   u32 cwd ;
   u32 swd ;
   u32 twd ;
   u32 fip ;
   u32 fcs ;
   u32 foo ;
   u32 fos ;
   u32 st_space[20] ;
   u32 status ;
};
struct __anonstruct____missing_field_name_28 {
   u64 rip ;
   u64 rdp ;
};
struct __anonstruct____missing_field_name_29 {
   u32 fip ;
   u32 fcs ;
   u32 foo ;
   u32 fos ;
};
union __anonunion____missing_field_name_27 {
   struct __anonstruct____missing_field_name_28 __annonCompField6 ;
   struct __anonstruct____missing_field_name_29 __annonCompField7 ;
};
struct i387_fxsave_struct {
   u16 cwd ;
   u16 swd ;
   u16 twd ;
   u16 fop ;
   union __anonunion____missing_field_name_27 __annonCompField8 ;
   u32 mxcsr ;
   u32 mxcsr_mask ;
   u32 st_space[32] ;
   u32 xmm_space[64] ;
   u32 padding[24] ;
} __attribute__((__aligned__(16))) ;
struct i387_soft_struct {
   u32 cwd ;
   u32 swd ;
   u32 twd ;
   u32 fip ;
   u32 fcs ;
   u32 foo ;
   u32 fos ;
   u32 st_space[20] ;
   u8 ftop ;
   u8 changed ;
   u8 lookahead ;
   u8 no_update ;
   u8 rm ;
   u8 alimit ;
   struct info *info ;
   u32 entry_eip ;
};
union thread_xstate {
   struct i387_fsave_struct fsave ;
   struct i387_fxsave_struct fxsave ;
   struct i387_soft_struct soft ;
};
struct kmem_cache;
struct thread_struct {
   struct desc_struct tls_array[3] ;
   unsigned long sp0 ;
   unsigned long sp ;
   unsigned long sysenter_cs ;
   unsigned long ip ;
   unsigned long fs ;
   unsigned long gs ;
   unsigned long debugreg0 ;
   unsigned long debugreg1 ;
   unsigned long debugreg2 ;
   unsigned long debugreg3 ;
   unsigned long debugreg6 ;
   unsigned long debugreg7 ;
   unsigned long cr2 ;
   unsigned long trap_no ;
   unsigned long error_code ;
   union thread_xstate *xstate ;
   struct vm86_struct *vm86_info ;
   unsigned long screen_bitmap ;
   unsigned long v86flags ;
   unsigned long v86mask ;
   unsigned long saved_sp0 ;
   unsigned int saved_fs ;
   unsigned int saved_gs ;
   unsigned long *io_bitmap_ptr ;
   unsigned long iopl ;
   unsigned int io_bitmap_max ;
   unsigned long debugctlmsr ;
   unsigned long ds_area_msr ;
};
struct list_head {
   struct list_head *next ;
   struct list_head *prev ;
};
struct hlist_node;
struct hlist_head {
   struct hlist_node *first ;
};
struct hlist_node {
   struct hlist_node *next ;
   struct hlist_node **pprev ;
};
struct raw_spinlock {
   unsigned int slock ;
};
typedef struct raw_spinlock raw_spinlock_t;
struct __anonstruct_raw_rwlock_t_31 {
   unsigned int lock ;
};
typedef struct __anonstruct_raw_rwlock_t_31 raw_rwlock_t;
struct task_struct;
struct lock_class_key {

};
struct __anonstruct_spinlock_t_32 {
   raw_spinlock_t raw_lock ;
};
typedef struct __anonstruct_spinlock_t_32 spinlock_t;
struct __anonstruct_rwlock_t_33 {
   raw_rwlock_t raw_lock ;
};
typedef struct __anonstruct_rwlock_t_33 rwlock_t;
struct __anonstruct_atomic_t_34 {
   int counter ;
};
typedef struct __anonstruct_atomic_t_34 atomic_t;
typedef atomic_t atomic_long_t;
struct __wait_queue;
typedef struct __wait_queue wait_queue_t;
struct __wait_queue {
   unsigned int flags ;
   void *private ;
   int (*func)(wait_queue_t *wait , unsigned int mode , int sync , void *key ) ;
   struct list_head task_list ;
};
struct __wait_queue_head {
   spinlock_t lock ;
   struct list_head task_list ;
};
typedef struct __wait_queue_head wait_queue_head_t;
struct task_struct;
struct seqcount {
   unsigned int sequence ;
};
typedef struct seqcount seqcount_t;
struct __anonstruct_nodemask_t_36 {
   unsigned long bits[((1U + 8U * sizeof(long )) - 1U) / (8U * sizeof(long ))] ;
};
typedef struct __anonstruct_nodemask_t_36 nodemask_t;
enum node_states {
    N_POSSIBLE = 0,
    N_ONLINE = 1,
    N_NORMAL_MEMORY = 2,
    N_HIGH_MEMORY = 3,
    N_CPU = 4,
    NR_NODE_STATES = 5
} ;
struct page;
struct free_area {
   struct list_head free_list[5] ;
   unsigned long nr_free ;
};
struct pglist_data;
struct pglist_data;
struct zone_padding {
   char x[0] ;
} __attribute__((__aligned__((1) <<  (7) ))) ;
enum zone_stat_item {
    NR_FREE_PAGES = 0,
    NR_INACTIVE = 1,
    NR_ACTIVE = 2,
    NR_ANON_PAGES = 3,
    NR_FILE_MAPPED = 4,
    NR_FILE_PAGES = 5,
    NR_FILE_DIRTY = 6,
    NR_WRITEBACK = 7,
    NR_SLAB_RECLAIMABLE = 8,
    NR_SLAB_UNRECLAIMABLE = 9,
    NR_PAGETABLE = 10,
    NR_UNSTABLE_NFS = 11,
    NR_BOUNCE = 12,
    NR_VMSCAN_WRITE = 13,
    NR_WRITEBACK_TEMP = 14,
    NR_VM_ZONE_STAT_ITEMS = 15
} ;
struct per_cpu_pages {
   int count ;
   int high ;
   int batch ;
   struct list_head list ;
};
struct per_cpu_pageset {
   struct per_cpu_pages pcp ;
   s8 stat_threshold ;
   s8 vm_stat_diff[15] ;
} __attribute__((__aligned__((1) <<  (7) ))) ;
enum zone_type {
    ZONE_DMA = 0,
    ZONE_NORMAL = 1,
    ZONE_HIGHMEM = 2,
    ZONE_MOVABLE = 3,
    __MAX_NR_ZONES = 4
} ;
struct zone {
   unsigned long pages_min ;
   unsigned long pages_low ;
   unsigned long pages_high ;
   unsigned long lowmem_reserve[4] ;
   struct per_cpu_pageset pageset[64] ;
   spinlock_t lock ;
   struct free_area free_area[11] ;
   unsigned long *pageblock_flags ;
   struct zone_padding _pad1_ ;
   spinlock_t lru_lock ;
   struct list_head active_list ;
   struct list_head inactive_list ;
   unsigned long nr_scan_active ;
   unsigned long nr_scan_inactive ;
   unsigned long pages_scanned ;
   unsigned long flags ;
   atomic_long_t vm_stat[15] ;
   int prev_priority ;
   struct zone_padding _pad2_ ;
   wait_queue_head_t *wait_table ;
   unsigned long wait_table_hash_nr_entries ;
   unsigned long wait_table_bits ;
   struct pglist_data *zone_pgdat ;
   unsigned long zone_start_pfn ;
   unsigned long spanned_pages ;
   unsigned long present_pages ;
   char const   *name ;
} __attribute__((__aligned__((1) <<  (7) ))) ;
struct zonelist_cache;
struct zonelist_cache;
struct zoneref {
   struct zone *zone ;
   int zone_idx ;
};
struct zonelist {
   struct zonelist_cache *zlcache_ptr ;
   struct zoneref _zonerefs[5] ;
};
struct bootmem_data;
struct bootmem_data;
struct pglist_data {
   struct zone node_zones[4] ;
   struct zonelist node_zonelists[1] ;
   int nr_zones ;
   struct page *node_mem_map ;
   struct bootmem_data *bdata ;
   unsigned long node_start_pfn ;
   unsigned long node_present_pages ;
   unsigned long node_spanned_pages ;
   int node_id ;
   wait_queue_head_t kswapd_wait ;
   struct task_struct *kswapd ;
   int kswapd_max_order ;
};
struct mutex {
   atomic_t count ;
   spinlock_t wait_lock ;
   struct list_head wait_list ;
};
struct rw_semaphore;
struct rw_semaphore;
struct rw_semaphore {
   long count ;
   spinlock_t wait_lock ;
   struct list_head wait_list ;
};
struct notifier_block {
   int (*notifier_call)(struct notifier_block * , unsigned long  , void * ) ;
   struct notifier_block *next ;
   int priority ;
};
struct page;
struct zone;
struct pglist_data;
struct file;
struct file;
struct __anonstruct_mm_context_t_38 {
   void *ldt ;
   int size ;
   struct mutex lock ;
   void *vdso ;
};
typedef struct __anonstruct_mm_context_t_38 mm_context_t;
struct vm_area_struct;
struct vm_area_struct;
struct timespec {
   time_t tv_sec ;
   long tv_nsec ;
};
union ktime {
   s64 tv64 ;
};
typedef union ktime ktime_t;
struct tvec_base;
struct tvec_base;
struct timer_list {
   struct list_head entry ;
   unsigned long expires ;
   void (*function)(unsigned long  ) ;
   unsigned long data ;
   struct tvec_base *base ;
   void *start_site ;
   char start_comm[16] ;
   int start_pid ;
};
struct hrtimer;
struct hrtimer;
enum hrtimer_restart;
struct work_struct;
struct work_struct;
struct work_struct {
   atomic_long_t data ;
   struct list_head entry ;
   void (*func)(struct work_struct *work ) ;
};
struct delayed_work {
   struct work_struct work ;
   struct timer_list timer ;
};
struct kobject;
struct kobject;
struct module;
struct attribute {
   char const   *name ;
   struct module *owner ;
   mode_t mode ;
};
struct vm_area_struct;
struct sysfs_ops {
   ssize_t (*show)(struct kobject * , struct attribute * , char * ) ;
   ssize_t (*store)(struct kobject * , struct attribute * , char const   * , size_t  ) ;
};
struct kref {
   atomic_t refcount ;
};
struct kset;
struct kobj_type;
struct sysfs_dirent;
struct kobject {
   char const   *name ;
   struct list_head entry ;
   struct kobject *parent ;
   struct kset *kset ;
   struct kobj_type *ktype ;
   struct sysfs_dirent *sd ;
   struct kref kref ;
   unsigned int state_initialized : 1 ;
   unsigned int state_in_sysfs : 1 ;
   unsigned int state_add_uevent_sent : 1 ;
   unsigned int state_remove_uevent_sent : 1 ;
};
struct kobj_type {
   void (*release)(struct kobject *kobj ) ;
   struct sysfs_ops *sysfs_ops ;
   struct attribute **default_attrs ;
};
struct kobj_uevent_env {
   char *envp[32] ;
   int envp_idx ;
   char buf[2048] ;
   int buflen ;
};
struct kset_uevent_ops {
   int (*filter)(struct kset *kset , struct kobject *kobj ) ;
   char const   *(*name)(struct kset *kset , struct kobject *kobj ) ;
   int (*uevent)(struct kset *kset , struct kobject *kobj , struct kobj_uevent_env *env ) ;
};
struct kset {
   struct list_head list ;
   spinlock_t list_lock ;
   struct kobject kobj ;
   struct kset_uevent_ops *uevent_ops ;
};
struct kmem_cache_cpu {
   void **freelist ;
   struct page *page ;
   int node ;
   unsigned int offset ;
   unsigned int objsize ;
};
struct kmem_cache_node {
   spinlock_t list_lock ;
   unsigned long nr_partial ;
   unsigned long min_partial ;
   struct list_head partial ;
   atomic_long_t nr_slabs ;
   atomic_long_t total_objects ;
   struct list_head full ;
};
struct kmem_cache_order_objects {
   unsigned long x ;
};
struct kmem_cache {
   unsigned long flags ;
   int size ;
   int objsize ;
   int offset ;
   struct kmem_cache_order_objects oo ;
   struct kmem_cache_node local_node ;
   struct kmem_cache_order_objects max ;
   struct kmem_cache_order_objects min ;
   gfp_t allocflags ;
   int refcount ;
   void (*ctor)(void * ) ;
   int inuse ;
   int align ;
   char const   *name ;
   struct list_head list ;
   struct kobject kobj ;
   struct kmem_cache_cpu *cpu_slab[64] ;
};
struct seq_operations;
struct rb_node {
   unsigned long rb_parent_color ;
   struct rb_node *rb_right ;
   struct rb_node *rb_left ;
} __attribute__((__aligned__(sizeof(long )))) ;
struct rb_root {
   struct rb_node *rb_node ;
};
struct prio_tree_node;
struct raw_prio_tree_node {
   struct prio_tree_node *left ;
   struct prio_tree_node *right ;
   struct prio_tree_node *parent ;
};
struct prio_tree_node {
   struct prio_tree_node *left ;
   struct prio_tree_node *right ;
   struct prio_tree_node *parent ;
   unsigned long start ;
   unsigned long last ;
};
struct prio_tree_root {
   struct prio_tree_node *prio_tree_node ;
   unsigned short index_bits ;
   unsigned short raw ;
};
struct task_struct;
struct task_struct;
struct completion {
   unsigned int done ;
   wait_queue_head_t wait ;
};
struct address_space;
struct address_space;
typedef atomic_long_t mm_counter_t;
struct __anonstruct____missing_field_name_94 {
   u16 inuse ;
   u16 objects ;
};
union __anonunion____missing_field_name_93 {
   atomic_t _mapcount ;
   struct __anonstruct____missing_field_name_94 __annonCompField9 ;
};
struct __anonstruct____missing_field_name_96 {
   unsigned long private ;
   struct address_space *mapping ;
};
union __anonunion____missing_field_name_95 {
   struct __anonstruct____missing_field_name_96 __annonCompField11 ;
   spinlock_t ptl ;
   struct kmem_cache *slab ;
   struct page *first_page ;
};
union __anonunion____missing_field_name_97 {
   unsigned long index ;
   void *freelist ;
};
struct page {
   unsigned long flags ;
   atomic_t _count ;
   union __anonunion____missing_field_name_93 __annonCompField10 ;
   union __anonunion____missing_field_name_95 __annonCompField12 ;
   union __anonunion____missing_field_name_97 __annonCompField13 ;
   struct list_head lru ;
   unsigned long page_cgroup ;
};
struct __anonstruct_vm_set_99 {
   struct list_head list ;
   void *parent ;
   struct vm_area_struct *head ;
};
union __anonunion_shared_98 {
   struct __anonstruct_vm_set_99 vm_set ;
   struct raw_prio_tree_node prio_tree_node ;
};
struct anon_vma;
struct vm_operations_struct;
struct vm_area_struct {
   struct mm_struct *vm_mm ;
   unsigned long vm_start ;
   unsigned long vm_end ;
   struct vm_area_struct *vm_next ;
   pgprot_t vm_page_prot ;
   unsigned long vm_flags ;
   struct rb_node vm_rb ;
   union __anonunion_shared_98 shared ;
   struct list_head anon_vma_node ;
   struct anon_vma *anon_vma ;
   struct vm_operations_struct *vm_ops ;
   unsigned long vm_pgoff ;
   struct file *vm_file ;
   void *vm_private_data ;
   unsigned long vm_truncate_count ;
};
struct core_thread {
   struct task_struct *task ;
   struct core_thread *next ;
};
struct core_state {
   atomic_t nr_threads ;
   struct core_thread dumper ;
   struct completion startup ;
};
struct kioctx;
struct mmu_notifier_mm;
struct mm_struct {
   struct vm_area_struct *mmap ;
   struct rb_root mm_rb ;
   struct vm_area_struct *mmap_cache ;
   unsigned long (*get_unmapped_area)(struct file *filp , unsigned long addr , unsigned long len ,
                                      unsigned long pgoff , unsigned long flags ) ;
   void (*unmap_area)(struct mm_struct *mm , unsigned long addr ) ;
   unsigned long mmap_base ;
   unsigned long task_size ;
   unsigned long cached_hole_size ;
   unsigned long free_area_cache ;
   pgd_t *pgd ;
   atomic_t mm_users ;
   atomic_t mm_count ;
   int map_count ;
   struct rw_semaphore mmap_sem ;
   spinlock_t page_table_lock ;
   struct list_head mmlist ;
   mm_counter_t _file_rss ;
   mm_counter_t _anon_rss ;
   unsigned long hiwater_rss ;
   unsigned long hiwater_vm ;
   unsigned long total_vm ;
   unsigned long locked_vm ;
   unsigned long shared_vm ;
   unsigned long exec_vm ;
   unsigned long stack_vm ;
   unsigned long reserved_vm ;
   unsigned long def_flags ;
   unsigned long nr_ptes ;
   unsigned long start_code ;
   unsigned long end_code ;
   unsigned long start_data ;
   unsigned long end_data ;
   unsigned long start_brk ;
   unsigned long brk ;
   unsigned long start_stack ;
   unsigned long arg_start ;
   unsigned long arg_end ;
   unsigned long env_start ;
   unsigned long env_end ;
   unsigned long saved_auxv[40] ;
   cpumask_t cpu_vm_mask ;
   mm_context_t context ;
   unsigned int faultstamp ;
   unsigned int token_priority ;
   unsigned int last_interval ;
   unsigned long flags ;
   struct core_state *core_state ;
   rwlock_t ioctx_list_lock ;
   struct kioctx *ioctx_list ;
   struct task_struct *owner ;
   struct file *exe_file ;
   unsigned long num_exe_file_vmas ;
   struct mmu_notifier_mm *mmu_notifier_mm ;
};
struct anon_vma;
struct file_ra_state;
struct file_ra_state;
struct user_struct;
struct user_struct;
struct writeback_control;
struct writeback_control;
struct file;
struct mm_struct;
struct vm_area_struct;
struct vm_area_struct;
struct vm_fault {
   unsigned int flags ;
   unsigned long pgoff ;
   void *virtual_address ;
   struct page *page ;
};
struct vm_operations_struct {
   void (*open)(struct vm_area_struct *area ) ;
   void (*close)(struct vm_area_struct *area ) ;
   int (*fault)(struct vm_area_struct *vma , struct vm_fault *vmf ) ;
   int (*page_mkwrite)(struct vm_area_struct *vma , struct page *page ) ;
   int (*access)(struct vm_area_struct *vma , unsigned long addr , void *buf , int len ,
                 int write ) ;
};
struct inode;
struct inode;
struct page;
struct page;
struct mm_struct;
struct task_struct;
struct kernel_cap_struct {
   __u32 cap[2] ;
};
typedef struct kernel_cap_struct kernel_cap_t;
typedef unsigned long cputime_t;
struct rcu_head {
   struct rcu_head *next ;
   void (*func)(struct rcu_head *head ) ;
};
struct task_struct;
struct sem_undo_list;
struct sem_undo_list {
   atomic_t refcnt ;
   spinlock_t lock ;
   struct list_head list_proc ;
};
struct sysv_sem {
   struct sem_undo_list *undo_list ;
};
struct siginfo;
struct siginfo;
struct __anonstruct_sigset_t_102 {
   unsigned long sig[2] ;
};
typedef struct __anonstruct_sigset_t_102 sigset_t;
typedef void __signalfn_t(int  );
typedef __signalfn_t *__sighandler_t;
typedef void __restorefn_t(void);
typedef __restorefn_t *__sigrestore_t;
struct sigaction {
   __sighandler_t sa_handler ;
   unsigned long sa_flags ;
   __sigrestore_t sa_restorer ;
   sigset_t sa_mask ;
};
struct k_sigaction {
   struct sigaction sa ;
};
union sigval {
   int sival_int ;
   void *sival_ptr ;
};
typedef union sigval sigval_t;
struct __anonstruct__kill_104 {
   pid_t _pid ;
   uid_t _uid ;
};
struct __anonstruct__timer_105 {
   timer_t _tid ;
   int _overrun ;
   char _pad[sizeof(uid_t ) - sizeof(int )] ;
   sigval_t _sigval ;
   int _sys_private ;
};
struct __anonstruct__rt_106 {
   pid_t _pid ;
   uid_t _uid ;
   sigval_t _sigval ;
};
struct __anonstruct__sigchld_107 {
   pid_t _pid ;
   uid_t _uid ;
   int _status ;
   clock_t _utime ;
   clock_t _stime ;
};
struct __anonstruct__sigfault_108 {
   void *_addr ;
};
struct __anonstruct__sigpoll_109 {
   long _band ;
   int _fd ;
};
union __anonunion__sifields_103 {
   int _pad[(128U - 3U * sizeof(int )) / sizeof(int )] ;
   struct __anonstruct__kill_104 _kill ;
   struct __anonstruct__timer_105 _timer ;
   struct __anonstruct__rt_106 _rt ;
   struct __anonstruct__sigchld_107 _sigchld ;
   struct __anonstruct__sigfault_108 _sigfault ;
   struct __anonstruct__sigpoll_109 _sigpoll ;
};
struct siginfo {
   int si_signo ;
   int si_errno ;
   int si_code ;
   union __anonunion__sifields_103 _sifields ;
};
typedef struct siginfo siginfo_t;
struct siginfo;
struct sigpending {
   struct list_head list ;
   sigset_t signal ;
};
struct dentry;
struct dentry;
struct vfsmount;
struct vfsmount;
struct path {
   struct vfsmount *mnt ;
   struct dentry *dentry ;
};
struct fs_struct {
   atomic_t count ;
   rwlock_t lock ;
   int umask ;
   struct path root ;
   struct path pwd ;
};
enum pid_type {
    PIDTYPE_PID = 0,
    PIDTYPE_PGID = 1,
    PIDTYPE_SID = 2,
    PIDTYPE_MAX = 3
} ;
struct pid_namespace;
struct upid {
   int nr ;
   struct pid_namespace *ns ;
   struct hlist_node pid_chain ;
};
struct pid {
   atomic_t count ;
   unsigned int level ;
   struct hlist_head tasks[3] ;
   struct rcu_head rcu ;
   struct upid numbers[1] ;
};
struct pid_link {
   struct hlist_node node ;
   struct pid *pid ;
};
struct pid_namespace;
struct prop_local_single {
   unsigned long events ;
   int shift ;
   unsigned long period ;
   spinlock_t lock ;
};
struct __anonstruct_seccomp_t_112 {
   int mode ;
};
typedef struct __anonstruct_seccomp_t_112 seccomp_t;
struct plist_head {
   struct list_head prio_list ;
   struct list_head node_list ;
};
struct rt_mutex_waiter;
struct rt_mutex_waiter;
struct task_struct;
struct rlimit {
   unsigned long rlim_cur ;
   unsigned long rlim_max ;
};
struct hrtimer_clock_base;
struct hrtimer_clock_base;
struct hrtimer_cpu_base;
struct hrtimer_cpu_base;
enum hrtimer_restart {
    HRTIMER_NORESTART = 0,
    HRTIMER_RESTART = 1
} ;
enum hrtimer_cb_mode {
    HRTIMER_CB_SOFTIRQ = 0,
    HRTIMER_CB_IRQSAFE = 1,
    HRTIMER_CB_IRQSAFE_NO_RESTART = 2,
    HRTIMER_CB_IRQSAFE_PERCPU = 3,
    HRTIMER_CB_IRQSAFE_UNLOCKED = 4
} ;
struct hrtimer {
   struct rb_node node ;
   ktime_t expires ;
   enum hrtimer_restart (*function)(struct hrtimer * ) ;
   struct hrtimer_clock_base *base ;
   unsigned long state ;
   enum hrtimer_cb_mode cb_mode ;
   struct list_head cb_entry ;
   void *start_site ;
   char start_comm[16] ;
   int start_pid ;
};
struct hrtimer_clock_base {
   struct hrtimer_cpu_base *cpu_base ;
   clockid_t index ;
   struct rb_root active ;
   struct rb_node *first ;
   ktime_t resolution ;
   ktime_t (*get_time)(void) ;
   ktime_t (*get_softirq_time)(void) ;
   ktime_t softirq_time ;
   ktime_t offset ;
   int (*reprogram)(struct hrtimer *t , struct hrtimer_clock_base *b , ktime_t n ) ;
};
struct hrtimer_cpu_base {
   spinlock_t lock ;
   struct hrtimer_clock_base clock_base[2] ;
   struct list_head cb_pending ;
   ktime_t expires_next ;
   int hres_active ;
   unsigned long nr_events ;
};
struct task_io_accounting {
   u64 rchar ;
   u64 wchar ;
   u64 syscr ;
   u64 syscw ;
   u64 read_bytes ;
   u64 write_bytes ;
   u64 cancelled_write_bytes ;
};
struct latency_record {
   unsigned long backtrace[12] ;
   unsigned int count ;
   unsigned long time ;
   unsigned long max ;
};
struct task_struct;
struct futex_pi_state;
struct futex_pi_state;
struct robust_list_head;
struct robust_list_head;
struct bio;
struct bio;
struct seq_file;
struct seq_file;
struct cfs_rq;
struct cfs_rq;
struct task_struct;
struct nsproxy;
struct nsproxy;
struct io_event {
   __u64 data ;
   __u64 obj ;
   __s64 res ;
   __s64 res2 ;
};
struct iovec {
   void *iov_base ;
   __kernel_size_t iov_len ;
};
struct kioctx;
union __anonunion_ki_obj_114 {
   void *user ;
   struct task_struct *tsk ;
};
struct kiocb {
   struct list_head ki_run_list ;
   unsigned long ki_flags ;
   int ki_users ;
   unsigned int ki_key ;
   struct file *ki_filp ;
   struct kioctx *ki_ctx ;
   int (*ki_cancel)(struct kiocb * , struct io_event * ) ;
   ssize_t (*ki_retry)(struct kiocb * ) ;
   void (*ki_dtor)(struct kiocb * ) ;
   union __anonunion_ki_obj_114 ki_obj ;
   __u64 ki_user_data ;
   wait_queue_t ki_wait ;
   loff_t ki_pos ;
   void *private ;
   unsigned short ki_opcode ;
   size_t ki_nbytes ;
   char *ki_buf ;
   size_t ki_left ;
   struct iovec ki_inline_vec ;
   struct iovec *ki_iovec ;
   unsigned long ki_nr_segs ;
   unsigned long ki_cur_seg ;
   struct list_head ki_list ;
   struct file *ki_eventfd ;
};
struct aio_ring_info {
   unsigned long mmap_base ;
   unsigned long mmap_size ;
   struct page **ring_pages ;
   spinlock_t ring_lock ;
   long nr_pages ;
   unsigned int nr ;
   unsigned int tail ;
   struct page *internal_pages[8] ;
};
struct kioctx {
   atomic_t users ;
   int dead ;
   struct mm_struct *mm ;
   unsigned long user_id ;
   struct kioctx *next ;
   wait_queue_head_t wait ;
   spinlock_t ctx_lock ;
   int reqs_active ;
   struct list_head active_reqs ;
   struct list_head run_list ;
   unsigned int max_reqs ;
   struct aio_ring_info ring_info ;
   struct delayed_work wq ;
};
struct mm_struct;
struct sighand_struct {
   atomic_t count ;
   struct k_sigaction action[64] ;
   spinlock_t siglock ;
   wait_queue_head_t signalfd_wqh ;
};
struct pacct_struct {
   int ac_flag ;
   long ac_exitcode ;
   unsigned long ac_mem ;
   cputime_t ac_utime ;
   cputime_t ac_stime ;
   unsigned long ac_minflt ;
   unsigned long ac_majflt ;
};
union __anonunion____missing_field_name_115 {
   pid_t pgrp ;
   pid_t __pgrp ;
};
union __anonunion____missing_field_name_116 {
   pid_t session ;
   pid_t __session ;
};
struct tty_struct;
struct key;
struct taskstats;
struct tty_audit_buf;
struct signal_struct {
   atomic_t count ;
   atomic_t live ;
   wait_queue_head_t wait_chldexit ;
   struct task_struct *curr_target ;
   struct sigpending shared_pending ;
   int group_exit_code ;
   struct task_struct *group_exit_task ;
   int notify_count ;
   int group_stop_count ;
   unsigned int flags ;
   struct list_head posix_timers ;
   struct hrtimer real_timer ;
   struct pid *leader_pid ;
   ktime_t it_real_incr ;
   cputime_t it_prof_expires ;
   cputime_t it_virt_expires ;
   cputime_t it_prof_incr ;
   cputime_t it_virt_incr ;
   union __anonunion____missing_field_name_115 __annonCompField14 ;
   struct pid *tty_old_pgrp ;
   union __anonunion____missing_field_name_116 __annonCompField15 ;
   int leader ;
   struct tty_struct *tty ;
   cputime_t utime ;
   cputime_t stime ;
   cputime_t cutime ;
   cputime_t cstime ;
   cputime_t gtime ;
   cputime_t cgtime ;
   unsigned long nvcsw ;
   unsigned long nivcsw ;
   unsigned long cnvcsw ;
   unsigned long cnivcsw ;
   unsigned long min_flt ;
   unsigned long maj_flt ;
   unsigned long cmin_flt ;
   unsigned long cmaj_flt ;
   unsigned long inblock ;
   unsigned long oublock ;
   unsigned long cinblock ;
   unsigned long coublock ;
   struct task_io_accounting ioac ;
   unsigned long long sum_sched_runtime ;
   struct rlimit rlim[16] ;
   struct list_head cpu_timers[3] ;
   struct key *session_keyring ;
   struct key *process_keyring ;
   struct pacct_struct pacct ;
   struct taskstats *stats ;
   unsigned int audit_tty ;
   struct tty_audit_buf *tty_audit_buf ;
};
struct user_struct {
   atomic_t __count ;
   atomic_t processes ;
   atomic_t files ;
   atomic_t sigpending ;
   atomic_t inotify_watches ;
   atomic_t inotify_devs ;
   atomic_t epoll_devs ;
   atomic_t epoll_watches ;
   unsigned long mq_bytes ;
   unsigned long locked_shm ;
   struct key *uid_keyring ;
   struct key *session_keyring ;
   struct hlist_node uidhash_node ;
   uid_t uid ;
};
struct backing_dev_info;
struct backing_dev_info;
struct reclaim_state;
struct reclaim_state;
struct sched_info {
   unsigned long pcount ;
   unsigned long long cpu_time ;
   unsigned long long run_delay ;
   unsigned long long last_arrival ;
   unsigned long long last_queued ;
   unsigned int bkl_count ;
};
struct file_operations;
enum cpu_idle_type {
    CPU_IDLE = 0,
    CPU_NOT_IDLE = 1,
    CPU_NEWLY_IDLE = 2,
    CPU_MAX_IDLE_TYPES = 3
} ;
struct sched_group {
   struct sched_group *next ;
   cpumask_t cpumask ;
   unsigned int __cpu_power ;
   u32 reciprocal_cpu_power ;
};
enum sched_domain_level {
    SD_LV_NONE = 0,
    SD_LV_SIBLING = 1,
    SD_LV_MC = 2,
    SD_LV_CPU = 3,
    SD_LV_NODE = 4,
    SD_LV_ALLNODES = 5,
    SD_LV_MAX = 6
} ;
struct sched_domain {
   struct sched_domain *parent ;
   struct sched_domain *child ;
   struct sched_group *groups ;
   cpumask_t span ;
   unsigned long min_interval ;
   unsigned long max_interval ;
   unsigned int busy_factor ;
   unsigned int imbalance_pct ;
   unsigned int cache_nice_tries ;
   unsigned int busy_idx ;
   unsigned int idle_idx ;
   unsigned int newidle_idx ;
   unsigned int wake_idx ;
   unsigned int forkexec_idx ;
   int flags ;
   enum sched_domain_level level ;
   unsigned long last_balance ;
   unsigned int balance_interval ;
   unsigned int nr_balance_failed ;
   u64 last_update ;
   unsigned int lb_count[3] ;
   unsigned int lb_failed[3] ;
   unsigned int lb_balanced[3] ;
   unsigned int lb_imbalance[3] ;
   unsigned int lb_gained[3] ;
   unsigned int lb_hot_gained[3] ;
   unsigned int lb_nobusyg[3] ;
   unsigned int lb_nobusyq[3] ;
   unsigned int alb_count ;
   unsigned int alb_failed ;
   unsigned int alb_pushed ;
   unsigned int sbe_count ;
   unsigned int sbe_balanced ;
   unsigned int sbe_pushed ;
   unsigned int sbf_count ;
   unsigned int sbf_balanced ;
   unsigned int sbf_pushed ;
   unsigned int ttwu_wake_remote ;
   unsigned int ttwu_move_affine ;
   unsigned int ttwu_move_balance ;
};
struct io_context;
struct io_context;
struct group_info {
   int ngroups ;
   atomic_t usage ;
   gid_t small_block[32] ;
   int nblocks ;
   gid_t *blocks[0] ;
};
struct audit_context;
struct audit_context;
struct pipe_inode_info;
struct pipe_inode_info;
struct rq;
struct rq;
struct sched_domain;
struct sched_class {
   struct sched_class  const  *next ;
   void (*enqueue_task)(struct rq *rq , struct task_struct *p , int wakeup ) ;
   void (*dequeue_task)(struct rq *rq , struct task_struct *p , int sleep ) ;
   void (*yield_task)(struct rq *rq ) ;
   int (*select_task_rq)(struct task_struct *p , int sync ) ;
   void (*check_preempt_curr)(struct rq *rq , struct task_struct *p ) ;
   struct task_struct *(*pick_next_task)(struct rq *rq ) ;
   void (*put_prev_task)(struct rq *rq , struct task_struct *p ) ;
   unsigned long (*load_balance)(struct rq *this_rq , int this_cpu , struct rq *busiest ,
                                 unsigned long max_load_move , struct sched_domain *sd ,
                                 enum cpu_idle_type idle , int *all_pinned , int *this_best_prio ) ;
   int (*move_one_task)(struct rq *this_rq , int this_cpu , struct rq *busiest , struct sched_domain *sd ,
                        enum cpu_idle_type idle ) ;
   void (*pre_schedule)(struct rq *this_rq , struct task_struct *task ) ;
   void (*post_schedule)(struct rq *this_rq ) ;
   void (*task_wake_up)(struct rq *this_rq , struct task_struct *task ) ;
   void (*set_curr_task)(struct rq *rq ) ;
   void (*task_tick)(struct rq *rq , struct task_struct *p , int queued ) ;
   void (*task_new)(struct rq *rq , struct task_struct *p ) ;
   void (*set_cpus_allowed)(struct task_struct *p , cpumask_t const   *newmask ) ;
   void (*rq_online)(struct rq *rq ) ;
   void (*rq_offline)(struct rq *rq ) ;
   void (*switched_from)(struct rq *this_rq , struct task_struct *task , int running ) ;
   void (*switched_to)(struct rq *this_rq , struct task_struct *task , int running ) ;
   void (*prio_changed)(struct rq *this_rq , struct task_struct *task , int oldprio ,
                        int running ) ;
   void (*moved_group)(struct task_struct *p ) ;
};
struct load_weight {
   unsigned long weight ;
   unsigned long inv_weight ;
};
struct sched_entity {
   struct load_weight load ;
   struct rb_node run_node ;
   struct list_head group_node ;
   unsigned int on_rq ;
   u64 exec_start ;
   u64 sum_exec_runtime ;
   u64 vruntime ;
   u64 prev_sum_exec_runtime ;
   u64 last_wakeup ;
   u64 avg_overlap ;
   u64 wait_start ;
   u64 wait_max ;
   u64 wait_count ;
   u64 wait_sum ;
   u64 sleep_start ;
   u64 sleep_max ;
   s64 sum_sleep_runtime ;
   u64 block_start ;
   u64 block_max ;
   u64 exec_max ;
   u64 slice_max ;
   u64 nr_migrations ;
   u64 nr_migrations_cold ;
   u64 nr_failed_migrations_affine ;
   u64 nr_failed_migrations_running ;
   u64 nr_failed_migrations_hot ;
   u64 nr_forced_migrations ;
   u64 nr_forced2_migrations ;
   u64 nr_wakeups ;
   u64 nr_wakeups_sync ;
   u64 nr_wakeups_migrate ;
   u64 nr_wakeups_local ;
   u64 nr_wakeups_remote ;
   u64 nr_wakeups_affine ;
   u64 nr_wakeups_affine_attempts ;
   u64 nr_wakeups_passive ;
   u64 nr_wakeups_idle ;
   struct sched_entity *parent ;
   struct cfs_rq *cfs_rq ;
   struct cfs_rq *my_q ;
};
struct rt_rq;
struct sched_rt_entity {
   struct list_head run_list ;
   unsigned int time_slice ;
   unsigned long timeout ;
   int nr_cpus_allowed ;
   struct sched_rt_entity *back ;
   struct sched_rt_entity *parent ;
   struct rt_rq *rt_rq ;
   struct rt_rq *my_q ;
};
struct linux_binfmt;
struct files_struct;
struct css_set;
struct task_struct {
   long volatile   state ;
   void *stack ;
   atomic_t usage ;
   unsigned int flags ;
   unsigned int ptrace ;
   int lock_depth ;
   int prio ;
   int static_prio ;
   int normal_prio ;
   unsigned int rt_priority ;
   struct sched_class  const  *sched_class ;
   struct sched_entity se ;
   struct sched_rt_entity rt ;
   struct hlist_head preempt_notifiers ;
   unsigned char fpu_counter ;
   s8 oomkilladj ;
   unsigned int btrace_seq ;
   unsigned int policy ;
   cpumask_t cpus_allowed ;
   struct sched_info sched_info ;
   struct list_head tasks ;
   struct mm_struct *mm ;
   struct mm_struct *active_mm ;
   struct linux_binfmt *binfmt ;
   int exit_state ;
   int exit_code ;
   int exit_signal ;
   int pdeath_signal ;
   unsigned int personality ;
   unsigned int did_exec : 1 ;
   pid_t pid ;
   pid_t tgid ;
   struct task_struct *real_parent ;
   struct task_struct *parent ;
   struct list_head children ;
   struct list_head sibling ;
   struct task_struct *group_leader ;
   struct list_head ptraced ;
   struct list_head ptrace_entry ;
   struct pid_link pids[3] ;
   struct list_head thread_group ;
   struct completion *vfork_done ;
   int *set_child_tid ;
   int *clear_child_tid ;
   cputime_t utime ;
   cputime_t stime ;
   cputime_t utimescaled ;
   cputime_t stimescaled ;
   cputime_t gtime ;
   cputime_t prev_utime ;
   cputime_t prev_stime ;
   unsigned long nvcsw ;
   unsigned long nivcsw ;
   struct timespec start_time ;
   struct timespec real_start_time ;
   unsigned long min_flt ;
   unsigned long maj_flt ;
   cputime_t it_prof_expires ;
   cputime_t it_virt_expires ;
   unsigned long long it_sched_expires ;
   struct list_head cpu_timers[3] ;
   uid_t uid ;
   uid_t euid ;
   uid_t suid ;
   uid_t fsuid ;
   gid_t gid ;
   gid_t egid ;
   gid_t sgid ;
   gid_t fsgid ;
   struct group_info *group_info ;
   kernel_cap_t cap_effective ;
   kernel_cap_t cap_inheritable ;
   kernel_cap_t cap_permitted ;
   kernel_cap_t cap_bset ;
   struct user_struct *user ;
   unsigned int securebits ;
   unsigned char jit_keyring ;
   struct key *request_key_auth ;
   struct key *thread_keyring ;
   char comm[16] ;
   int link_count ;
   int total_link_count ;
   struct sysv_sem sysvsem ;
   unsigned long last_switch_timestamp ;
   unsigned long last_switch_count ;
   struct thread_struct thread ;
   struct fs_struct *fs ;
   struct files_struct *files ;
   struct nsproxy *nsproxy ;
   struct signal_struct *signal ;
   struct sighand_struct *sighand ;
   sigset_t blocked ;
   sigset_t real_blocked ;
   sigset_t saved_sigmask ;
   struct sigpending pending ;
   unsigned long sas_ss_sp ;
   size_t sas_ss_size ;
   int (*notifier)(void *priv ) ;
   void *notifier_data ;
   sigset_t *notifier_mask ;
   void *security ;
   struct audit_context *audit_context ;
   uid_t loginuid ;
   unsigned int sessionid ;
   seccomp_t seccomp ;
   u32 parent_exec_id ;
   u32 self_exec_id ;
   spinlock_t alloc_lock ;
   spinlock_t pi_lock ;
   struct plist_head pi_waiters ;
   struct rt_mutex_waiter *pi_blocked_on ;
   void *journal_info ;
   struct bio *bio_list ;
   struct bio **bio_tail ;
   struct reclaim_state *reclaim_state ;
   struct backing_dev_info *backing_dev_info ;
   struct io_context *io_context ;
   unsigned long ptrace_message ;
   siginfo_t *last_siginfo ;
   struct task_io_accounting ioac ;
   u64 acct_rss_mem1 ;
   u64 acct_vm_mem1 ;
   cputime_t acct_timexpd ;
   nodemask_t mems_allowed ;
   int cpuset_mems_generation ;
   int cpuset_mem_spread_rotor ;
   struct css_set *cgroups ;
   struct list_head cg_list ;
   struct robust_list_head *robust_list ;
   struct list_head pi_state_list ;
   struct futex_pi_state *pi_state_cache ;
   atomic_t fs_excl ;
   struct rcu_head rcu ;
   struct list_head *scm_work_list ;
   struct pipe_inode_info *splice_pipe ;
   struct prop_local_single dirties ;
   int latency_record_count ;
   struct latency_record latency_record[32] ;
};
struct pid_namespace;
struct notifier_block;
struct bio;
struct reclaim_state {
   unsigned long reclaimed_slab ;
};
struct address_space;
struct writeback_control;
struct zone;
struct block_device;
struct backing_dev_info;
struct notifier_block;
struct task_struct;
struct mm_struct;
struct task_struct;
struct taskstats {
   __u16 version ;
   __u32 ac_exitcode ;
   __u8 ac_flag ;
   __u8 ac_nice ;
   __u64 cpu_count  __attribute__((__aligned__(8))) ;
   __u64 cpu_delay_total ;
   __u64 blkio_count ;
   __u64 blkio_delay_total ;
   __u64 swapin_count ;
   __u64 swapin_delay_total ;
   __u64 cpu_run_real_total ;
   __u64 cpu_run_virtual_total ;
   char ac_comm[32] ;
   __u8 ac_sched  __attribute__((__aligned__(8))) ;
   __u8 ac_pad[3] ;
   __u32 ac_uid  __attribute__((__aligned__(8))) ;
   __u32 ac_gid ;
   __u32 ac_pid ;
   __u32 ac_ppid ;
   __u32 ac_btime ;
   __u64 ac_etime  __attribute__((__aligned__(8))) ;
   __u64 ac_utime ;
   __u64 ac_stime ;
   __u64 ac_minflt ;
   __u64 ac_majflt ;
   __u64 coremem ;
   __u64 virtmem ;
   __u64 hiwater_rss ;
   __u64 hiwater_vm ;
   __u64 read_char ;
   __u64 write_char ;
   __u64 read_syscalls ;
   __u64 write_syscalls ;
   __u64 read_bytes ;
   __u64 write_bytes ;
   __u64 cancelled_write_bytes ;
   __u64 nvcsw ;
   __u64 nivcsw ;
   __u64 ac_utimescaled ;
   __u64 ac_stimescaled ;
   __u64 cpu_scaled_run_real_total ;
   __u64 freepages_count ;
   __u64 freepages_delay_total ;
};
struct cgroupfs_root;
struct cgroupfs_root;
struct inode;
struct cgroup;
struct cgroup;
struct cgroup_subsys_state {
   struct cgroup *cgroup ;
   atomic_t refcnt ;
   unsigned long flags ;
};
struct cgroup {
   unsigned long flags ;
   atomic_t count ;
   struct list_head sibling ;
   struct list_head children ;
   struct cgroup *parent ;
   struct dentry *dentry ;
   struct cgroup_subsys_state *subsys[5] ;
   struct cgroupfs_root *root ;
   struct cgroup *top_cgroup ;
   struct list_head css_sets ;
   struct list_head release_list ;
};
struct css_set {
   struct kref ref ;
   struct hlist_node hlist ;
   struct list_head tasks ;
   struct list_head cg_links ;
   struct cgroup_subsys_state *subsys[5] ;
};
struct seq_file;
struct seq_operations;
struct file;
struct path;
struct inode;
struct dentry;
struct seq_file {
   char *buf ;
   size_t size ;
   size_t from ;
   size_t count ;
   loff_t index ;
   u64 version ;
   struct mutex lock ;
   struct seq_operations  const  *op ;
   void *private ;
};
struct seq_operations {
   void *(*start)(struct seq_file *m , loff_t *pos ) ;
   void (*stop)(struct seq_file *m , void *v ) ;
   void *(*next)(struct seq_file *m , void *v , loff_t *pos ) ;
   int (*show)(struct seq_file *m , void *v ) ;
};
struct kstat {
   u64 ino ;
   dev_t dev ;
   umode_t mode ;
   unsigned int nlink ;
   uid_t uid ;
   gid_t gid ;
   dev_t rdev ;
   loff_t size ;
   struct timespec atime ;
   struct timespec mtime ;
   struct timespec ctime ;
   unsigned long blksize ;
   unsigned long long blocks ;
};
struct key;
struct file;
struct file;
struct task_struct;
struct file;
typedef __u32 Elf32_Addr;
typedef __u16 Elf32_Half;
typedef __u32 Elf32_Word;
struct elf32_sym {
   Elf32_Word st_name ;
   Elf32_Addr st_value ;
   Elf32_Word st_size ;
   unsigned char st_info ;
   unsigned char st_other ;
   Elf32_Half st_shndx ;
};
typedef struct elf32_sym Elf32_Sym;
struct module;
struct module;
struct marker;
struct marker;
typedef void marker_probe_func(void *probe_private , void *call_private , char const   *fmt ,
                               va_list *args );
struct marker_probe_closure {
   marker_probe_func *func ;
   void *probe_private ;
};
struct marker {
   char const   *name ;
   char const   *format ;
   char state ;
   char ptype ;
   void (*call)(struct marker  const  *mdata , void *call_private  , ...) ;
   struct marker_probe_closure single ;
   struct marker_probe_closure *multi ;
} __attribute__((__aligned__(8))) ;
struct __anonstruct_local_t_137 {
   atomic_long_t a ;
};
typedef struct __anonstruct_local_t_137 local_t;
struct mod_arch_specific {

};
struct kernel_symbol {
   unsigned long value ;
   char const   *name ;
};
struct module;
struct module_attribute {
   struct attribute attr ;
   ssize_t (*show)(struct module_attribute * , struct module * , char * ) ;
   ssize_t (*store)(struct module_attribute * , struct module * , char const   * ,
                    size_t count ) ;
   void (*setup)(struct module * , char const   * ) ;
   int (*test)(struct module * ) ;
   void (*free)(struct module * ) ;
};
struct module_kobject {
   struct kobject kobj ;
   struct module *mod ;
   struct kobject *drivers_dir ;
};
struct exception_table_entry;
struct exception_table_entry;
struct notifier_block;
struct module_ref {
   local_t count ;
} __attribute__((__aligned__((1) <<  (7) ))) ;
enum module_state {
    MODULE_STATE_LIVE = 0,
    MODULE_STATE_COMING = 1,
    MODULE_STATE_GOING = 2
} ;
struct module_param_attrs;
struct module_sect_attrs;
struct module_notes_attrs;
struct module {
   enum module_state state ;
   struct list_head list ;
   char name[64U - sizeof(unsigned long )] ;
   struct module_kobject mkobj ;
   struct module_param_attrs *param_attrs ;
   struct module_attribute *modinfo_attrs ;
   char const   *version ;
   char const   *srcversion ;
   struct kobject *holders_dir ;
   struct kernel_symbol  const  *syms ;
   unsigned long const   *crcs ;
   unsigned int num_syms ;
   unsigned int num_gpl_syms ;
   struct kernel_symbol  const  *gpl_syms ;
   unsigned long const   *gpl_crcs ;
   struct kernel_symbol  const  *unused_syms ;
   unsigned long const   *unused_crcs ;
   unsigned int num_unused_syms ;
   unsigned int num_unused_gpl_syms ;
   struct kernel_symbol  const  *unused_gpl_syms ;
   unsigned long const   *unused_gpl_crcs ;
   struct kernel_symbol  const  *gpl_future_syms ;
   unsigned long const   *gpl_future_crcs ;
   unsigned int num_gpl_future_syms ;
   unsigned int num_exentries ;
   struct exception_table_entry  const  *extable ;
   int (*init)(void) ;
   void *module_init ;
   void *module_core ;
   unsigned int init_size ;
   unsigned int core_size ;
   unsigned int init_text_size ;
   unsigned int core_text_size ;
   void *unwind_info ;
   struct mod_arch_specific arch ;
   unsigned int taints ;
   unsigned int num_bugs ;
   struct list_head bug_list ;
   struct bug_entry *bug_table ;
   Elf32_Sym *symtab ;
   unsigned int num_symtab ;
   char *strtab ;
   struct module_sect_attrs *sect_attrs ;
   struct module_notes_attrs *notes_attrs ;
   void *percpu ;
   char *args ;
   struct marker *markers ;
   unsigned int num_markers ;
   struct list_head modules_which_use_me ;
   struct task_struct *waiter ;
   void (*exit)(void) ;
   struct module_ref ref[64] ;
};
struct module;
struct notifier_block;
struct file;
struct completion;
struct nsproxy;
struct exception_table_entry {
   unsigned long insn ;
   unsigned long fixup ;
};
struct nameidata;
struct nameidata;
struct path;
struct vfsmount;
struct qstr {
   unsigned int hash ;
   unsigned int len ;
   unsigned char const   *name ;
};
struct dcookie_struct;
struct dcookie_struct;
union __anonunion_d_u_193 {
   struct list_head d_child ;
   struct rcu_head d_rcu ;
};
struct dentry_operations;
struct super_block;
struct dentry {
   atomic_t d_count ;
   unsigned int d_flags ;
   spinlock_t d_lock ;
   struct inode *d_inode ;
   struct hlist_node d_hash ;
   struct dentry *d_parent ;
   struct qstr d_name ;
   struct list_head d_lru ;
   union __anonunion_d_u_193 d_u ;
   struct list_head d_subdirs ;
   struct list_head d_alias ;
   unsigned long d_time ;
   struct dentry_operations *d_op ;
   struct super_block *d_sb ;
   void *d_fsdata ;
   struct dcookie_struct *d_cookie ;
   int d_mounted ;
   unsigned char d_iname[36] ;
};
struct dentry_operations {
   int (*d_revalidate)(struct dentry * , struct nameidata * ) ;
   int (*d_hash)(struct dentry * , struct qstr * ) ;
   int (*d_compare)(struct dentry * , struct qstr * , struct qstr * ) ;
   int (*d_delete)(struct dentry * ) ;
   void (*d_release)(struct dentry * ) ;
   void (*d_iput)(struct dentry * , struct inode * ) ;
   char *(*d_dname)(struct dentry * , char * , int  ) ;
};
struct radix_tree_node;
struct radix_tree_root {
   unsigned int height ;
   gfp_t gfp_mask ;
   struct radix_tree_node *rnode ;
};
struct semaphore {
   spinlock_t lock ;
   unsigned int count ;
   struct list_head wait_list ;
};
struct export_operations;
struct export_operations;
struct iovec;
struct nameidata;
struct kiocb;
struct pipe_inode_info;
struct poll_table_struct;
struct poll_table_struct;
struct kstatfs;
struct kstatfs;
struct vm_area_struct;
struct vfsmount;
struct iattr {
   unsigned int ia_valid ;
   umode_t ia_mode ;
   uid_t ia_uid ;
   gid_t ia_gid ;
   loff_t ia_size ;
   struct timespec ia_atime ;
   struct timespec ia_mtime ;
   struct timespec ia_ctime ;
   struct file *ia_file ;
};
struct if_dqblk {
   __u64 dqb_bhardlimit ;
   __u64 dqb_bsoftlimit ;
   __u64 dqb_curspace ;
   __u64 dqb_ihardlimit ;
   __u64 dqb_isoftlimit ;
   __u64 dqb_curinodes ;
   __u64 dqb_btime ;
   __u64 dqb_itime ;
   __u32 dqb_valid ;
};
struct if_dqinfo {
   __u64 dqi_bgrace ;
   __u64 dqi_igrace ;
   __u32 dqi_flags ;
   __u32 dqi_valid ;
};
struct fs_disk_quota {
   __s8 d_version ;
   __s8 d_flags ;
   __u16 d_fieldmask ;
   __u32 d_id ;
   __u64 d_blk_hardlimit ;
   __u64 d_blk_softlimit ;
   __u64 d_ino_hardlimit ;
   __u64 d_ino_softlimit ;
   __u64 d_bcount ;
   __u64 d_icount ;
   __s32 d_itimer ;
   __s32 d_btimer ;
   __u16 d_iwarns ;
   __u16 d_bwarns ;
   __s32 d_padding2 ;
   __u64 d_rtb_hardlimit ;
   __u64 d_rtb_softlimit ;
   __u64 d_rtbcount ;
   __s32 d_rtbtimer ;
   __u16 d_rtbwarns ;
   __s16 d_padding3 ;
   char d_padding4[8] ;
};
struct fs_qfilestat {
   __u64 qfs_ino ;
   __u64 qfs_nblks ;
   __u32 qfs_nextents ;
};
typedef struct fs_qfilestat fs_qfilestat_t;
struct fs_quota_stat {
   __s8 qs_version ;
   __u16 qs_flags ;
   __s8 qs_pad ;
   fs_qfilestat_t qs_uquota ;
   fs_qfilestat_t qs_gquota ;
   __u32 qs_incoredqs ;
   __s32 qs_btimelimit ;
   __s32 qs_itimelimit ;
   __s32 qs_rtbtimelimit ;
   __u16 qs_bwarnlimit ;
   __u16 qs_iwarnlimit ;
};
struct v1_mem_dqinfo {

};
struct v2_mem_dqinfo {
   unsigned int dqi_blocks ;
   unsigned int dqi_free_blk ;
   unsigned int dqi_free_entry ;
};
typedef __kernel_uid32_t qid_t;
typedef __u64 qsize_t;
struct mem_dqblk {
   __u32 dqb_bhardlimit ;
   __u32 dqb_bsoftlimit ;
   qsize_t dqb_curspace ;
   __u32 dqb_ihardlimit ;
   __u32 dqb_isoftlimit ;
   __u32 dqb_curinodes ;
   time_t dqb_btime ;
   time_t dqb_itime ;
};
struct quota_format_type;
struct quota_format_type;
union __anonunion_u_196 {
   struct v1_mem_dqinfo v1_i ;
   struct v2_mem_dqinfo v2_i ;
};
struct mem_dqinfo {
   struct quota_format_type *dqi_format ;
   int dqi_fmt_id ;
   struct list_head dqi_dirty_list ;
   unsigned long dqi_flags ;
   unsigned int dqi_bgrace ;
   unsigned int dqi_igrace ;
   qsize_t dqi_maxblimit ;
   qsize_t dqi_maxilimit ;
   union __anonunion_u_196 u ;
};
struct super_block;
struct dquot {
   struct hlist_node dq_hash ;
   struct list_head dq_inuse ;
   struct list_head dq_free ;
   struct list_head dq_dirty ;
   struct mutex dq_lock ;
   atomic_t dq_count ;
   wait_queue_head_t dq_wait_unused ;
   struct super_block *dq_sb ;
   unsigned int dq_id ;
   loff_t dq_off ;
   unsigned long dq_flags ;
   short dq_type ;
   struct mem_dqblk dq_dqb ;
};
struct quota_format_ops {
   int (*check_quota_file)(struct super_block *sb , int type ) ;
   int (*read_file_info)(struct super_block *sb , int type ) ;
   int (*write_file_info)(struct super_block *sb , int type ) ;
   int (*free_file_info)(struct super_block *sb , int type ) ;
   int (*read_dqblk)(struct dquot *dquot ) ;
   int (*commit_dqblk)(struct dquot *dquot ) ;
   int (*release_dqblk)(struct dquot *dquot ) ;
};
struct dquot_operations {
   int (*initialize)(struct inode * , int  ) ;
   int (*drop)(struct inode * ) ;
   int (*alloc_space)(struct inode * , qsize_t  , int  ) ;
   int (*alloc_inode)(struct inode  const  * , unsigned long  ) ;
   int (*free_space)(struct inode * , qsize_t  ) ;
   int (*free_inode)(struct inode  const  * , unsigned long  ) ;
   int (*transfer)(struct inode * , struct iattr * ) ;
   int (*write_dquot)(struct dquot * ) ;
   int (*acquire_dquot)(struct dquot * ) ;
   int (*release_dquot)(struct dquot * ) ;
   int (*mark_dirty)(struct dquot * ) ;
   int (*write_info)(struct super_block * , int  ) ;
};
struct quotactl_ops {
   int (*quota_on)(struct super_block * , int  , int  , char * , int  ) ;
   int (*quota_off)(struct super_block * , int  , int  ) ;
   int (*quota_sync)(struct super_block * , int  ) ;
   int (*get_info)(struct super_block * , int  , struct if_dqinfo * ) ;
   int (*set_info)(struct super_block * , int  , struct if_dqinfo * ) ;
   int (*get_dqblk)(struct super_block * , int  , qid_t  , struct if_dqblk * ) ;
   int (*set_dqblk)(struct super_block * , int  , qid_t  , struct if_dqblk * ) ;
   int (*get_xstate)(struct super_block * , struct fs_quota_stat * ) ;
   int (*set_xstate)(struct super_block * , unsigned int  , int  ) ;
   int (*get_xquota)(struct super_block * , int  , qid_t  , struct fs_disk_quota * ) ;
   int (*set_xquota)(struct super_block * , int  , qid_t  , struct fs_disk_quota * ) ;
};
struct quota_format_type {
   int qf_fmt_id ;
   struct quota_format_ops *qf_ops ;
   struct module *qf_owner ;
   struct quota_format_type *qf_next ;
};
struct quota_info {
   unsigned int flags ;
   struct mutex dqio_mutex ;
   struct mutex dqonoff_mutex ;
   struct rw_semaphore dqptr_sem ;
   struct inode *files[2] ;
   struct mem_dqinfo info[2] ;
   struct quota_format_ops *ops[2] ;
};
struct page;
struct address_space;
struct writeback_control;
union __anonunion_arg_198 {
   char *buf ;
   void *data ;
};
struct __anonstruct_read_descriptor_t_197 {
   size_t written ;
   size_t count ;
   union __anonunion_arg_198 arg ;
   int error ;
};
typedef struct __anonstruct_read_descriptor_t_197 read_descriptor_t;
struct address_space_operations {
   int (*writepage)(struct page *page , struct writeback_control *wbc ) ;
   int (*readpage)(struct file * , struct page * ) ;
   void (*sync_page)(struct page * ) ;
   int (*writepages)(struct address_space * , struct writeback_control * ) ;
   int (*set_page_dirty)(struct page *page ) ;
   int (*readpages)(struct file *filp , struct address_space *mapping , struct list_head *pages ,
                    unsigned int nr_pages ) ;
   int (*prepare_write)(struct file * , struct page * , unsigned int  , unsigned int  ) ;
   int (*commit_write)(struct file * , struct page * , unsigned int  , unsigned int  ) ;
   int (*write_begin)(struct file * , struct address_space *mapping , loff_t pos ,
                      unsigned int len , unsigned int flags , struct page **pagep ,
                      void **fsdata ) ;
   int (*write_end)(struct file * , struct address_space *mapping , loff_t pos , unsigned int len ,
                    unsigned int copied , struct page *page , void *fsdata ) ;
   sector_t (*bmap)(struct address_space * , sector_t  ) ;
   void (*invalidatepage)(struct page * , unsigned long  ) ;
   int (*releasepage)(struct page * , gfp_t  ) ;
   ssize_t (*direct_IO)(int  , struct kiocb * , struct iovec  const  *iov , loff_t offset ,
                        unsigned long nr_segs ) ;
   int (*get_xip_mem)(struct address_space * , unsigned long  , int  , void ** , unsigned long * ) ;
   int (*migratepage)(struct address_space * , struct page * , struct page * ) ;
   int (*launder_page)(struct page * ) ;
   int (*is_partially_uptodate)(struct page * , read_descriptor_t * , unsigned long  ) ;
};
struct backing_dev_info;
struct address_space {
   struct inode *host ;
   struct radix_tree_root page_tree ;
   spinlock_t tree_lock ;
   unsigned int i_mmap_writable ;
   struct prio_tree_root i_mmap ;
   struct list_head i_mmap_nonlinear ;
   spinlock_t i_mmap_lock ;
   unsigned int truncate_count ;
   unsigned long nrpages ;
   unsigned long writeback_index ;
   struct address_space_operations  const  *a_ops ;
   unsigned long flags ;
   struct backing_dev_info *backing_dev_info ;
   spinlock_t private_lock ;
   struct list_head private_list ;
   struct address_space *assoc_mapping ;
} __attribute__((__aligned__(sizeof(long )))) ;
struct hd_struct;
struct gendisk;
struct block_device {
   dev_t bd_dev ;
   struct inode *bd_inode ;
   int bd_openers ;
   struct mutex bd_mutex ;
   struct semaphore bd_mount_sem ;
   struct list_head bd_inodes ;
   void *bd_holder ;
   int bd_holders ;
   struct list_head bd_holder_list ;
   struct block_device *bd_contains ;
   unsigned int bd_block_size ;
   struct hd_struct *bd_part ;
   unsigned int bd_part_count ;
   int bd_invalidated ;
   struct gendisk *bd_disk ;
   struct list_head bd_list ;
   struct backing_dev_info *bd_inode_backing_dev_info ;
   unsigned long bd_private ;
};
struct inode_operations;
struct file_lock;
struct cdev;
union __anonunion____missing_field_name_199 {
   struct pipe_inode_info *i_pipe ;
   struct block_device *i_bdev ;
   struct cdev *i_cdev ;
};
struct dnotify_struct;
struct inode {
   struct hlist_node i_hash ;
   struct list_head i_list ;
   struct list_head i_sb_list ;
   struct list_head i_dentry ;
   unsigned long i_ino ;
   atomic_t i_count ;
   unsigned int i_nlink ;
   uid_t i_uid ;
   gid_t i_gid ;
   dev_t i_rdev ;
   u64 i_version ;
   loff_t i_size ;
   seqcount_t i_size_seqcount ;
   struct timespec i_atime ;
   struct timespec i_mtime ;
   struct timespec i_ctime ;
   unsigned int i_blkbits ;
   blkcnt_t i_blocks ;
   unsigned short i_bytes ;
   umode_t i_mode ;
   spinlock_t i_lock ;
   struct mutex i_mutex ;
   struct rw_semaphore i_alloc_sem ;
   struct inode_operations  const  *i_op ;
   struct file_operations  const  *i_fop ;
   struct super_block *i_sb ;
   struct file_lock *i_flock ;
   struct address_space *i_mapping ;
   struct address_space i_data ;
   struct dquot *i_dquot[2] ;
   struct list_head i_devices ;
   union __anonunion____missing_field_name_199 __annonCompField17 ;
   int i_cindex ;
   __u32 i_generation ;
   unsigned long i_dnotify_mask ;
   struct dnotify_struct *i_dnotify ;
   struct list_head inotify_watches ;
   struct mutex inotify_mutex ;
   unsigned long i_state ;
   unsigned long dirtied_when ;
   unsigned int i_flags ;
   atomic_t i_writecount ;
   void *i_security ;
   void *i_private ;
};
struct fown_struct {
   rwlock_t lock ;
   struct pid *pid ;
   enum pid_type pid_type ;
   uid_t uid ;
   uid_t euid ;
   int signum ;
};
struct file_ra_state {
   unsigned long start ;
   unsigned int size ;
   unsigned int async_size ;
   unsigned int ra_pages ;
   int mmap_miss ;
   loff_t prev_pos ;
};
union __anonunion_f_u_200 {
   struct list_head fu_list ;
   struct rcu_head fu_rcuhead ;
};
struct file {
   union __anonunion_f_u_200 f_u ;
   struct path f_path ;
   struct file_operations  const  *f_op ;
   atomic_long_t f_count ;
   unsigned int f_flags ;
   mode_t f_mode ;
   loff_t f_pos ;
   struct fown_struct f_owner ;
   unsigned int f_uid ;
   unsigned int f_gid ;
   struct file_ra_state f_ra ;
   u64 f_version ;
   void *f_security ;
   void *private_data ;
   struct list_head f_ep_links ;
   spinlock_t f_ep_lock ;
   struct address_space *f_mapping ;
};
typedef struct files_struct *fl_owner_t;
struct file_lock_operations {
   void (*fl_copy_lock)(struct file_lock * , struct file_lock * ) ;
   void (*fl_release_private)(struct file_lock * ) ;
};
struct lock_manager_operations {
   int (*fl_compare_owner)(struct file_lock * , struct file_lock * ) ;
   void (*fl_notify)(struct file_lock * ) ;
   int (*fl_grant)(struct file_lock * , struct file_lock * , int  ) ;
   void (*fl_copy_lock)(struct file_lock * , struct file_lock * ) ;
   void (*fl_release_private)(struct file_lock * ) ;
   void (*fl_break)(struct file_lock * ) ;
   int (*fl_mylease)(struct file_lock * , struct file_lock * ) ;
   int (*fl_change)(struct file_lock ** , int  ) ;
};
struct nlm_lockowner;
struct nlm_lockowner;
struct nfs_lock_info {
   u32 state ;
   struct nlm_lockowner *owner ;
   struct list_head list ;
};
struct nfs4_lock_state;
struct nfs4_lock_state;
struct nfs4_lock_info {
   struct nfs4_lock_state *owner ;
};
struct fasync_struct;
struct __anonstruct_afs_202 {
   struct list_head link ;
   int state ;
};
union __anonunion_fl_u_201 {
   struct nfs_lock_info nfs_fl ;
   struct nfs4_lock_info nfs4_fl ;
   struct __anonstruct_afs_202 afs ;
};
struct file_lock {
   struct file_lock *fl_next ;
   struct list_head fl_link ;
   struct list_head fl_block ;
   fl_owner_t fl_owner ;
   unsigned char fl_flags ;
   unsigned char fl_type ;
   unsigned int fl_pid ;
   struct pid *fl_nspid ;
   wait_queue_head_t fl_wait ;
   struct file *fl_file ;
   loff_t fl_start ;
   loff_t fl_end ;
   struct fasync_struct *fl_fasync ;
   unsigned long fl_break_time ;
   struct file_lock_operations *fl_ops ;
   struct lock_manager_operations *fl_lmops ;
   union __anonunion_fl_u_201 fl_u ;
};
struct fasync_struct {
   int magic ;
   int fa_fd ;
   struct fasync_struct *fa_next ;
   struct file *fa_file ;
};
struct file_system_type;
struct super_operations;
struct xattr_handler;
struct mtd_info;
struct super_block {
   struct list_head s_list ;
   dev_t s_dev ;
   unsigned long s_blocksize ;
   unsigned char s_blocksize_bits ;
   unsigned char s_dirt ;
   unsigned long long s_maxbytes ;
   struct file_system_type *s_type ;
   struct super_operations  const  *s_op ;
   struct dquot_operations *dq_op ;
   struct quotactl_ops *s_qcop ;
   struct export_operations  const  *s_export_op ;
   unsigned long s_flags ;
   unsigned long s_magic ;
   struct dentry *s_root ;
   struct rw_semaphore s_umount ;
   struct mutex s_lock ;
   int s_count ;
   int s_syncing ;
   int s_need_sync_fs ;
   atomic_t s_active ;
   void *s_security ;
   struct xattr_handler **s_xattr ;
   struct list_head s_inodes ;
   struct list_head s_dirty ;
   struct list_head s_io ;
   struct list_head s_more_io ;
   struct hlist_head s_anon ;
   struct list_head s_files ;
   struct list_head s_dentry_lru ;
   int s_nr_dentry_unused ;
   struct block_device *s_bdev ;
   struct mtd_info *s_mtd ;
   struct list_head s_instances ;
   struct quota_info s_dquot ;
   int s_frozen ;
   wait_queue_head_t s_wait_unfrozen ;
   char s_id[32] ;
   void *s_fs_info ;
   struct mutex s_vfs_rename_mutex ;
   u32 s_time_gran ;
   char *s_subtype ;
   char *s_options ;
};
struct file_operations {
   struct module *owner ;
   loff_t (*llseek)(struct file * , loff_t  , int  ) ;
   ssize_t (*read)(struct file * , char * , size_t  , loff_t * ) ;
   ssize_t (*write)(struct file * , char const   * , size_t  , loff_t * ) ;
   ssize_t (*aio_read)(struct kiocb * , struct iovec  const  * , unsigned long  ,
                       loff_t  ) ;
   ssize_t (*aio_write)(struct kiocb * , struct iovec  const  * , unsigned long  ,
                        loff_t  ) ;
   int (*readdir)(struct file * , void * , int (*)(void * , char const   * , int  ,
                                                   loff_t  , u64  , unsigned int  ) ) ;
   unsigned int (*poll)(struct file * , struct poll_table_struct * ) ;
   int (*ioctl)(struct inode * , struct file * , unsigned int  , unsigned long  ) ;
   long (*unlocked_ioctl)(struct file * , unsigned int  , unsigned long  ) ;
   long (*compat_ioctl)(struct file * , unsigned int  , unsigned long  ) ;
   int (*mmap)(struct file * , struct vm_area_struct * ) ;
   int (*open)(struct inode * , struct file * ) ;
   int (*flush)(struct file * , fl_owner_t id ) ;
   int (*release)(struct inode * , struct file * ) ;
   int (*fsync)(struct file * , struct dentry * , int datasync ) ;
   int (*aio_fsync)(struct kiocb * , int datasync ) ;
   int (*fasync)(int  , struct file * , int  ) ;
   int (*lock)(struct file * , int  , struct file_lock * ) ;
   ssize_t (*sendpage)(struct file * , struct page * , int  , size_t  , loff_t * ,
                       int  ) ;
   unsigned long (*get_unmapped_area)(struct file * , unsigned long  , unsigned long  ,
                                      unsigned long  , unsigned long  ) ;
   int (*check_flags)(int  ) ;
   int (*dir_notify)(struct file *filp , unsigned long arg ) ;
   int (*flock)(struct file * , int  , struct file_lock * ) ;
   ssize_t (*splice_write)(struct pipe_inode_info * , struct file * , loff_t * , size_t  ,
                           unsigned int  ) ;
   ssize_t (*splice_read)(struct file * , loff_t * , struct pipe_inode_info * , size_t  ,
                          unsigned int  ) ;
   int (*setlease)(struct file * , long  , struct file_lock ** ) ;
   int (*fsetattr)(struct file * , struct iattr * ) ;
};
struct inode_operations {
   int (*create)(struct inode * , struct dentry * , int  , struct nameidata * ) ;
   struct dentry *(*lookup)(struct inode * , struct dentry * , struct nameidata * ) ;
   int (*link)(struct dentry * , struct inode * , struct dentry * ) ;
   int (*unlink)(struct inode * , struct dentry * ) ;
   int (*symlink)(struct inode * , struct dentry * , char const   * ) ;
   int (*mkdir)(struct inode * , struct dentry * , int  ) ;
   int (*rmdir)(struct inode * , struct dentry * ) ;
   int (*mknod)(struct inode * , struct dentry * , int  , dev_t  ) ;
   int (*rename)(struct inode * , struct dentry * , struct inode * , struct dentry * ) ;
   int (*readlink)(struct dentry * , char * , int  ) ;
   void *(*follow_link)(struct dentry * , struct nameidata * ) ;
   void (*put_link)(struct dentry * , struct nameidata * , void * ) ;
   void (*truncate)(struct inode * ) ;
   int (*permission)(struct inode * , int  ) ;
   int (*setattr)(struct dentry * , struct iattr * ) ;
   int (*getattr)(struct vfsmount *mnt , struct dentry * , struct kstat * ) ;
   int (*setxattr)(struct dentry * , char const   * , void const   * , size_t  , int  ) ;
   ssize_t (*getxattr)(struct dentry * , char const   * , void * , size_t  ) ;
   ssize_t (*listxattr)(struct dentry * , char * , size_t  ) ;
   int (*removexattr)(struct dentry * , char const   * ) ;
   void (*truncate_range)(struct inode * , loff_t  , loff_t  ) ;
   long (*fallocate)(struct inode *inode , int mode , loff_t offset , loff_t len ) ;
};
struct seq_file;
struct super_operations {
   struct inode *(*alloc_inode)(struct super_block *sb ) ;
   void (*destroy_inode)(struct inode * ) ;
   void (*dirty_inode)(struct inode * ) ;
   int (*write_inode)(struct inode * , int  ) ;
   void (*drop_inode)(struct inode * ) ;
   void (*delete_inode)(struct inode * ) ;
   void (*put_super)(struct super_block * ) ;
   void (*write_super)(struct super_block * ) ;
   int (*sync_fs)(struct super_block *sb , int wait ) ;
   void (*write_super_lockfs)(struct super_block * ) ;
   void (*unlockfs)(struct super_block * ) ;
   int (*statfs)(struct dentry * , struct kstatfs * ) ;
   int (*remount_fs)(struct super_block * , int * , char * ) ;
   void (*clear_inode)(struct inode * ) ;
   void (*umount_begin)(struct super_block * ) ;
   int (*show_options)(struct seq_file * , struct vfsmount * ) ;
   int (*show_stats)(struct seq_file * , struct vfsmount * ) ;
   ssize_t (*quota_read)(struct super_block * , int  , char * , size_t  , loff_t  ) ;
   ssize_t (*quota_write)(struct super_block * , int  , char const   * , size_t  ,
                          loff_t  ) ;
};
struct file_system_type {
   char const   *name ;
   int fs_flags ;
   int (*get_sb)(struct file_system_type * , int  , char const   * , void * , struct vfsmount * ) ;
   void (*kill_sb)(struct super_block * ) ;
   struct module *owner ;
   struct file_system_type *next ;
   struct list_head fs_supers ;
   struct lock_class_key s_lock_key ;
   struct lock_class_key s_umount_key ;
   struct lock_class_key i_lock_key ;
   struct lock_class_key i_mutex_key ;
   struct lock_class_key i_mutex_dir_key ;
   struct lock_class_key i_alloc_sem_key ;
};
struct bio;
struct mm_struct;
typedef unsigned int kmem_bufctl_t;
struct slab {
   struct list_head list ;
   unsigned long colouroff ;
   void *s_mem ;
   unsigned int inuse ;
   kmem_bufctl_t free ;
   unsigned short nodeid ;
};
struct array_cache {
   unsigned int avail ;
   unsigned int limit ;
   unsigned int batchcount ;
   unsigned int touched ;
   spinlock_t lock ;
   void *entry[] ;
};
struct arraycache_init {
   struct array_cache cache ;
   void *entries[1] ;
};
struct kmem_list3 {
   struct list_head slabs_partial ;
   struct list_head slabs_full ;
   struct list_head slabs_free ;
   unsigned long free_objects ;
   unsigned int free_limit ;
   unsigned int colour_next ;
   spinlock_t list_lock ;
   struct array_cache *shared ;
   struct array_cache **alien ;
   unsigned long next_reap ;
   int free_touched ;
};
struct kmem_cache___0 {
   struct array_cache *array[64] ;
   unsigned int batchcount ;
   unsigned int limit ;
   unsigned int shared ;
   unsigned int buffer_size ;
   u32 reciprocal_buffer_size ;
   unsigned int flags ;
   unsigned int num ;
   unsigned int gfporder ;
   gfp_t gfpflags ;
   size_t colour ;
   unsigned int colour_off ;
   struct kmem_cache *slabp_cache ;
   unsigned int slab_size ;
   unsigned int dflags ;
   void (*ctor)(void *obj ) ;
   char const   *name ;
   struct list_head next ;
   struct kmem_list3 *nodelists[1] ;
};
struct cache_sizes;
struct cache_names {
   char *name ;
   char *name_dma ;
};
enum __anonenum_g_cpucache_up_205 {
    NONE = 0,
    PARTIAL_AC = 1,
    PARTIAL_L3 = 2,
    FULL = 3
} ;
struct ccupdate_struct {
   struct kmem_cache___0 *cachep ;
   struct array_cache *new[64] ;
};
/* compiler builtin: 
   long __builtin_expect(long  , long  ) ;  */
__inline static void __set_bit(int nr , unsigned long volatile   *addr ) 
{ 

  {
  {
  __asm__  volatile   ("bts %1,%0": "+m" (*((long volatile   *)addr)): "Ir" (nr): "memory");
  }
  return;
}
}
__inline static void __clear_bit(int nr , unsigned long volatile   *addr ) 
{ 

  {
  {
  __asm__  volatile   ("btr %1,%0": "+m" (*((long volatile   *)addr)): "Ir" (nr));
  }
  return;
}
}
__inline static int constant_test_bit(int nr , unsigned long const volatile   *addr ) 
{ 

  {
  return (((1UL << nr % 32) & *((unsigned long *)addr + nr / 32)) != 0UL);
}
}
extern unsigned int hweight32(unsigned int w ) ;
extern unsigned long hweight64(__u64 w ) ;
__inline static unsigned long hweight_long(unsigned long w ) 
{ unsigned int tmp ;
  unsigned long tmp___0 ;
  unsigned long tmp___1 ;

  {
  if (sizeof(w) == 4U) {
    {
    tmp = hweight32((unsigned int )w);
    tmp___1 = (unsigned long )tmp;
    }
  } else {
    {
    tmp___0 = hweight64((unsigned long long )w);
    tmp___1 = tmp___0;
    }
  }
  return (tmp___1);
}
}
extern int strcmp(char const   *cs , char const   *ct ) ;
extern char *strchr(char const   *s , int c ) ;
__inline static void *( __attribute__((__always_inline__)) __memcpy)(void *to , void const   *from ,
                                                                     size_t n ) 
{ int d0 ;
  int d1 ;
  int d2 ;

  {
  {
  __asm__  volatile   ("rep ; movsl\n\t"
                       "movl %4,%%ecx\n\t"
                       "andl $3,%%ecx\n\t"
                       "jz 1f\n\t"
                       "rep ; movsb\n\t"
                       "1:": "=&c" (d0), "=&D" (d1), "=&S" (d2): "0" (n / 4U), "g" (n),
                       "1" ((long )to), "2" ((long )from): "memory");
  }
  return (to);
}
}
__inline static void *( __attribute__((__always_inline__)) __constant_memcpy)(void *to ,
                                                                              void const   *from ,
                                                                              size_t n ) 
{ long esi ;
  long edi ;
  int ecx ;

  {
  if (! n) {
    return (to);
  }
  if ((int )n == 1) {
    goto switch_0_1;
  } else {
    if ((int )n == 2) {
      goto switch_0_2;
    } else {
      if ((int )n == 4) {
        goto switch_0_4;
      } else {
        if ((int )n == 3) {
          goto switch_0_3;
        } else {
          if ((int )n == 5) {
            goto switch_0_5;
          } else {
            if ((int )n == 6) {
              goto switch_0_6;
            } else {
              if ((int )n == 8) {
                goto switch_0_8;
              } else {
                if (0) {
                  switch_0_1: /* CIL Label */ 
                  {
                  *((char *)to) = *((char *)from);
                  }
                  return (to);
                  switch_0_2: /* CIL Label */ 
                  {
                  *((short *)to) = *((short *)from);
                  }
                  return (to);
                  switch_0_4: /* CIL Label */ 
                  {
                  *((int *)to) = *((int *)from);
                  }
                  return (to);
                  switch_0_3: /* CIL Label */ 
                  {
                  *((short *)to) = *((short *)from);
                  *((char *)to + 2) = *((char *)from + 2);
                  }
                  return (to);
                  switch_0_5: /* CIL Label */ 
                  {
                  *((int *)to) = *((int *)from);
                  *((char *)to + 4) = *((char *)from + 4);
                  }
                  return (to);
                  switch_0_6: /* CIL Label */ 
                  {
                  *((int *)to) = *((int *)from);
                  *((short *)to + 2) = *((short *)from + 2);
                  }
                  return (to);
                  switch_0_8: /* CIL Label */ 
                  {
                  *((int *)to) = *((int *)from);
                  *((int *)to + 1) = *((int *)from + 1);
                  }
                  return (to);
                } else {
                  switch_0_break: /* CIL Label */ ;
                }
              }
            }
          }
        }
      }
    }
  }
  {
  esi = (long )from;
  edi = (long )to;
  }
  if (n >= 20U) {
    {
    __asm__  volatile   ("rep ; movsl": "=&c" (ecx), "=&D" (edi), "=&S" (esi): "0" (n / 4U),
                         "1" (edi), "2" (esi): "memory");
    }
  } else {
    if (n >= 16U) {
      {
      __asm__  volatile   ("movsl": "=&D" (edi), "=&S" (esi): "0" (edi), "1" (esi): "memory");
      }
    }
    if (n >= 12U) {
      {
      __asm__  volatile   ("movsl": "=&D" (edi), "=&S" (esi): "0" (edi), "1" (esi): "memory");
      }
    }
    if (n >= 8U) {
      {
      __asm__  volatile   ("movsl": "=&D" (edi), "=&S" (esi): "0" (edi), "1" (esi): "memory");
      }
    }
    if (n >= 4U) {
      {
      __asm__  volatile   ("movsl": "=&D" (edi), "=&S" (esi): "0" (edi), "1" (esi): "memory");
      }
    }
  }
  if ((int )(n % 4U) == 0) {
    goto switch_1_0;
  } else {
    if ((int )(n % 4U) == 1) {
      goto switch_1_1;
    } else {
      if ((int )(n % 4U) == 2) {
        goto switch_1_2;
      } else {
        {
        goto switch_1_default;
        if (0) {
          switch_1_0: /* CIL Label */ 
          return (to);
          switch_1_1: /* CIL Label */ 
          {
          __asm__  volatile   ("movsb": "=&D" (edi), "=&S" (esi): "0" (edi), "1" (esi): "memory");
          }
          return (to);
          switch_1_2: /* CIL Label */ 
          {
          __asm__  volatile   ("movsw": "=&D" (edi), "=&S" (esi): "0" (edi), "1" (esi): "memory");
          }
          return (to);
          switch_1_default: /* CIL Label */ 
          {
          __asm__  volatile   ("movsw\n\tmovsb": "=&D" (edi), "=&S" (esi): "0" (edi),
                               "1" (esi): "memory");
          }
          return (to);
        } else {
          switch_1_break: /* CIL Label */ ;
        }
        }
      }
    }
  }
}
}
extern void *memmove(void *dest , void const   *src , size_t n ) ;
__inline static void *( __attribute__((__always_inline__)) __constant_c_memset)(void *s ,
                                                                                unsigned long c ,
                                                                                size_t count ) 
{ int d0 ;
  int d1 ;

  {
  {
  __asm__  volatile   ("rep ; stosl\n\t"
                       "testb $2,%b3\n\t"
                       "je 1f\n\t"
                       "stosw\n"
                       "1:\ttestb $1,%b3\n\t"
                       "je 2f\n\t"
                       "stosb\n"
                       "2:": "=&c" (d0), "=&D" (d1): "a" (c), "q" (count), "0" (count / 4U),
                       "1" ((long )s): "memory");
  }
  return (s);
}
}
extern int _cond_resched(void) ;
extern int ( /* format attribute */  sscanf)(char const   * , char const   *  , ...) ;
extern int ( /* format attribute */ __attribute__((__regparm__(0))) printk)(char const   *fmt 
                                                                            , ...)  __attribute__((__cold__)) ;
extern int __bitmap_empty(unsigned long const   *bitmap , int bits ) ;
extern int __bitmap_weight(unsigned long const   *bitmap , int bits ) ;
__inline static int bitmap_empty(unsigned long const   *src , int nbits ) 
{ unsigned long tmp ;
  int tmp___0 ;
  int tmp___1 ;

  {
  if (nbits <= 32) {
    if (nbits % 32) {
      {
      tmp = (1UL << nbits % 32) - 1UL;
      }
    } else {
      {
      tmp = ~ 0UL;
      }
    }
    if (*src & (unsigned long const   )tmp) {
      {
      tmp___0 = 0;
      }
    } else {
      {
      tmp___0 = 1;
      }
    }
    return (tmp___0);
  } else {
    {
    tmp___1 = __bitmap_empty(src, nbits);
    }
    return (tmp___1);
  }
}
}
__inline static int bitmap_weight(unsigned long const   *src , int nbits ) 
{ unsigned long tmp ;
  unsigned long tmp___0 ;
  int tmp___1 ;

  {
  if (nbits <= 32) {
    if (nbits % 32) {
      {
      tmp = (1UL << nbits % 32) - 1UL;
      }
    } else {
      {
      tmp = ~ 0UL;
      }
    }
    {
    tmp___0 = hweight_long((unsigned long )(*src & (unsigned long const   )tmp));
    }
    return ((int )tmp___0);
  }
  {
  tmp___1 = __bitmap_weight(src, nbits);
  }
  return (tmp___1);
}
}
__inline static int __cpus_empty(cpumask_t const   *srcp , int nbits ) 
{ int tmp ;

  {
  {
  tmp = bitmap_empty((unsigned long const   *)(srcp->bits), nbits);
  }
  return (tmp);
}
}
__inline static int __cpus_weight(cpumask_t const   *srcp , int nbits ) 
{ int tmp ;

  {
  {
  tmp = bitmap_weight((unsigned long const   *)(srcp->bits), nbits);
  }
  return (tmp);
}
}
extern int __next_cpu(int n , cpumask_t const   *srcp ) ;
extern cpumask_t cpu_online_map ;
extern struct pv_irq_ops pv_irq_ops ;
extern struct pv_lock_ops pv_lock_ops ;
__inline static void ( __attribute__((__always_inline__)) __raw_spin_unlock)(struct raw_spinlock *lock ) 
{ unsigned long __eax ;
  unsigned long __edx ;
  unsigned long __ecx ;

  {
  {
  __asm__  volatile   (""
                       "771:\n\t"
                       "call *%[paravirt_opptr];"
                       "\n"
                       "772:\n"
                       ".pushsection .parainstructions,\"a\"\n"
                       " "
                       ".balign 4"
                       " "
                       "\n"
                       " "
                       ".long"
                       " "
                       " 771b\n"
                       "  .byte "
                       "%c[paravirt_typenum]"
                       "\n"
                       "  .byte 772b-771b\n"
                       "  .short "
                       "%c[paravirt_clobber]"
                       "\n"
                       ".popsection\n"
                       "": "=a" (__eax), "=d" (__edx), "=c" (__ecx): [paravirt_typenum] "i" ((unsigned int )(& ((struct paravirt_patch_template *)0)->pv_lock_ops.spin_unlock) / sizeof(void *)),
                       [paravirt_opptr] "m" (pv_lock_ops.spin_unlock), [paravirt_clobber] "i" ((1 << 3) - 1),
                       "0" ((unsigned long )lock): "memory", "cc");
  }
  return;
}
}
__inline static unsigned long __raw_local_save_flags(void) 
{ unsigned long f ;

  {
  {
  __asm__  volatile   ("771:\n\t"
                       "pushl %%ecx; pushl %%edx;"
                       "call *%[paravirt_opptr];"
                       "popl %%edx; popl %%ecx"
                       "\n"
                       "772:\n"
                       ".pushsection .parainstructions,\"a\"\n"
                       " "
                       ".balign 4"
                       " "
                       "\n"
                       " "
                       ".long"
                       " "
                       " 771b\n"
                       "  .byte "
                       "%c[paravirt_typenum]"
                       "\n"
                       "  .byte 772b-771b\n"
                       "  .short "
                       "%c[paravirt_clobber]"
                       "\n"
                       ".popsection\n": "=a" (f): [paravirt_typenum] "i" ((unsigned int )(& ((struct paravirt_patch_template *)0)->pv_irq_ops.save_fl) / sizeof(void *)),
                       [paravirt_opptr] "m" (pv_irq_ops.save_fl), [paravirt_clobber] "i" (1): "memory",
                       "cc");
  }
  return (f);
}
}
__inline static void raw_local_irq_restore(unsigned long f ) 
{ 

  {
  {
  __asm__  volatile   ("771:\n\t"
                       "pushl %%ecx; pushl %%edx;"
                       "call *%[paravirt_opptr];"
                       "popl %%edx; popl %%ecx"
                       "\n"
                       "772:\n"
                       ".pushsection .parainstructions,\"a\"\n"
                       " "
                       ".balign 4"
                       " "
                       "\n"
                       " "
                       ".long"
                       " "
                       " 771b\n"
                       "  .byte "
                       "%c[paravirt_typenum]"
                       "\n"
                       "  .byte 772b-771b\n"
                       "  .short "
                       "%c[paravirt_clobber]"
                       "\n"
                       ".popsection\n": "=a" (f): "0" (f), [paravirt_typenum] "i" ((unsigned int )(& ((struct paravirt_patch_template *)0)->pv_irq_ops.restore_fl) / sizeof(void *)),
                       [paravirt_opptr] "m" (pv_irq_ops.restore_fl), [paravirt_clobber] "i" (1): "memory",
                       "cc");
  }
  return;
}
}
__inline static void raw_local_irq_disable(void) 
{ 

  {
  {
  __asm__  volatile   ("771:\n\t"
                       "pushl %%ecx; pushl %%edx;"
                       "call *%[paravirt_opptr];"
                       "popl %%edx; popl %%ecx"
                       "\n"
                       "772:\n"
                       ".pushsection .parainstructions,\"a\"\n"
                       " "
                       ".balign 4"
                       " "
                       "\n"
                       " "
                       ".long"
                       " "
                       " 771b\n"
                       "  .byte "
                       "%c[paravirt_typenum]"
                       "\n"
                       "  .byte 772b-771b\n"
                       "  .short "
                       "%c[paravirt_clobber]"
                       "\n"
                       ".popsection\n": : [paravirt_typenum] "i" ((unsigned int )(& ((struct paravirt_patch_template *)0)->pv_irq_ops.irq_disable) / sizeof(void *)),
                       [paravirt_opptr] "m" (pv_irq_ops.irq_disable), [paravirt_clobber] "i" (1): "memory",
                       "eax", "cc");
  }
  return;
}
}
__inline static void raw_local_irq_enable(void) 
{ 

  {
  {
  __asm__  volatile   ("771:\n\t"
                       "pushl %%ecx; pushl %%edx;"
                       "call *%[paravirt_opptr];"
                       "popl %%edx; popl %%ecx"
                       "\n"
                       "772:\n"
                       ".pushsection .parainstructions,\"a\"\n"
                       " "
                       ".balign 4"
                       " "
                       "\n"
                       " "
                       ".long"
                       " "
                       " 771b\n"
                       "  .byte "
                       "%c[paravirt_typenum]"
                       "\n"
                       "  .byte 772b-771b\n"
                       "  .short "
                       "%c[paravirt_clobber]"
                       "\n"
                       ".popsection\n": : [paravirt_typenum] "i" ((unsigned int )(& ((struct paravirt_patch_template *)0)->pv_irq_ops.irq_enable) / sizeof(void *)),
                       [paravirt_opptr] "m" (pv_irq_ops.irq_enable), [paravirt_clobber] "i" (1): "memory",
                       "eax", "cc");
  }
  return;
}
}
__inline static unsigned long __raw_local_irq_save(void) 
{ unsigned long f ;

  {
  {
  f = __raw_local_save_flags();
  raw_local_irq_disable();
  }
  return (f);
}
}
extern unsigned long __per_cpu_offset[64] ;
extern void __bad_percpu_size(void) ;
extern struct task_struct *per_cpu__current_task ;
__inline static struct task_struct *( __attribute__((__always_inline__)) get_current)(void) 
{ struct task_struct *ret__ ;

  {
  if ((int )sizeof(per_cpu__current_task) == 1) {
    goto switch_2_1;
  } else {
    if ((int )sizeof(per_cpu__current_task) == 2) {
      goto switch_2_2;
    } else {
      if ((int )sizeof(per_cpu__current_task) == 4) {
        goto switch_2_4;
      } else {
        {
        goto switch_2_default;
        if (0) {
          switch_2_1: /* CIL Label */ 
          {
          __asm__  ("mov"
                    "b "
                    "%%fs:"
                    "%1,%0": "=r" (ret__): "m" (per_cpu__current_task));
          }
          goto switch_2_break;
          switch_2_2: /* CIL Label */ 
          {
          __asm__  ("mov"
                    "w "
                    "%%fs:"
                    "%1,%0": "=r" (ret__): "m" (per_cpu__current_task));
          }
          goto switch_2_break;
          switch_2_4: /* CIL Label */ 
          {
          __asm__  ("mov"
                    "l "
                    "%%fs:"
                    "%1,%0": "=r" (ret__): "m" (per_cpu__current_task));
          }
          goto switch_2_break;
          switch_2_default: /* CIL Label */ 
          {
          __bad_percpu_size();
          }
        } else {
          switch_2_break: /* CIL Label */ ;
        }
        }
      }
    }
  }
  return (ret__);
}
}
__inline static int raw_irqs_disabled_flags(unsigned long flags ) 
{ 

  {
  return (! (flags & 512UL));
}
}
extern struct cpuinfo_x86 boot_cpu_data ;
__inline static void prefetch(void const   *x ) 
{ 

  {
  {
  __asm__  volatile   ("661:\n\t"
                       ".byte 0x8d,0x74,0x26,0x00\n"
                       "\n662:\n"
                       ".section .altinstructions,\"a\"\n"
                       " "
                       ".balign 4"
                       " "
                       "\n"
                       " "
                       ".long"
                       " "
                       "661b\n"
                       " "
                       ".long"
                       " "
                       "663f\n"
                       "\t .byte %c0\n"
                       "\t .byte 662b-661b\n"
                       "\t .byte 664f-663f\n"
                       ".previous\n"
                       ".section .altinstr_replacement,\"ax\"\n"
                       "663:\n\t"
                       "prefetchnta (%1)"
                       "\n664:\n"
                       ".previous": : "i" (25), "r" (x));
  }
  return;
}
}
__inline static void prefetchw(void const   *x ) 
{ 

  {
  {
  __asm__  volatile   ("661:\n\t"
                       ".byte 0x8d,0x74,0x26,0x00\n"
                       "\n662:\n"
                       ".section .altinstructions,\"a\"\n"
                       " "
                       ".balign 4"
                       " "
                       "\n"
                       " "
                       ".long"
                       " "
                       "661b\n"
                       " "
                       ".long"
                       " "
                       "663f\n"
                       "\t .byte %c0\n"
                       "\t .byte 662b-661b\n"
                       "\t .byte 664f-663f\n"
                       ".previous\n"
                       ".section .altinstr_replacement,\"ax\"\n"
                       "663:\n\t"
                       "prefetchw (%1)"
                       "\n664:\n"
                       ".previous": : "i" (63), "r" (x));
  }
  return;
}
}
register unsigned long current_stack_pointer  __asm__("esp") __attribute__((__used__))  ;
__inline static void INIT_LIST_HEAD(struct list_head *list ) 
{ 

  {
  {
  list->next = list;
  list->prev = list;
  }
  return;
}
}
__inline static void __list_add(struct list_head *new , struct list_head *prev , struct list_head *next ) 
{ 

  {
  {
  next->prev = new;
  new->next = next;
  new->prev = prev;
  prev->next = new;
  }
  return;
}
}
__inline static void list_add(struct list_head *new , struct list_head *head ) 
{ 

  {
  {
  __list_add(new, head, head->next);
  }
  return;
}
}
__inline static void list_add_tail(struct list_head *new , struct list_head *head ) 
{ 

  {
  {
  __list_add(new, head->prev, head);
  }
  return;
}
}
__inline static void __list_del(struct list_head *prev , struct list_head *next ) 
{ 

  {
  {
  next->prev = prev;
  prev->next = next;
  }
  return;
}
}
__inline static void list_del(struct list_head *entry ) 
{ 

  {
  {
  __list_del(entry->prev, entry->next);
  entry->next = (struct list_head *)((void *)1048832);
  entry->prev = (struct list_head *)((void *)2097664);
  }
  return;
}
}
__inline static int list_empty(struct list_head  const  *head ) 
{ 

  {
  return ((unsigned int )head->next == (unsigned int )head);
}
}
__inline static void __list_splice(struct list_head  const  *list , struct list_head *prev ,
                                   struct list_head *next ) 
{ struct list_head *first ;
  struct list_head *last ;

  {
  {
  first = (struct list_head *)list->next;
  last = (struct list_head *)list->prev;
  first->prev = prev;
  prev->next = first;
  last->next = next;
  next->prev = last;
  }
  return;
}
}
__inline static void list_splice(struct list_head  const  *list , struct list_head *head ) 
{ int tmp ;

  {
  {
  tmp = list_empty(list);
  }
  if (! tmp) {
    {
    __list_splice(list, head, head->next);
    }
  }
  return;
}
}
extern void _spin_lock(spinlock_t *lock )  __attribute__((__section__(".spinlock.text"))) ;
extern void _spin_lock_irq(spinlock_t *lock )  __attribute__((__section__(".spinlock.text"))) ;
__inline static int num_node_state(enum node_states state ) 
{ 

  {
  return (1);
}
}
extern struct page *mem_map ;
extern void mutex_lock(struct mutex *lock ) ;
extern int mutex_trylock(struct mutex *lock ) ;
extern void mutex_unlock(struct mutex *lock ) ;
extern int per_cpu__cpu_number ;
extern int on_each_cpu(void (*func)(void *info ) , void *info , int wait ) ;
__inline static cpumask_t const   *_node_to_cpumask_ptr(int node ) 
{ 

  {
  return ((cpumask_t const   *)(& cpu_online_map));
}
}
extern struct pglist_data contig_page_data ;
__inline static int gfp_zonelist(gfp_t flags ) 
{ 

  {
  return (0);
}
}
__inline static struct zonelist *node_zonelist(int nid , gfp_t flags ) 
{ int tmp ;

  {
  {
  tmp = gfp_zonelist(flags);
  }
  return (contig_page_data.node_zonelists + tmp);
}
}
extern struct page *__alloc_pages_internal(gfp_t gfp_mask , unsigned int order , struct zonelist *zonelist ,
                                           nodemask_t *nodemask ) ;
__inline static struct page *__alloc_pages(gfp_t gfp_mask , unsigned int order , struct zonelist *zonelist ) 
{ struct page *tmp ;

  {
  {
  tmp = __alloc_pages_internal(gfp_mask, order, zonelist, (nodemask_t *)((void *)0));
  }
  return (tmp);
}
}
__inline static struct page *alloc_pages_node(int nid , gfp_t gfp_mask , unsigned int order ) 
{ long tmp ;
  struct zonelist *tmp___0 ;
  struct page *tmp___1 ;

  {
  {
  tmp = __builtin_expect((long )(! (! (order >= 11U))), 0L);
  }
  if (tmp) {
    return ((struct page *)((void *)0));
  }
  if (nid < 0) {
    {
    nid = 0;
    }
  }
  {
  tmp___0 = node_zonelist(nid, gfp_mask);
  tmp___1 = __alloc_pages(gfp_mask, order, tmp___0);
  }
  return (tmp___1);
}
}
extern void free_pages(unsigned long addr , unsigned int order ) ;
void __attribute__((__cold__))  kmem_cache_init(void)  __attribute__((__section__(".init.text"))) ;
int slab_is_available(void) ;
struct kmem_cache *kmem_cache_create(char const   * , size_t  , size_t  , unsigned long  ,
                                     void (*)(void * ) ) ;
void kmem_cache_destroy(struct kmem_cache * ) ;
int kmem_cache_shrink(struct kmem_cache * ) ;
void kmem_cache_free(struct kmem_cache * , void * ) ;
unsigned int kmem_cache_size(struct kmem_cache * ) ;
char const   *kmem_cache_name(struct kmem_cache * ) ;
void kfree(void const   *objp___7 ) ;
size_t ksize(void const   *objp___8 ) ;
extern unsigned long volatile   jiffies  __attribute__((__section__(".data"))) ;
__inline static void debug_check_no_obj_freed(void const   *address , unsigned long size ) 
{ 

  {
  return;
}
}
extern void init_timer(struct timer_list *timer ) ;
extern unsigned long __round_jiffies_relative(unsigned long j , int cpu ) ;
extern unsigned long round_jiffies_relative(unsigned long j ) ;
extern int schedule_delayed_work(struct delayed_work *work , unsigned long delay ) ;
extern int schedule_delayed_work_on(int cpu , struct delayed_work *work , unsigned long delay ) ;
extern int keventd_up(void) ;
extern int cancel_delayed_work_sync(struct delayed_work *work ) ;
__inline static void cancel_rearming_delayed_work(struct delayed_work *work ) 
{ 

  {
  {
  cancel_delayed_work_sync(work);
  }
  return;
}
}
void *kmem_cache_alloc(struct kmem_cache * , gfp_t  ) ;
void *__kmalloc(size_t size___0 , gfp_t flags___8 ) ;
__inline static void *( __attribute__((__always_inline__)) kmalloc)(size_t size ,
                                                                    gfp_t flags ) 
{ void *tmp___2 ;

  {
  {
  tmp___2 = __kmalloc(size, flags);
  }
  return (tmp___2);
}
}
__inline static void *kmalloc_node(size_t size , gfp_t flags , int node ) 
{ void *tmp ;

  {
  {
  tmp = kmalloc(size, flags);
  }
  return (tmp);
}
}
__inline static void *kmem_cache_alloc_node(struct kmem_cache *cachep , gfp_t flags ,
                                            int node ) 
{ void *tmp ;

  {
  {
  tmp = kmem_cache_alloc(cachep, flags);
  }
  return (tmp);
}
}
__inline static void *kzalloc(size_t size , gfp_t flags ) 
{ void *tmp ;

  {
  {
  tmp = kmalloc(size, flags | 32768U);
  }
  return (tmp);
}
}
struct seq_operations  const  slabinfo_op ;
ssize_t slabinfo_write(struct file *file , char const   *buffer , size_t count , loff_t *ppos ) ;
__inline static void debug_check_no_locks_freed(void const   *from , unsigned long len ) 
{ 

  {
  return;
}
}
extern unsigned long num_physpages ;
__inline static int PageSlab(struct page *page ) 
{ int tmp ;

  {
  {
  tmp = constant_test_bit(7, (unsigned long const volatile   *)(& page->flags));
  }
  return (tmp);
}
}
__inline static void __SetPageSlab(struct page *page ) 
{ 

  {
  {
  __set_bit(7, (unsigned long volatile   *)(& page->flags));
  }
  return;
}
}
__inline static void __ClearPageSlab(struct page *page ) 
{ 

  {
  {
  __clear_bit(7, (unsigned long volatile   *)(& page->flags));
  }
  return;
}
}
__inline static int PageTail(struct page *page ) 
{ int tmp ;

  {
  {
  tmp = constant_test_bit(14, (unsigned long const volatile   *)(& page->flags));
  }
  return (tmp);
}
}
__inline static int PageCompound(struct page *page ) 
{ 

  {
  return ((int )(page->flags & (unsigned long )((1L << 13) | (1L << 14))));
}
}
__inline static struct page *compound_head(struct page *page ) 
{ int tmp ;
  int tmp___0 ;
  long tmp___1 ;

  {
  {
  tmp = PageTail(page);
  }
  if (tmp) {
    {
    tmp___0 = 1;
    }
  } else {
    {
    tmp___0 = 0;
    }
  }
  {
  tmp___1 = __builtin_expect((long )tmp___0, 0L);
  }
  if (tmp___1) {
    return (page->__annonCompField12.first_page);
  }
  return (page);
}
}
__inline static struct page *virt_to_head_page(void const   *x ) 
{ struct page *page ;
  struct page *tmp ;

  {
  {
  page = mem_map + ((((unsigned long )x - 3221225472UL) >> 12) - 0UL);
  tmp = compound_head(page);
  }
  return (tmp);
}
}
__inline static enum zone_type page_zonenum(struct page *page ) 
{ 

  {
  return ((enum zone_type )((page->flags >> (((sizeof(unsigned long ) * 8U - 0U) - 0U) - 2U)) & ((1UL << 2) - 1UL)));
}
}
__inline static struct zone *page_zone(struct page *page ) 
{ enum zone_type tmp ;

  {
  {
  tmp = page_zonenum(page);
  }
  return (& contig_page_data.node_zones[tmp]);
}
}
extern void mod_zone_page_state(struct zone * , enum zone_stat_item  , int  ) ;
extern void *page_address(struct page *page ) ;
__inline static int cond_resched(void) 
{ int tmp ;

  {
  {
  tmp = _cond_resched();
  }
  return (tmp);
}
}
extern int seq_putc(struct seq_file *m , char c ) ;
extern int seq_puts(struct seq_file *m , char const   *s ) ;
extern int ( /* format attribute */  seq_printf)(struct seq_file * , char const   * 
                                                 , ...) ;
extern struct list_head *seq_list_start(struct list_head *head , loff_t pos ) ;
extern struct list_head *seq_list_next(void *v , struct list_head *head , loff_t *ppos ) ;
extern int register_cpu_notifier(struct notifier_block *nb ) ;
extern unsigned long copy_from_user(void *to , void const   *from , unsigned long n ) ;
extern u32 reciprocal_value(u32 B ) ;
struct kmem_list3 initkmem_list3[3]  __attribute__((__section__(".init.data")))  ;
static int drain_freelist(struct kmem_cache *cache , struct kmem_list3 *l3 , int tofree ) ;
static void free_block(struct kmem_cache *cachep , void **objpp , int len , int node ) ;
static int enable_cpucache(struct kmem_cache *cachep ) ;
static void cache_reap(struct work_struct *w ) ;
extern void __bad_size(void) ;
__inline static int ( __attribute__((__always_inline__)) index_of)(size_t size ) 
{ 

  {
  {
  __bad_size();
  }
  return (0);
}
}
static int slab_early_init  =    1;
static void kmem_list3_init(struct kmem_list3 *parent ) 
{ spinlock_t __constr_expr_0 ;

  {
  {
  INIT_LIST_HEAD(& parent->slabs_full);
  INIT_LIST_HEAD(& parent->slabs_partial);
  INIT_LIST_HEAD(& parent->slabs_free);
  parent->shared = (struct array_cache *)((void *)0);
  parent->alien = (struct array_cache **)((void *)0);
  parent->colour_next = 0U;
  }
  {
  while (1) {
    while_3_continue: /* CIL Label */ ;
    {
    __constr_expr_0.raw_lock.slock = 0U;
    parent->list_lock = __constr_expr_0;
    }
    goto while_3_break;
  }
  while_3_break: /* CIL Label */ ;
  }
  {
  parent->free_objects = 0UL;
  parent->free_touched = 0;
  }
  return;
}
}
static int slab_break_gfp_order  =    0;
__inline static void page_set_cache(struct page *page , struct kmem_cache___0 *cache ) 
{ 

  {
  {
  page->lru.next = (struct list_head *)cache;
  }
  return;
}
}
__inline static struct kmem_cache___0 *page_get_cache(struct page *page ) 
{ int tmp ;
  int tmp___0 ;
  long tmp___1 ;

  {
  {
  page = compound_head(page);
  }
  {
  while (1) {
    while_4_continue: /* CIL Label */ ;
    {
    tmp = PageSlab(page);
    }
    if (tmp) {
      {
      tmp___0 = 0;
      }
    } else {
      {
      tmp___0 = 1;
      }
    }
    {
    tmp___1 = __builtin_expect((long )tmp___0, 0L);
    }
    if (tmp___1) {
      {
      while (1) {
        while_5_continue: /* CIL Label */ ;
        {
        __asm__  volatile   ("1:\tud2\n"
                             ".pushsection __bug_table,\"a\"\n"
                             "2:\t.long 1b, %c0\n"
                             "\t.word %c1, 0\n"
                             "\t.org 2b+%c2\n"
                             ".popsection": : "i" ("slab.c"), "i" (590), "i" (sizeof(struct bug_entry )));
        }
        {
        while (1) {
          while_6_continue: /* CIL Label */ ;
        }
        while_6_break: /* CIL Label */ ;
        }
        goto while_5_break;
      }
      while_5_break: /* CIL Label */ ;
      }
    }
    goto while_4_break;
  }
  while_4_break: /* CIL Label */ ;
  }
  return ((struct kmem_cache___0 *)page->lru.next);
}
}
__inline static void page_set_slab(struct page *page , struct slab *slab ) 
{ 

  {
  {
  page->lru.prev = (struct list_head *)slab;
  }
  return;
}
}
__inline static struct kmem_cache___0 *virt_to_cache(void const   *obj ) 
{ struct page *page ;
  struct page *tmp ;
  struct kmem_cache___0 *tmp___0 ;

  {
  {
  tmp = virt_to_head_page(obj);
  page = tmp;
  tmp___0 = page_get_cache(page);
  }
  return (tmp___0);
}
}
__inline static void *index_to_obj(struct kmem_cache___0 *cache , struct slab *slab ,
                                   unsigned int idx ) 
{ 

  {
  return (slab->s_mem + cache->buffer_size * idx);
}
}
__asm__("booo - error in global malloc_sizes (slab.c:639)");
extern void *__crc_malloc_sizes  __attribute__((__weak__)) ;
static unsigned long const   __kcrctab_malloc_sizes  __attribute__((__used__, __unused__,
__section__("__kcrctab")))  =    (unsigned long const   )((unsigned long )(& __crc_malloc_sizes));
static char const   __kstrtab_malloc_sizes[13]  __attribute__((__section__("__ksymtab_strings"),
__aligned__(1)))  = 
  {      (char const   )'m',      (char const   )'a',      (char const   )'l',      (char const   )'l', 
        (char const   )'o',      (char const   )'c',      (char const   )'_',      (char const   )'s', 
        (char const   )'i',      (char const   )'z',      (char const   )'e',      (char const   )'s', 
        (char const   )'\000'};
static struct kernel_symbol  const  __ksymtab_malloc_sizes  __attribute__((__used__,
__unused__, __section__("__ksymtab")))  =    {(unsigned long )(& malloc_sizes), __kstrtab_malloc_sizes};
static struct cache_names cache_names[19]  __attribute__((__section__(".init.data")))  = 
  {      {(char *)"size-32", (char *)"size-32(DMA)"}, 
        {(char *)"size-64", (char *)"size-64(DMA)"}, 
        {(char *)"size-128", (char *)"size-128(DMA)"}, 
        {(char *)"size-256", (char *)"size-256(DMA)"}, 
        {(char *)"size-512", (char *)"size-512(DMA)"}, 
        {(char *)"size-1024", (char *)"size-1024(DMA)"}, 
        {(char *)"size-2048", (char *)"size-2048(DMA)"}, 
        {(char *)"size-4096", (char *)"size-4096(DMA)"}, 
        {(char *)"size-8192", (char *)"size-8192(DMA)"}, 
        {(char *)"size-16384", (char *)"size-16384(DMA)"}, 
        {(char *)"size-32768", (char *)"size-32768(DMA)"}, 
        {(char *)"size-65536", (char *)"size-65536(DMA)"}, 
        {(char *)"size-131072", (char *)"size-131072(DMA)"}, 
        {(char *)"size-262144", (char *)"size-262144(DMA)"}, 
        {(char *)"size-524288", (char *)"size-524288(DMA)"}, 
        {(char *)"size-1048576", (char *)"size-1048576(DMA)"}, 
        {(char *)"size-2097152", (char *)"size-2097152(DMA)"}, 
        {(char *)"size-4194304", (char *)"size-4194304(DMA)"}, 
        {(char *)((void *)0), (char *)0}};
static struct arraycache_init initarray_cache  __attribute__((__section__(".init.data")))  =    {{0U,
     1U, 1U, 0U, {{0U}}, {}}, {(void *)0}};
static struct arraycache_init initarray_generic  =    {{0U, 1U, 1U, 0U, {{0U}}, {}}, {(void *)0}};
static struct kmem_cache___0 cache_cache  = 
     {{(struct array_cache *)0, (struct array_cache *)0, (struct array_cache *)0, (struct array_cache *)0,
     (struct array_cache *)0, (struct array_cache *)0, (struct array_cache *)0, (struct array_cache *)0,
     (struct array_cache *)0, (struct array_cache *)0, (struct array_cache *)0, (struct array_cache *)0,
     (struct array_cache *)0, (struct array_cache *)0, (struct array_cache *)0, (struct array_cache *)0,
     (struct array_cache *)0, (struct array_cache *)0, (struct array_cache *)0, (struct array_cache *)0,
     (struct array_cache *)0, (struct array_cache *)0, (struct array_cache *)0, (struct array_cache *)0,
     (struct array_cache *)0, (struct array_cache *)0, (struct array_cache *)0, (struct array_cache *)0,
     (struct array_cache *)0, (struct array_cache *)0, (struct array_cache *)0, (struct array_cache *)0,
     (struct array_cache *)0, (struct array_cache *)0, (struct array_cache *)0, (struct array_cache *)0,
     (struct array_cache *)0, (struct array_cache *)0, (struct array_cache *)0, (struct array_cache *)0,
     (struct array_cache *)0, (struct array_cache *)0, (struct array_cache *)0, (struct array_cache *)0,
     (struct array_cache *)0, (struct array_cache *)0, (struct array_cache *)0, (struct array_cache *)0,
     (struct array_cache *)0, (struct array_cache *)0, (struct array_cache *)0, (struct array_cache *)0,
     (struct array_cache *)0, (struct array_cache *)0, (struct array_cache *)0, (struct array_cache *)0,
     (struct array_cache *)0, (struct array_cache *)0, (struct array_cache *)0, (struct array_cache *)0,
     (struct array_cache *)0, (struct array_cache *)0, (struct array_cache *)0, (struct array_cache *)0},
    1U, 1U, 1U, sizeof(struct kmem_cache___0 ), 0U, 0U, 0U, 0U, 0U, 0U, 0U, (struct kmem_cache *)0,
    0U, 0U, (void (*)(void *obj ))0, "kmem_cache", {(struct list_head *)0, (struct list_head *)0},
    {(struct kmem_list3 *)0}};
__inline static void init_lock_keys(void) 
{ 

  {
  return;
}
}
static struct mutex cache_chain_mutex  =    {{1}, {{0U}}, {& cache_chain_mutex.wait_list, & cache_chain_mutex.wait_list}};
static struct list_head cache_chain  ;
static enum __anonenum_g_cpucache_up_205 g_cpucache_up  ;
int slab_is_available(void) 
{ 

  {
  return ((int )g_cpucache_up == 3);
}
}
static struct delayed_work per_cpu__reap_work  __attribute__((__section__(".data.percpu")))  ;
__inline static struct array_cache *cpu_cache_get(struct kmem_cache___0 *cachep ) 
{ int ret__ ;

  {
  if ((int )sizeof(per_cpu__cpu_number) == 1) {
    goto switch_7_1;
  } else {
    if ((int )sizeof(per_cpu__cpu_number) == 2) {
      goto switch_7_2;
    } else {
      if ((int )sizeof(per_cpu__cpu_number) == 4) {
        goto switch_7_4;
      } else {
        {
        goto switch_7_default;
        if (0) {
          switch_7_1: /* CIL Label */ 
          {
          __asm__  ("mov"
                    "b "
                    "%%fs:"
                    "%1,%0": "=r" (ret__): "m" (per_cpu__cpu_number));
          }
          goto switch_7_break;
          switch_7_2: /* CIL Label */ 
          {
          __asm__  ("mov"
                    "w "
                    "%%fs:"
                    "%1,%0": "=r" (ret__): "m" (per_cpu__cpu_number));
          }
          goto switch_7_break;
          switch_7_4: /* CIL Label */ 
          {
          __asm__  ("mov"
                    "l "
                    "%%fs:"
                    "%1,%0": "=r" (ret__): "m" (per_cpu__cpu_number));
          }
          goto switch_7_break;
          switch_7_default: /* CIL Label */ 
          {
          __bad_percpu_size();
          }
        } else {
          switch_7_break: /* CIL Label */ ;
        }
        }
      }
    }
  }
  return (cachep->array[ret__]);
}
}
__inline static struct kmem_cache___0 *__find_general_cachep(size_t size , gfp_t gfpflags ) 
{ struct cache_sizes *csizep ;
  long tmp ;

  {
  {
  csizep = malloc_sizes;
  }
  if (! size) {
    return ((struct kmem_cache___0 *)((void *)16));
  }
  {
  while (1) {
    while_8_continue: /* CIL Label */ ;
    {
    __asm__  ("booo_exp(slab.c:778)":);
    }
    if (! (size > 0U)) {
      goto while_8_break;
    }
    {
    csizep ++;
    }
  }
  while_8_break: /* CIL Label */ ;
  }
  {
  tmp = __builtin_expect((long )(! (! (gfpflags & 1U))), 0L);
  }
  if (tmp) {
    {
    __asm__  ("booo_exp(slab.c:788)":);
    }
    return ((struct kmem_cache___0 *)0);
  }
  {
  __asm__  ("booo_exp(slab.c:790)":);
  }
  return ((struct kmem_cache___0 *)0);
}
}
static size_t slab_mgmt_size(size_t nr_objs , size_t align ) 
{ 

  {
  return (((sizeof(struct slab ) + nr_objs * sizeof(kmem_bufctl_t )) + (align - 1U)) & ~ (align - 1U));
}
}
static void cache_estimate(unsigned long gfporder , size_t buffer_size , size_t align ,
                           int flags , size_t *left_over , unsigned int *num ) 
{ int nr_objs ;
  size_t mgmt_size ;
  size_t slab_size ;
  size_t tmp ;

  {
  {
  slab_size = (size_t )((1UL << 12) << gfporder);
  }
  if ((unsigned long )flags & 2147483648UL) {
    {
    mgmt_size = 0U;
    nr_objs = (int )(slab_size / buffer_size);
    }
    if ((unsigned int )nr_objs > 4294967292U) {
      {
      nr_objs = -4;
      }
    }
  } else {
    {
    nr_objs = (int )((slab_size - sizeof(struct slab )) / (buffer_size + sizeof(kmem_bufctl_t )));
    tmp = slab_mgmt_size((unsigned int )nr_objs, align);
    }
    if (tmp + (size_t )nr_objs * buffer_size > slab_size) {
      {
      nr_objs --;
      }
    }
    if ((unsigned int )nr_objs > 4294967292U) {
      {
      nr_objs = -4;
      }
    }
    {
    mgmt_size = slab_mgmt_size((unsigned int )nr_objs, align);
    }
  }
  {
  *num = (unsigned int )nr_objs;
  *left_over = (slab_size - (size_t )nr_objs * buffer_size) - mgmt_size;
  }
  return;
}
}
static int use_alien_caches  __attribute__((__section__(".data.read_mostly")))  =    1;
static int numa_platform  __attribute__((__section__(".data.read_mostly")))  =    1;
static int __attribute__((__cold__))  noaliencache_setup(char *s )  __attribute__((__section__(".init.text"))) ;
static int __attribute__((__cold__))  noaliencache_setup(char *s ) 
{ 

  {
  {
  use_alien_caches = 0;
  }
  return ((int __attribute__((__cold__))  )1);
}
}
static char __setup_str_noaliencache_setup[13]  __attribute__((__section__(".init.data"),
__aligned__(1)))  = 
  {      (char )'n',      (char )'o',      (char )'a',      (char )'l', 
        (char )'i',      (char )'e',      (char )'n',      (char )'c', 
        (char )'a',      (char )'c',      (char )'h',      (char )'e', 
        (char )'\000'};
static struct obs_kernel_param __setup_noaliencache_setup  __attribute__((__used__,
__section__(".init.setup"), __aligned__(sizeof(long ))))  =    {(char const   *)(__setup_str_noaliencache_setup), (int (*)(char * ))(& noaliencache_setup),
    0};
static void __attribute__((__cold__))  start_cpu_timer(int cpu )  __attribute__((__section__(".cpuinit.text"))) ;
static void __attribute__((__cold__))  start_cpu_timer(int cpu ) 
{ struct delayed_work *reap_work ;
  unsigned long __ptr ;
  atomic_long_t __constr_expr_0 ;
  unsigned long tmp ;
  int tmp___0 ;

  {
  {
  __asm__  ("": "=r" (__ptr): "0" (& per_cpu__reap_work));
  reap_work = (struct delayed_work *)(__ptr + __per_cpu_offset[cpu]);
  tmp___0 = keventd_up();
  }
  if (tmp___0) {
    if ((unsigned int )reap_work->work.func == (unsigned int )((void *)0)) {
      {
      while (1) {
        while_9_continue: /* CIL Label */ ;
        goto while_9_break;
      }
      while_9_break: /* CIL Label */ ;
      }
      {
      while (1) {
        while_10_continue: /* CIL Label */ ;
        {
        while (1) {
          while_11_continue: /* CIL Label */ ;
          {
          __constr_expr_0.counter = 0;
          reap_work->work.data = __constr_expr_0;
          INIT_LIST_HEAD(& reap_work->work.entry);
          }
          {
          while (1) {
            while_12_continue: /* CIL Label */ ;
            {
            reap_work->work.func = & cache_reap;
            }
            goto while_12_break;
          }
          while_12_break: /* CIL Label */ ;
          }
          goto while_11_break;
        }
        while_11_break: /* CIL Label */ ;
        }
        {
        init_timer(& reap_work->timer);
        }
        goto while_10_break;
      }
      while_10_break: /* CIL Label */ ;
      }
      {
      tmp = __round_jiffies_relative(250UL, cpu);
      schedule_delayed_work_on(cpu, reap_work, tmp);
      }
    }
  }
  return;
}
}
static struct array_cache *alloc_arraycache(int node , int entries , int batchcount ) 
{ int memsize ;
  struct array_cache *nc ;
  void *tmp ;
  spinlock_t __constr_expr_0 ;

  {
  {
  memsize = (int )(sizeof(void *) * (unsigned int )entries + sizeof(struct array_cache ));
  nc = (struct array_cache *)((void *)0);
  tmp = kmalloc_node((unsigned int )memsize, 208U, node);
  nc = (struct array_cache *)tmp;
  }
  if (nc) {
    {
    nc->avail = 0U;
    nc->limit = (unsigned int )entries;
    nc->batchcount = (unsigned int )batchcount;
    nc->touched = 0U;
    }
    {
    while (1) {
      while_13_continue: /* CIL Label */ ;
      {
      __constr_expr_0.raw_lock.slock = 0U;
      nc->lock = __constr_expr_0;
      }
      goto while_13_break;
    }
    while_13_break: /* CIL Label */ ;
    }
  }
  return (nc);
}
}
static int transfer_objects(struct array_cache *to , struct array_cache *from , unsigned int max ) 
{ int nr ;
  unsigned int _min1___0 ;
  unsigned int _min1___1 ;
  unsigned int _min2___0 ;
  unsigned int tmp___0 ;
  unsigned int _min2___1 ;
  unsigned int tmp___1 ;

  {
  {
  _min1___1 = from->avail;
  _min2___0 = max;
  }
  if (_min1___1 < _min2___0) {
    {
    tmp___0 = _min1___1;
    }
  } else {
    {
    tmp___0 = _min2___0;
    }
  }
  {
  _min1___0 = tmp___0;
  _min2___1 = to->limit - to->avail;
  }
  if (_min1___0 < _min2___1) {
    {
    tmp___1 = _min1___0;
    }
  } else {
    {
    tmp___1 = _min2___1;
    }
  }
  {
  nr = (int )tmp___1;
  }
  if (! nr) {
    return (0);
  }
  {
  __memcpy((void *)(to->entry + to->avail), (void const   *)((from->entry + from->avail) - nr),
           sizeof(void *) * (unsigned int )nr);
  from->avail -= (unsigned int )nr;
  to->avail += (unsigned int )nr;
  to->touched = 1U;
  }
  return (nr);
}
}
__inline static struct array_cache **alloc_alien_cache(int node , int limit ) 
{ 

  {
  return ((struct array_cache **)16909060UL);
}
}
__inline static void free_alien_cache(struct array_cache **ac_ptr ) 
{ 

  {
  return;
}
}
__inline static int cache_free_alien(struct kmem_cache___0 *cachep , void *objp ) 
{ 

  {
  return (0);
}
}
static void __attribute__((__cold__))  cpuup_canceled(long cpu )  __attribute__((__section__(".cpuinit.text"))) ;
static void __attribute__((__cold__))  cpuup_canceled(long cpu ) 
{ struct kmem_cache___0 *cachep ;
  struct kmem_list3 *l3 ;
  int node ;
  cpumask_t const   *mask ;
  cpumask_t const   *tmp ;
  struct list_head  const  *__mptr ;
  struct list_head  const  *__mptr___0 ;
  struct array_cache *nc ;
  struct array_cache *shared ;
  struct array_cache **alien ;
  int tmp___0 ;
  struct list_head  const  *__mptr___1 ;
  struct list_head  const  *__mptr___2 ;

  {
  {
  l3 = (struct kmem_list3 *)((void *)0);
  node = 0;
  tmp = _node_to_cpumask_ptr(node);
  mask = tmp;
  __mptr = (struct list_head  const  *)cache_chain.next;
  cachep = (struct kmem_cache___0 *)((char *)__mptr - (unsigned int )(& ((struct kmem_cache___0 *)0)->next));
  }
  {
  while (1) {
    while_14_continue: /* CIL Label */ ;
    {
    prefetch((void const   *)cachep->next.next);
    }
    if (! ((unsigned int )(& cachep->next) != (unsigned int )(& cache_chain))) {
      goto while_14_break;
    }
    {
    nc = cachep->array[cpu];
    cachep->array[cpu] = (struct array_cache *)((void *)0);
    l3 = cachep->nodelists[node];
    }
    if (! l3) {
      goto free_array_cache;
    }
    {
    _spin_lock_irq(& l3->list_lock);
    l3->free_limit -= cachep->batchcount;
    }
    if (nc) {
      {
      free_block((struct kmem_cache *)cachep, nc->entry, (int )nc->avail, node);
      }
    }
    {
    tmp___0 = __cpus_empty(mask, 64);
    }
    if (! tmp___0) {
      {
      while (1) {
        while_15_continue: /* CIL Label */ ;
        {
        __raw_spin_unlock(& l3->list_lock.raw_lock);
        }
        {
        while (1) {
          while_16_continue: /* CIL Label */ ;
          {
          while (1) {
            while_17_continue: /* CIL Label */ ;
            goto while_17_break;
          }
          while_17_break: /* CIL Label */ ;
          }
          {
          raw_local_irq_enable();
          }
          goto while_16_break;
        }
        while_16_break: /* CIL Label */ ;
        }
        goto while_15_break;
      }
      while_15_break: /* CIL Label */ ;
      }
      goto free_array_cache;
    }
    {
    shared = l3->shared;
    }
    if (shared) {
      {
      free_block((struct kmem_cache *)cachep, shared->entry, (int )shared->avail,
                 node);
      l3->shared = (struct array_cache *)((void *)0);
      }
    }
    {
    alien = l3->alien;
    l3->alien = (struct array_cache **)((void *)0);
    }
    {
    while (1) {
      while_18_continue: /* CIL Label */ ;
      {
      __raw_spin_unlock(& l3->list_lock.raw_lock);
      }
      {
      while (1) {
        while_19_continue: /* CIL Label */ ;
        {
        while (1) {
          while_20_continue: /* CIL Label */ ;
          goto while_20_break;
        }
        while_20_break: /* CIL Label */ ;
        }
        {
        raw_local_irq_enable();
        }
        goto while_19_break;
      }
      while_19_break: /* CIL Label */ ;
      }
      goto while_18_break;
    }
    while_18_break: /* CIL Label */ ;
    }
    {
    kfree((void const   *)shared);
    }
    if (alien) {
      {
      while (1) {
        while_21_continue: /* CIL Label */ ;
        goto while_21_break;
      }
      while_21_break: /* CIL Label */ ;
      }
      {
      free_alien_cache(alien);
      }
    }
    free_array_cache: 
    {
    kfree((void const   *)nc);
    __mptr___0 = (struct list_head  const  *)cachep->next.next;
    cachep = (struct kmem_cache___0 *)((char *)__mptr___0 - (unsigned int )(& ((struct kmem_cache___0 *)0)->next));
    }
  }
  while_14_break: /* CIL Label */ ;
  }
  {
  __mptr___1 = (struct list_head  const  *)cache_chain.next;
  cachep = (struct kmem_cache___0 *)((char *)__mptr___1 - (unsigned int )(& ((struct kmem_cache___0 *)0)->next));
  }
  {
  while (1) {
    while_22_continue: /* CIL Label */ ;
    {
    prefetch((void const   *)cachep->next.next);
    }
    if (! ((unsigned int )(& cachep->next) != (unsigned int )(& cache_chain))) {
      goto while_22_break;
    }
    {
    l3 = cachep->nodelists[node];
    }
    if (! l3) {
      goto __Cont;
    }
    {
    drain_freelist((struct kmem_cache *)cachep, l3, (int )l3->free_objects);
    }
    __Cont: /* CIL Label */ 
    {
    __mptr___2 = (struct list_head  const  *)cachep->next.next;
    cachep = (struct kmem_cache___0 *)((char *)__mptr___2 - (unsigned int )(& ((struct kmem_cache___0 *)0)->next));
    }
  }
  while_22_break: /* CIL Label */ ;
  }
  return;
}
}
static int __attribute__((__cold__))  cpuup_prepare(long cpu )  __attribute__((__section__(".cpuinit.text"))) ;
static int __attribute__((__cold__))  cpuup_prepare(long cpu ) 
{ struct kmem_cache___0 *cachep ;
  struct kmem_list3 *l3 ;
  int node ;
  int memsize ;
  struct list_head  const  *__mptr ;
  struct list_head  const  *__mptr___0 ;
  void *tmp ;
  cpumask_t const   *__tmp__ ;
  cpumask_t const   *tmp___0 ;
  int tmp___1 ;
  struct list_head  const  *__mptr___1 ;
  struct list_head  const  *__mptr___2 ;
  struct array_cache *nc ;
  struct array_cache *shared ;
  struct array_cache **alien ;
  long tmp___2 ;

  {
  {
  l3 = (struct kmem_list3 *)((void *)0);
  node = 0;
  memsize = (int )sizeof(struct kmem_list3 );
  __mptr = (struct list_head  const  *)cache_chain.next;
  cachep = (struct kmem_cache___0 *)((char *)__mptr - (unsigned int )(& ((struct kmem_cache___0 *)0)->next));
  }
  {
  while (1) {
    while_23_continue: /* CIL Label */ ;
    {
    prefetch((void const   *)cachep->next.next);
    }
    if (! ((unsigned int )(& cachep->next) != (unsigned int )(& cache_chain))) {
      goto while_23_break;
    }
    if (! cachep->nodelists[node]) {
      {
      tmp = kmalloc_node((unsigned int )memsize, 208U, node);
      l3 = (struct kmem_list3 *)tmp;
      }
      if (! l3) {
        goto bad;
      }
      {
      kmem_list3_init(l3);
      l3->next_reap = (unsigned long )((jiffies + (unsigned long volatile   )1000) + (unsigned long volatile   )((unsigned long )cachep % 1000UL));
      cachep->nodelists[node] = l3;
      }
    }
    {
    _spin_lock_irq(& (cachep->nodelists[node])->list_lock);
    tmp___0 = _node_to_cpumask_ptr(node);
    __tmp__ = tmp___0;
    tmp___1 = __cpus_weight(__tmp__, 64);
    (cachep->nodelists[node])->free_limit = (unsigned int )(1 + tmp___1) * cachep->batchcount + cachep->num;
    }
    {
    while (1) {
      while_24_continue: /* CIL Label */ ;
      {
      __raw_spin_unlock(& (cachep->nodelists[node])->list_lock.raw_lock);
      }
      {
      while (1) {
        while_25_continue: /* CIL Label */ ;
        {
        while (1) {
          while_26_continue: /* CIL Label */ ;
          goto while_26_break;
        }
        while_26_break: /* CIL Label */ ;
        }
        {
        raw_local_irq_enable();
        }
        goto while_25_break;
      }
      while_25_break: /* CIL Label */ ;
      }
      goto while_24_break;
    }
    while_24_break: /* CIL Label */ ;
    }
    {
    __mptr___0 = (struct list_head  const  *)cachep->next.next;
    cachep = (struct kmem_cache___0 *)((char *)__mptr___0 - (unsigned int )(& ((struct kmem_cache___0 *)0)->next));
    }
  }
  while_23_break: /* CIL Label */ ;
  }
  {
  __mptr___1 = (struct list_head  const  *)cache_chain.next;
  cachep = (struct kmem_cache___0 *)((char *)__mptr___1 - (unsigned int )(& ((struct kmem_cache___0 *)0)->next));
  }
  {
  while (1) {
    while_27_continue: /* CIL Label */ ;
    {
    prefetch((void const   *)cachep->next.next);
    }
    if (! ((unsigned int )(& cachep->next) != (unsigned int )(& cache_chain))) {
      goto while_27_break;
    }
    {
    shared = (struct array_cache *)((void *)0);
    alien = (struct array_cache **)((void *)0);
    nc = alloc_arraycache(node, (int )cachep->limit, (int )cachep->batchcount);
    }
    if (! nc) {
      goto bad;
    }
    if (cachep->shared) {
      {
      shared = alloc_arraycache(node, (int )(cachep->shared * cachep->batchcount),
                                -1163005939);
      }
      if (! shared) {
        {
        kfree((void const   *)nc);
        }
        goto bad;
      }
    }
    if (use_alien_caches) {
      {
      alien = alloc_alien_cache(node, (int )cachep->limit);
      }
      if (! alien) {
        {
        kfree((void const   *)shared);
        kfree((void const   *)nc);
        }
        goto bad;
      }
    }
    {
    cachep->array[cpu] = nc;
    l3 = cachep->nodelists[node];
    }
    {
    while (1) {
      while_28_continue: /* CIL Label */ ;
      {
      tmp___2 = __builtin_expect((long )(! (! (! l3))), 0L);
      }
      if (tmp___2) {
        {
        while (1) {
          while_29_continue: /* CIL Label */ ;
          {
          __asm__  volatile   ("1:\tud2\n"
                               ".pushsection __bug_table,\"a\"\n"
                               "2:\t.long 1b, %c0\n"
                               "\t.word %c1, 0\n"
                               "\t.org 2b+%c2\n"
                               ".popsection": : "i" ("slab.c"), "i" (1297), "i" (sizeof(struct bug_entry )));
          }
          {
          while (1) {
            while_30_continue: /* CIL Label */ ;
          }
          while_30_break: /* CIL Label */ ;
          }
          goto while_29_break;
        }
        while_29_break: /* CIL Label */ ;
        }
      }
      goto while_28_break;
    }
    while_28_break: /* CIL Label */ ;
    }
    {
    _spin_lock_irq(& l3->list_lock);
    }
    if (! l3->shared) {
      {
      l3->shared = shared;
      shared = (struct array_cache *)((void *)0);
      }
    }
    {
    while (1) {
      while_31_continue: /* CIL Label */ ;
      {
      __raw_spin_unlock(& l3->list_lock.raw_lock);
      }
      {
      while (1) {
        while_32_continue: /* CIL Label */ ;
        {
        while (1) {
          while_33_continue: /* CIL Label */ ;
          goto while_33_break;
        }
        while_33_break: /* CIL Label */ ;
        }
        {
        raw_local_irq_enable();
        }
        goto while_32_break;
      }
      while_32_break: /* CIL Label */ ;
      }
      goto while_31_break;
    }
    while_31_break: /* CIL Label */ ;
    }
    {
    kfree((void const   *)shared);
    free_alien_cache(alien);
    __mptr___2 = (struct list_head  const  *)cachep->next.next;
    cachep = (struct kmem_cache___0 *)((char *)__mptr___2 - (unsigned int )(& ((struct kmem_cache___0 *)0)->next));
    }
  }
  while_27_break: /* CIL Label */ ;
  }
  return ((int __attribute__((__cold__))  )0);
  bad: 
  {
  cpuup_canceled(cpu);
  }
  return ((int __attribute__((__cold__))  )-12);
}
}
static int __attribute__((__cold__))  cpuup_callback(struct notifier_block *nfb ,
                                                     unsigned long action , void *hcpu )  __attribute__((__section__(".cpuinit.text"))) ;
static int __attribute__((__cold__))  cpuup_callback(struct notifier_block *nfb ,
                                                     unsigned long action , void *hcpu ) 
{ long cpu ;
  int err ;
  int __attribute__((__cold__))  tmp ;
  unsigned long __ptr ;
  unsigned long __ptr___0 ;
  int tmp___0 ;

  {
  {
  cpu = (long )hcpu;
  err = 0;
  }
  if ((int )action == 3) {
    goto switch_34_3;
  } else {
    if ((int )action == 19) {
      goto switch_34_3;
    } else {
      if ((int )action == 2) {
        goto switch_34_2;
      } else {
        if ((int )action == 18) {
          goto switch_34_2;
        } else {
          if ((int )action == 5) {
            goto switch_34_5;
          } else {
            if ((int )action == 21) {
              goto switch_34_5;
            } else {
              if ((int )action == 6) {
                goto switch_34_6;
              } else {
                if ((int )action == 22) {
                  goto switch_34_6;
                } else {
                  if ((int )action == 7) {
                    goto switch_34_7;
                  } else {
                    if ((int )action == 23) {
                      goto switch_34_7;
                    } else {
                      if ((int )action == 4) {
                        goto switch_34_7;
                      } else {
                        if ((int )action == 20) {
                          goto switch_34_7;
                        } else {
                          if (0) {
                            switch_34_3: /* CIL Label */ 
                            switch_34_19: /* CIL Label */ 
                            {
                            mutex_lock(& cache_chain_mutex);
                            tmp = cpuup_prepare(cpu);
                            err = (int )tmp;
                            mutex_unlock(& cache_chain_mutex);
                            }
                            goto switch_34_break;
                            switch_34_2: /* CIL Label */ 
                            switch_34_18: /* CIL Label */ 
                            {
                            start_cpu_timer((int )cpu);
                            }
                            goto switch_34_break;
                            switch_34_5: /* CIL Label */ 
                            switch_34_21: /* CIL Label */ 
                            {
                            __asm__  ("": "=r" (__ptr): "0" (& per_cpu__reap_work));
                            cancel_rearming_delayed_work((struct delayed_work *)(__ptr + __per_cpu_offset[cpu]));
                            __asm__  ("": "=r" (__ptr___0): "0" (& per_cpu__reap_work));
                            ((struct delayed_work *)(__ptr___0 + __per_cpu_offset[cpu]))->work.func = (void (*)(struct work_struct *work ))((void *)0);
                            }
                            goto switch_34_break;
                            switch_34_6: /* CIL Label */ 
                            switch_34_22: /* CIL Label */ 
                            {
                            start_cpu_timer((int )cpu);
                            }
                            goto switch_34_break;
                            switch_34_7: /* CIL Label */ 
                            switch_34_23: /* CIL Label */ 
                            switch_34_4: /* CIL Label */ 
                            switch_34_20: /* CIL Label */ 
                            {
                            mutex_lock(& cache_chain_mutex);
                            cpuup_canceled(cpu);
                            mutex_unlock(& cache_chain_mutex);
                            }
                            goto switch_34_break;
                          } else {
                            switch_34_break: /* CIL Label */ ;
                          }
                        }
                      }
                    }
                  }
                }
              }
            }
          }
        }
      }
    }
  }
  if (err) {
    {
    tmp___0 = 32770;
    }
  } else {
    {
    tmp___0 = 1;
    }
  }
  return ((int __attribute__((__cold__))  )tmp___0);
}
}
static struct notifier_block cpucache_notifier  __attribute__((__section__(".cpuinit.data")))  =    {(int (*)(struct notifier_block * ,
             unsigned long  , void * ))(& cpuup_callback), (struct notifier_block *)((void *)0),
    0};
static void init_list(struct kmem_cache___0 *cachep , struct kmem_list3 *list , int nodeid ) 
{ struct kmem_list3 *ptr ;
  void *tmp ;
  long tmp___0 ;
  spinlock_t __constr_expr_0 ;

  {
  {
  tmp = kmalloc_node(sizeof(struct kmem_list3 ), 208U, nodeid);
  ptr = (struct kmem_list3 *)tmp;
  }
  {
  while (1) {
    while_35_continue: /* CIL Label */ ;
    {
    tmp___0 = __builtin_expect((long )(! (! (! ptr))), 0L);
    }
    if (tmp___0) {
      {
      while (1) {
        while_36_continue: /* CIL Label */ ;
        {
        __asm__  volatile   ("1:\tud2\n"
                             ".pushsection __bug_table,\"a\"\n"
                             "2:\t.long 1b, %c0\n"
                             "\t.word %c1, 0\n"
                             "\t.org 2b+%c2\n"
                             ".popsection": : "i" ("slab.c"), "i" (1393), "i" (sizeof(struct bug_entry )));
        }
        {
        while (1) {
          while_37_continue: /* CIL Label */ ;
        }
        while_37_break: /* CIL Label */ ;
        }
        goto while_36_break;
      }
      while_36_break: /* CIL Label */ ;
      }
    }
    goto while_35_break;
  }
  while_35_break: /* CIL Label */ ;
  }
  {
  while (1) {
    while_38_continue: /* CIL Label */ ;
    {
    raw_local_irq_disable();
    }
    {
    while (1) {
      while_39_continue: /* CIL Label */ ;
      goto while_39_break;
    }
    while_39_break: /* CIL Label */ ;
    }
    goto while_38_break;
  }
  while_38_break: /* CIL Label */ ;
  }
  {
  __constant_memcpy((void *)ptr, (void const   *)list, sizeof(struct kmem_list3 ));
  }
  {
  while (1) {
    while_40_continue: /* CIL Label */ ;
    {
    __constr_expr_0.raw_lock.slock = 0U;
    ptr->list_lock = __constr_expr_0;
    }
    goto while_40_break;
  }
  while_40_break: /* CIL Label */ ;
  }
  {
  while (1) {
    while_41_continue: /* CIL Label */ ;
    {
    while (1) {
      while_42_continue: /* CIL Label */ ;
      {
      INIT_LIST_HEAD(& ptr->slabs_full);
      list_splice((struct list_head  const  *)(& (cachep->nodelists[nodeid])->slabs_full),
                  & ptr->slabs_full);
      }
      goto while_42_break;
    }
    while_42_break: /* CIL Label */ ;
    }
    {
    while (1) {
      while_43_continue: /* CIL Label */ ;
      {
      INIT_LIST_HEAD(& ptr->slabs_partial);
      list_splice((struct list_head  const  *)(& (cachep->nodelists[nodeid])->slabs_partial),
                  & ptr->slabs_partial);
      }
      goto while_43_break;
    }
    while_43_break: /* CIL Label */ ;
    }
    {
    while (1) {
      while_44_continue: /* CIL Label */ ;
      {
      INIT_LIST_HEAD(& ptr->slabs_free);
      list_splice((struct list_head  const  *)(& (cachep->nodelists[nodeid])->slabs_free),
                  & ptr->slabs_free);
      }
      goto while_44_break;
    }
    while_44_break: /* CIL Label */ ;
    }
    goto while_41_break;
  }
  while_41_break: /* CIL Label */ ;
  }
  {
  cachep->nodelists[nodeid] = ptr;
  }
  {
  while (1) {
    while_45_continue: /* CIL Label */ ;
    {
    while (1) {
      while_46_continue: /* CIL Label */ ;
      goto while_46_break;
    }
    while_46_break: /* CIL Label */ ;
    }
    {
    raw_local_irq_enable();
    }
    goto while_45_break;
  }
  while_45_break: /* CIL Label */ ;
  }
  return;
}
}
static void __attribute__((__cold__))  set_up_list3s(struct kmem_cache___0 *cachep ,
                                                     int index )  __attribute__((__section__(".init.text"))) ;
static void __attribute__((__cold__))  set_up_list3s(struct kmem_cache___0 *cachep ,
                                                     int index ) 
{ int node ;

  {
  {
  node = 0;
  }
  {
  while (1) {
    while_47_continue: /* CIL Label */ ;
    if (! (node == 0)) {
      goto while_47_break;
    }
    {
    cachep->nodelists[node] = & initkmem_list3[index + node];
    (cachep->nodelists[node])->next_reap = (unsigned long )((jiffies + (unsigned long volatile   )1000) + (unsigned long volatile   )((unsigned long )cachep % 1000UL));
    node = 1;
    }
  }
  while_47_break: /* CIL Label */ ;
  }
  return;
}
}
void __attribute__((__cold__))  kmem_cache_init(void)  __attribute__((__section__(".init.text"))) ;
void __attribute__((__cold__))  kmem_cache_init(void) 
{ size_t left_over ;
  struct cache_sizes *sizes ;
  struct cache_names *names ;
  int i ;
  int order ;
  int node ;
  int tmp ;
  int ret__ ;
  long tmp___0 ;
  int tmp___3 ;
  int tmp___4 ;
  struct array_cache *ptr ;
  void *tmp___5 ;
  struct array_cache *tmp___6 ;
  int tmp___7 ;
  long tmp___8 ;
  struct array_cache *tmp___10 ;
  spinlock_t __constr_expr_0 ;
  int ret_____0 ;
  void *tmp___11 ;
  struct array_cache *tmp___13 ;
  int tmp___14 ;
  long tmp___15 ;
  struct array_cache *tmp___19 ;
  spinlock_t __constr_expr_1 ;
  int nid ;
  int tmp___23 ;
  int tmp___24 ;
  struct kmem_cache___0 *cachep ;
  struct list_head  const  *__mptr ;
  struct list_head  const  *__mptr___0 ;
  int tmp___25 ;

  {
  {
  tmp = num_node_state(0);
  }
  if (tmp == 1) {
    {
    use_alien_caches = 0;
    numa_platform = 0;
    }
  }
  {
  i = 0;
  }
  {
  while (1) {
    while_48_continue: /* CIL Label */ ;
    if (! (i < 3)) {
      goto while_48_break;
    }
    {
    kmem_list3_init(& initkmem_list3[i]);
    }
    if (i < 1) {
      {
      cache_cache.nodelists[i] = (struct kmem_list3 *)((void *)0);
      }
    }
    {
    i ++;
    }
  }
  while_48_break: /* CIL Label */ ;
  }
  {
  set_up_list3s(& cache_cache, 0);
  }
  if (num_physpages > (unsigned long )((32 << 20) >> 12)) {
    {
    slab_break_gfp_order = 1;
    }
  }
  {
  node = 0;
  INIT_LIST_HEAD(& cache_chain);
  list_add(& cache_cache.next, & cache_chain);
  cache_cache.colour_off = (unsigned int )boot_cpu_data.x86_cache_alignment;
  }
  if ((int )sizeof(per_cpu__cpu_number) == 1) {
    goto switch_49_1;
  } else {
    if ((int )sizeof(per_cpu__cpu_number) == 2) {
      goto switch_49_2;
    } else {
      if ((int )sizeof(per_cpu__cpu_number) == 4) {
        goto switch_49_4;
      } else {
        {
        goto switch_49_default;
        if (0) {
          switch_49_1: /* CIL Label */ 
          {
          __asm__  ("mov"
                    "b "
                    "%%fs:"
                    "%1,%0": "=r" (ret__): "m" (per_cpu__cpu_number));
          }
          goto switch_49_break;
          switch_49_2: /* CIL Label */ 
          {
          __asm__  ("mov"
                    "w "
                    "%%fs:"
                    "%1,%0": "=r" (ret__): "m" (per_cpu__cpu_number));
          }
          goto switch_49_break;
          switch_49_4: /* CIL Label */ 
          {
          __asm__  ("mov"
                    "l "
                    "%%fs:"
                    "%1,%0": "=r" (ret__): "m" (per_cpu__cpu_number));
          }
          goto switch_49_break;
          switch_49_default: /* CIL Label */ 
          {
          __bad_percpu_size();
          }
        } else {
          switch_49_break: /* CIL Label */ ;
        }
        }
      }
    }
  }
  {
  cache_cache.array[ret__] = & initarray_cache.cache;
  cache_cache.nodelists[node] = & initkmem_list3[node];
  cache_cache.buffer_size = (unsigned int )(& ((struct kmem_cache___0 *)0)->nodelists) + sizeof(struct kmem_list3 *);
  cache_cache.buffer_size = (cache_cache.buffer_size + ((unsigned int )boot_cpu_data.x86_cache_alignment - 1U)) & ~ ((unsigned int )boot_cpu_data.x86_cache_alignment - 1U);
  cache_cache.reciprocal_buffer_size = reciprocal_value(cache_cache.buffer_size);
  order = 0;
  }
  {
  while (1) {
    while_50_continue: /* CIL Label */ ;
    if (! (order < 11)) {
      goto while_50_break;
    }
    {
    cache_estimate((unsigned long )order, cache_cache.buffer_size, (unsigned int )boot_cpu_data.x86_cache_alignment,
                   0, & left_over, & cache_cache.num);
    }
    if (cache_cache.num) {
      goto while_50_break;
    }
    {
    order ++;
    }
  }
  while_50_break: /* CIL Label */ ;
  }
  {
  while (1) {
    while_51_continue: /* CIL Label */ ;
    {
    tmp___0 = __builtin_expect((long )(! (! (! cache_cache.num))), 0L);
    }
    if (tmp___0) {
      {
      while (1) {
        while_52_continue: /* CIL Label */ ;
        {
        __asm__  volatile   ("1:\tud2\n"
                             ".pushsection __bug_table,\"a\"\n"
                             "2:\t.long 1b, %c0\n"
                             "\t.word %c1, 0\n"
                             "\t.org 2b+%c2\n"
                             ".popsection": : "i" ("slab.c"), "i" (1504), "i" (sizeof(struct bug_entry )));
        }
        {
        while (1) {
          while_53_continue: /* CIL Label */ ;
        }
        while_53_break: /* CIL Label */ ;
        }
        goto while_52_break;
      }
      while_52_break: /* CIL Label */ ;
      }
    }
    goto while_51_break;
  }
  while_51_break: /* CIL Label */ ;
  }
  {
  cache_cache.gfporder = (unsigned int )order;
  cache_cache.colour = left_over / cache_cache.colour_off;
  cache_cache.slab_size = ((cache_cache.num * sizeof(kmem_bufctl_t ) + sizeof(struct slab )) + ((unsigned int )boot_cpu_data.x86_cache_alignment - 1U)) & ~ ((unsigned int )boot_cpu_data.x86_cache_alignment - 1U);
  sizes = malloc_sizes;
  names = cache_names;
  __asm__  ("booo_exp(slab.c:1520)":);
  tmp___3 = index_of(sizeof(struct arraycache_init ));
  tmp___4 = index_of(sizeof(struct kmem_list3 ));
  }
  if (tmp___3 != tmp___4) {
    {
    __asm__  ("booo_exp(slab.c:1527)":);
    }
  }
  {
  slab_early_init = 0;
  }
  {
  while (1) {
    while_54_continue: /* CIL Label */ ;
    {
    __asm__  ("booo_exp(slab.c:1537)":);
    __asm__  ("booo_exp(slab.c:1545)":);
    __asm__  ("booo_exp(slab.c:1546)":);
    __asm__  ("booo_exp(slab.c:1553)":);
    sizes ++;
    names ++;
    }
  }
  while_54_break: /* CIL Label */ ;
  }
  {
  tmp___5 = kmalloc(sizeof(struct arraycache_init ), 208U);
  ptr = (struct array_cache *)tmp___5;
  }
  {
  while (1) {
    while_55_continue: /* CIL Label */ ;
    {
    raw_local_irq_disable();
    }
    {
    while (1) {
      while_56_continue: /* CIL Label */ ;
      goto while_56_break;
    }
    while_56_break: /* CIL Label */ ;
    }
    goto while_55_break;
  }
  while_55_break: /* CIL Label */ ;
  }
  {
  while (1) {
    while_57_continue: /* CIL Label */ ;
    {
    tmp___6 = cpu_cache_get(& cache_cache);
    }
    if ((unsigned int )tmp___6 != (unsigned int )(& initarray_cache.cache)) {
      {
      tmp___7 = 1;
      }
    } else {
      {
      tmp___7 = 0;
      }
    }
    {
    tmp___8 = __builtin_expect((long )tmp___7, 0L);
    }
    if (tmp___8) {
      {
      while (1) {
        while_58_continue: /* CIL Label */ ;
        {
        __asm__  volatile   ("1:\tud2\n"
                             ".pushsection __bug_table,\"a\"\n"
                             "2:\t.long 1b, %c0\n"
                             "\t.word %c1, 0\n"
                             "\t.org 2b+%c2\n"
                             ".popsection": : "i" ("slab.c"), "i" (1571), "i" (sizeof(struct bug_entry )));
        }
        {
        while (1) {
          while_59_continue: /* CIL Label */ ;
        }
        while_59_break: /* CIL Label */ ;
        }
        goto while_58_break;
      }
      while_58_break: /* CIL Label */ ;
      }
    }
    goto while_57_break;
  }
  while_57_break: /* CIL Label */ ;
  }
  {
  tmp___10 = cpu_cache_get(& cache_cache);
  __constant_memcpy((void *)ptr, (void const   *)tmp___10, sizeof(struct arraycache_init ));
  }
  {
  while (1) {
    while_60_continue: /* CIL Label */ ;
    {
    __constr_expr_0.raw_lock.slock = 0U;
    ptr->lock = __constr_expr_0;
    }
    goto while_60_break;
  }
  while_60_break: /* CIL Label */ ;
  }
  if ((int )sizeof(per_cpu__cpu_number) == 1) {
    goto switch_61_1;
  } else {
    if ((int )sizeof(per_cpu__cpu_number) == 2) {
      goto switch_61_2;
    } else {
      if ((int )sizeof(per_cpu__cpu_number) == 4) {
        goto switch_61_4;
      } else {
        {
        goto switch_61_default;
        if (0) {
          switch_61_1: /* CIL Label */ 
          {
          __asm__  ("mov"
                    "b "
                    "%%fs:"
                    "%1,%0": "=r" (ret_____0): "m" (per_cpu__cpu_number));
          }
          goto switch_61_break;
          switch_61_2: /* CIL Label */ 
          {
          __asm__  ("mov"
                    "w "
                    "%%fs:"
                    "%1,%0": "=r" (ret_____0): "m" (per_cpu__cpu_number));
          }
          goto switch_61_break;
          switch_61_4: /* CIL Label */ 
          {
          __asm__  ("mov"
                    "l "
                    "%%fs:"
                    "%1,%0": "=r" (ret_____0): "m" (per_cpu__cpu_number));
          }
          goto switch_61_break;
          switch_61_default: /* CIL Label */ 
          {
          __bad_percpu_size();
          }
        } else {
          switch_61_break: /* CIL Label */ ;
        }
        }
      }
    }
  }
  {
  cache_cache.array[ret_____0] = ptr;
  }
  {
  while (1) {
    while_62_continue: /* CIL Label */ ;
    {
    while (1) {
      while_63_continue: /* CIL Label */ ;
      goto while_63_break;
    }
    while_63_break: /* CIL Label */ ;
    }
    {
    raw_local_irq_enable();
    }
    goto while_62_break;
  }
  while_62_break: /* CIL Label */ ;
  }
  {
  tmp___11 = kmalloc(sizeof(struct arraycache_init ), 208U);
  ptr = (struct array_cache *)tmp___11;
  }
  {
  while (1) {
    while_64_continue: /* CIL Label */ ;
    {
    raw_local_irq_disable();
    }
    {
    while (1) {
      while_65_continue: /* CIL Label */ ;
      goto while_65_break;
    }
    while_65_break: /* CIL Label */ ;
    }
    goto while_64_break;
  }
  while_64_break: /* CIL Label */ ;
  }
  {
  while (1) {
    while_66_continue: /* CIL Label */ ;
    {
    __asm__  ("booo_exp(slab.c:1585)":);
    tmp___13 = cpu_cache_get((struct kmem_cache___0 *)0);
    }
    if ((unsigned int )tmp___13 != (unsigned int )(& initarray_generic.cache)) {
      {
      tmp___14 = 1;
      }
    } else {
      {
      tmp___14 = 0;
      }
    }
    {
    tmp___15 = __builtin_expect((long )tmp___14, 0L);
    }
    if (tmp___15) {
      {
      while (1) {
        while_67_continue: /* CIL Label */ ;
        {
        __asm__  volatile   ("1:\tud2\n"
                             ".pushsection __bug_table,\"a\"\n"
                             "2:\t.long 1b, %c0\n"
                             "\t.word %c1, 0\n"
                             "\t.org 2b+%c2\n"
                             ".popsection": : "i" ("slab.c"), "i" (1586), "i" (sizeof(struct bug_entry )));
        }
        {
        while (1) {
          while_68_continue: /* CIL Label */ ;
        }
        while_68_break: /* CIL Label */ ;
        }
        goto while_67_break;
      }
      while_67_break: /* CIL Label */ ;
      }
    }
    goto while_66_break;
  }
  while_66_break: /* CIL Label */ ;
  }
  {
  __asm__  ("booo_exp(slab.c:1587)":);
  tmp___19 = cpu_cache_get((struct kmem_cache___0 *)0);
  __constant_memcpy((void *)ptr, (void const   *)tmp___19, sizeof(struct arraycache_init ));
  }
  {
  while (1) {
    while_69_continue: /* CIL Label */ ;
    {
    __constr_expr_1.raw_lock.slock = 0U;
    ptr->lock = __constr_expr_1;
    }
    goto while_69_break;
  }
  while_69_break: /* CIL Label */ ;
  }
  {
  __asm__  ("booo_exp(slab.c:1594)":);
  }
  {
  while (1) {
    while_70_continue: /* CIL Label */ ;
    {
    while (1) {
      while_71_continue: /* CIL Label */ ;
      goto while_71_break;
    }
    while_71_break: /* CIL Label */ ;
    }
    {
    raw_local_irq_enable();
    }
    goto while_70_break;
  }
  while_70_break: /* CIL Label */ ;
  }
  {
  nid = 0;
  }
  {
  while (1) {
    while_72_continue: /* CIL Label */ ;
    if (! (nid == 0)) {
      goto while_72_break;
    }
    {
    init_list(& cache_cache, & initkmem_list3[nid], nid);
    __asm__  ("booo_exp(slab.c:1605)":);
    init_list((struct kmem_cache___0 *)0, & initkmem_list3[1 + nid], nid);
    tmp___23 = index_of(sizeof(struct arraycache_init ));
    tmp___24 = index_of(sizeof(struct kmem_list3 ));
    }
    if (tmp___23 != tmp___24) {
      {
      __asm__  ("booo_exp(slab.c:1609)":);
      init_list((struct kmem_cache___0 *)0, & initkmem_list3[2 + nid], nid);
      }
    }
    {
    nid = 1;
    }
  }
  while_72_break: /* CIL Label */ ;
  }
  {
  mutex_lock(& cache_chain_mutex);
  __mptr = (struct list_head  const  *)cache_chain.next;
  cachep = (struct kmem_cache___0 *)((char *)__mptr - (unsigned int )(& ((struct kmem_cache___0 *)0)->next));
  }
  {
  while (1) {
    while_73_continue: /* CIL Label */ ;
    {
    prefetch((void const   *)cachep->next.next);
    }
    if (! ((unsigned int )(& cachep->next) != (unsigned int )(& cache_chain))) {
      goto while_73_break;
    }
    {
    tmp___25 = enable_cpucache((struct kmem_cache *)cachep);
    }
    if (tmp___25) {
      {
      while (1) {
        while_74_continue: /* CIL Label */ ;
        {
        __asm__  volatile   ("1:\tud2\n"
                             ".pushsection __bug_table,\"a\"\n"
                             "2:\t.long 1b, %c0\n"
                             "\t.word %c1, 0\n"
                             "\t.org 2b+%c2\n"
                             ".popsection": : "i" ("slab.c"), "i" (1621), "i" (sizeof(struct bug_entry )));
        }
        {
        while (1) {
          while_75_continue: /* CIL Label */ ;
        }
        while_75_break: /* CIL Label */ ;
        }
        goto while_74_break;
      }
      while_74_break: /* CIL Label */ ;
      }
    }
    {
    __mptr___0 = (struct list_head  const  *)cachep->next.next;
    cachep = (struct kmem_cache___0 *)((char *)__mptr___0 - (unsigned int )(& ((struct kmem_cache___0 *)0)->next));
    }
  }
  while_73_break: /* CIL Label */ ;
  }
  {
  mutex_unlock(& cache_chain_mutex);
  init_lock_keys();
  g_cpucache_up = 3;
  register_cpu_notifier(& cpucache_notifier);
  }
}
}
static int __attribute__((__cold__))  cpucache_init(void)  __attribute__((__section__(".init.text"))) ;
static int __attribute__((__cold__))  cpucache_init(void) 
{ int cpu ;

  {
  {
  cpu = -1;
  }
  {
  while (1) {
    while_76_continue: /* CIL Label */ ;
    {
    cpu = __next_cpu(cpu, (cpumask_t const   *)(& cpu_online_map));
    }
    if (! (cpu < 64)) {
      goto while_76_break;
    }
    {
    start_cpu_timer(cpu);
    }
  }
  while_76_break: /* CIL Label */ ;
  }
  return ((int __attribute__((__cold__))  )0);
}
}
static int (*__initcall_cpucache_init6)(void)  __attribute__((__used__, __section__(".initcall6.init")))  =    (int (*)(void))(& cpucache_init);
static void *kmem_getpages(struct kmem_cache___0 *cachep ,
                           gfp_t flags , int nodeid ) 
{ struct page *page ;
  int nr_pages ;
  int i ;
  struct zone *tmp ;
  struct zone *tmp___0 ;
  void *tmp___1 ;

  {
  {
  flags |= cachep->gfpflags;
  }
  if ((unsigned long )cachep->flags & 131072UL) {
    {
    flags |= 524288U;
    }
  }
  {
  page = alloc_pages_node(nodeid, flags, cachep->gfporder);
  }
  if (! page) {
    return ((void *)0);
  }
  {
  nr_pages = 1 << cachep->gfporder;
  }
  if ((unsigned long )cachep->flags & 131072UL) {
    {
    tmp = page_zone(page);
    mod_zone_page_state(tmp, 8, nr_pages);
    }
  } else {
    {
    tmp___0 = page_zone(page);
    mod_zone_page_state(tmp___0, 9, nr_pages);
    }
  }
  {
  i = 0;
  }
  {
  while (1) {
    while_77_continue: /* CIL Label */ ;
    if (! (i < nr_pages)) {
      goto while_77_break;
    }
    {
    __SetPageSlab(page + i);
    i ++;
    }
  }
  while_77_break: /* CIL Label */ ;
  }
  {
  tmp___1 = page_address(page);
  }
  return (tmp___1);
}
}
static void kmem_freepages(struct kmem_cache___0 *cachep , void *addr ) 
{ unsigned long i ;
  struct page *page ;
  unsigned long nr_freed ;
  struct zone *tmp ;
  struct zone *tmp___0 ;
  int tmp___1 ;
  int tmp___2 ;
  long tmp___3 ;
  unsigned long tmp___4 ;
  struct task_struct *tmp___5 ;
  struct task_struct *tmp___6 ;

  {
  {
  i = (unsigned long )(1 << cachep->gfporder);
  page = mem_map + ((((unsigned long )addr - 3221225472UL) >> 12) - 0UL);
  nr_freed = i;
  }
  if ((unsigned long )cachep->flags & 131072UL) {
    {
    tmp = page_zone(page);
    mod_zone_page_state(tmp, 8, (int )(- nr_freed));
    }
  } else {
    {
    tmp___0 = page_zone(page);
    mod_zone_page_state(tmp___0, 9, (int )(- nr_freed));
    }
  }
  {
  while (1) {
    while_78_continue: /* CIL Label */ ;
    {
    tmp___4 = i;
    i --;
    }
    if (! tmp___4) {
      goto while_78_break;
    }
    {
    while (1) {
      while_79_continue: /* CIL Label */ ;
      {
      tmp___1 = PageSlab(page);
      }
      if (tmp___1) {
        {
        tmp___2 = 0;
        }
      } else {
        {
        tmp___2 = 1;
        }
      }
      {
      tmp___3 = __builtin_expect((long )tmp___2, 0L);
      }
      if (tmp___3) {
        {
        while (1) {
          while_80_continue: /* CIL Label */ ;
          {
          __asm__  volatile   ("1:\tud2\n"
                               ".pushsection __bug_table,\"a\"\n"
                               "2:\t.long 1b, %c0\n"
                               "\t.word %c1, 0\n"
                               "\t.org 2b+%c2\n"
                               ".popsection": : "i" ("slab.c"), "i" (1714), "i" (sizeof(struct bug_entry )));
          }
          {
          while (1) {
            while_81_continue: /* CIL Label */ ;
          }
          while_81_break: /* CIL Label */ ;
          }
          goto while_80_break;
        }
        while_80_break: /* CIL Label */ ;
        }
      }
      goto while_79_break;
    }
    while_79_break: /* CIL Label */ ;
    }
    {
    __ClearPageSlab(page);
    page ++;
    }
  }
  while_78_break: /* CIL Label */ ;
  }
  {
  tmp___6 = get_current();
  }
  if (tmp___6->reclaim_state) {
    {
    tmp___5 = get_current();
    (tmp___5->reclaim_state)->reclaimed_slab += nr_freed;
    }
  }
  {
  free_pages((unsigned long )addr, cachep->gfporder);
  }
  return;
}
}
__asm__("error in function kmem_cache_create");
extern void *__crc_kmem_cache_create  __attribute__((__weak__)) ;
static unsigned long const   __kcrctab_kmem_cache_create  __attribute__((__used__,
__unused__, __section__("__kcrctab")))  =    (unsigned long const   )((unsigned long )(& __crc_kmem_cache_create));
static char const   __kstrtab_kmem_cache_create[18]  __attribute__((__section__("__ksymtab_strings"),
__aligned__(1)))  = 
  {      (char const   )'k',      (char const   )'m',      (char const   )'e',      (char const   )'m', 
        (char const   )'_',      (char const   )'c',      (char const   )'a',      (char const   )'c', 
        (char const   )'h',      (char const   )'e',      (char const   )'_',      (char const   )'c', 
        (char const   )'r',      (char const   )'e',      (char const   )'a',      (char const   )'t', 
        (char const   )'e',      (char const   )'\000'};
static struct kernel_symbol  const  __ksymtab_kmem_cache_create  __attribute__((__used__,
__unused__, __section__("__ksymtab")))  =    {(unsigned long )(& kmem_cache_create), __kstrtab_kmem_cache_create};
static void drain_array(struct kmem_cache___0 *cachep___19 , struct kmem_list3 *l3___5 ,
                        struct array_cache *ac___4 , int force , int node___4 ) ;
__asm__("error in function drain_freelist");
__asm__("error in function kmem_cache_shrink");
extern void *__crc_kmem_cache_shrink  __attribute__((__weak__)) ;
static unsigned long const   __kcrctab_kmem_cache_shrink  __attribute__((__used__,
__unused__, __section__("__kcrctab")))  =    (unsigned long const   )((unsigned long )(& __crc_kmem_cache_shrink));
static char const   __kstrtab_kmem_cache_shrink[18]  __attribute__((__section__("__ksymtab_strings"),
__aligned__(1)))  = 
  {      (char const   )'k',      (char const   )'m',      (char const   )'e',      (char const   )'m', 
        (char const   )'_',      (char const   )'c',      (char const   )'a',      (char const   )'c', 
        (char const   )'h',      (char const   )'e',      (char const   )'_',      (char const   )'s', 
        (char const   )'h',      (char const   )'r',      (char const   )'i',      (char const   )'n', 
        (char const   )'k',      (char const   )'\000'};
static struct kernel_symbol  const  __ksymtab_kmem_cache_shrink  __attribute__((__used__,
__unused__, __section__("__ksymtab")))  =    {(unsigned long )(& kmem_cache_shrink), __kstrtab_kmem_cache_shrink};
__asm__("error in function kmem_cache_destroy");
extern void *__crc_kmem_cache_destroy  __attribute__((__weak__)) ;
static unsigned long const   __kcrctab_kmem_cache_destroy  __attribute__((__used__,
__unused__, __section__("__kcrctab")))  =    (unsigned long const   )((unsigned long )(& __crc_kmem_cache_destroy));
static char const   __kstrtab_kmem_cache_destroy[19]  __attribute__((__section__("__ksymtab_strings"),
__aligned__(1)))  = 
  {      (char const   )'k',      (char const   )'m',      (char const   )'e',      (char const   )'m', 
        (char const   )'_',      (char const   )'c',      (char const   )'a',      (char const   )'c', 
        (char const   )'h',      (char const   )'e',      (char const   )'_',      (char const   )'d', 
        (char const   )'e',      (char const   )'s',      (char const   )'t',      (char const   )'r', 
        (char const   )'o',      (char const   )'y',      (char const   )'\000'};
static struct kernel_symbol  const  __ksymtab_kmem_cache_destroy  __attribute__((__used__,
__unused__, __section__("__ksymtab")))  =    {(unsigned long )(& kmem_cache_destroy), __kstrtab_kmem_cache_destroy};
static struct slab *alloc_slabmgmt(struct kmem_cache___0 *cachep___2 , void *objp ,
                                   int colour_off , gfp_t local_flags , int nodeid ) 
{ struct slab *slabp ;
  void *tmp___3 ;

  {
  if ((unsigned long )cachep___2->flags & 2147483648UL) {
    {
    tmp___3 = kmem_cache_alloc_node(cachep___2->slabp_cache, local_flags & 4294967295U,
                                    nodeid);
    slabp = (struct slab *)tmp___3;
    }
    if (! slabp) {
      return ((struct slab *)((void *)0));
    }
  } else {
    {
    slabp = (struct slab *)(objp + colour_off);
    colour_off = (int )((unsigned int )colour_off + cachep___2->slab_size);
    }
  }
  {
  slabp->inuse = 0U;
  slabp->colouroff = (unsigned long )colour_off;
  slabp->s_mem = objp + colour_off;
  slabp->nodeid = (unsigned short )nodeid;
  slabp->free = 0U;
  }
  return (slabp);
}
}
__inline static kmem_bufctl_t *slab_bufctl(struct slab *slabp___0 ) 
{ 

  {
  return ((kmem_bufctl_t *)(slabp___0 + 1));
}
}
static void cache_init_objs(struct kmem_cache___0 *cachep___3 , struct slab *slabp___1 ) 
{ int i___0 ;
  void *objp___0 ;
  void *tmp___4 ;
  kmem_bufctl_t *tmp___5 ;
  kmem_bufctl_t *tmp___6 ;

  {
  {
  i___0 = 0;
  }
  {
  while (1) {
    while_82_continue: /* CIL Label */ ;
    if (! ((unsigned int )i___0 < cachep___3->num)) {
      goto while_82_break;
    }
    {
    tmp___4 = index_to_obj(cachep___3, slabp___1, (unsigned int )i___0);
    objp___0 = tmp___4;
    }
    if (cachep___3->ctor) {
      {
      (*(cachep___3->ctor))(objp___0);
      }
    }
    {
    tmp___5 = slab_bufctl(slabp___1);
    *(tmp___5 + i___0) = (unsigned int )(i___0 + 1);
    i___0 ++;
    }
  }
  while_82_break: /* CIL Label */ ;
  }
  {
  tmp___6 = slab_bufctl(slabp___1);
  *(tmp___6 + (i___0 - 1)) = 4294967295U;
  }
  return;
}
}
static void kmem_flagcheck(struct kmem_cache___0 *cachep___4 , gfp_t flags ) 
{ long tmp___7 ;
  long tmp___8 ;

  {
  if (flags & 1U) {
    {
    while (1) {
      while_83_continue: /* CIL Label */ ;
      {
      tmp___7 = __builtin_expect((long )(! (! (! (cachep___4->gfpflags & 1U)))), 0L);
      }
      if (tmp___7) {
        {
        while (1) {
          while_84_continue: /* CIL Label */ ;
          {
          __asm__  volatile   ("1:\tud2\n"
                               ".pushsection __bug_table,\"a\"\n"
                               "2:\t.long 1b, %c0\n"
                               "\t.word %c1, 0\n"
                               "\t.org 2b+%c2\n"
                               ".popsection": : "i" ("slab.c"), "i" (2682), "i" (sizeof(struct bug_entry )));
          }
          {
          while (1) {
            while_85_continue: /* CIL Label */ ;
          }
          while_85_break: /* CIL Label */ ;
          }
          goto while_84_break;
        }
        while_84_break: /* CIL Label */ ;
        }
      }
      goto while_83_break;
    }
    while_83_break: /* CIL Label */ ;
    }
  } else {
    {
    while (1) {
      while_86_continue: /* CIL Label */ ;
      {
      tmp___8 = __builtin_expect((long )(! (! (cachep___4->gfpflags & 1U))), 0L);
      }
      if (tmp___8) {
        {
        while (1) {
          while_87_continue: /* CIL Label */ ;
          {
          __asm__  volatile   ("1:\tud2\n"
                               ".pushsection __bug_table,\"a\"\n"
                               "2:\t.long 1b, %c0\n"
                               "\t.word %c1, 0\n"
                               "\t.org 2b+%c2\n"
                               ".popsection": : "i" ("slab.c"), "i" (2684), "i" (sizeof(struct bug_entry )));
          }
          {
          while (1) {
            while_88_continue: /* CIL Label */ ;
          }
          while_88_break: /* CIL Label */ ;
          }
          goto while_87_break;
        }
        while_87_break: /* CIL Label */ ;
        }
      }
      goto while_86_break;
    }
    while_86_break: /* CIL Label */ ;
    }
  }
  return;
}
}
static void *slab_get_obj(struct kmem_cache___0 *cachep___5 , struct slab *slabp___2 ,
                          int nodeid___0 ) 
{ void *objp___1 ;
  void *tmp___9 ;
  kmem_bufctl_t next ;
  kmem_bufctl_t *tmp___10 ;

  {
  {
  tmp___9 = index_to_obj(cachep___5, slabp___2, slabp___2->free);
  objp___1 = tmp___9;
  (slabp___2->inuse) ++;
  tmp___10 = slab_bufctl(slabp___2);
  next = *(tmp___10 + slabp___2->free);
  slabp___2->free = next;
  }
  return (objp___1);
}
}
static void slab_map_pages(struct kmem_cache___0 *cache , struct slab *slab , void *addr ) 
{ int nr_pages ;
  struct page *page ;
  int tmp___13 ;
  int tmp___14 ;
  long tmp___15 ;

  {
  {
  page = mem_map + ((((unsigned long )addr - 3221225472UL) >> 12) - 0UL);
  nr_pages = 1;
  tmp___13 = PageCompound(page);
  }
  if (tmp___13) {
    {
    tmp___14 = 0;
    }
  } else {
    {
    tmp___14 = 1;
    }
  }
  {
  tmp___15 = __builtin_expect((long )tmp___14, 1L);
  }
  if (tmp___15) {
    {
    nr_pages <<= cache->gfporder;
    }
  }
  {
  while (1) {
    while_89_continue: /* CIL Label */ ;
    {
    page_set_cache(page, cache);
    page_set_slab(page, slab);
    page ++;
    nr_pages --;
    }
    if (! nr_pages) {
      goto while_89_break;
    }
  }
  while_89_break: /* CIL Label */ ;
  }
  return;
}
}
static int cache_grow(struct kmem_cache___0 *cachep___7 , gfp_t flags___0 , int nodeid___2 ,
                      void *objp___3 ) 
{ struct slab *slabp___4 ;
  size_t offset ;
  gfp_t local_flags___0 ;
  struct kmem_list3 *l3___1 ;
  long tmp___16 ;

  {
  {
  while (1) {
    while_90_continue: /* CIL Label */ ;
    {
    tmp___16 = __builtin_expect((long )(! (! (flags___0 & (6U | ~ ((unsigned int )((1 << 21) - 1)))))),
                                0L);
    }
    if (tmp___16) {
      {
      while (1) {
        while_91_continue: /* CIL Label */ ;
        {
        __asm__  volatile   ("1:\tud2\n"
                             ".pushsection __bug_table,\"a\"\n"
                             "2:\t.long 1b, %c0\n"
                             "\t.word %c1, 0\n"
                             "\t.org 2b+%c2\n"
                             ".popsection": : "i" ("slab.c"), "i" (2765), "i" (sizeof(struct bug_entry )));
        }
        {
        while (1) {
          while_92_continue: /* CIL Label */ ;
        }
        while_92_break: /* CIL Label */ ;
        }
        goto while_91_break;
      }
      while_91_break: /* CIL Label */ ;
      }
    }
    goto while_90_break;
  }
  while_90_break: /* CIL Label */ ;
  }
  {
  local_flags___0 = flags___0 & 466672U;
  }
  {
  while (1) {
    while_93_continue: /* CIL Label */ ;
    goto while_93_break;
  }
  while_93_break: /* CIL Label */ ;
  }
  {
  l3___1 = cachep___7->nodelists[nodeid___2];
  _spin_lock(& l3___1->list_lock);
  offset = l3___1->colour_next;
  (l3___1->colour_next) ++;
  }
  if (l3___1->colour_next >= cachep___7->colour) {
    {
    l3___1->colour_next = 0U;
    }
  }
  {
  while (1) {
    while_94_continue: /* CIL Label */ ;
    {
    __raw_spin_unlock(& l3___1->list_lock.raw_lock);
    }
    goto while_94_break;
  }
  while_94_break: /* CIL Label */ ;
  }
  {
  offset *= cachep___7->colour_off;
  }
  if (local_flags___0 & 16U) {
    {
    while (1) {
      while_95_continue: /* CIL Label */ ;
      {
      while (1) {
        while_96_continue: /* CIL Label */ ;
        goto while_96_break;
      }
      while_96_break: /* CIL Label */ ;
      }
      {
      raw_local_irq_enable();
      }
      goto while_95_break;
    }
    while_95_break: /* CIL Label */ ;
    }
  }
  {
  kmem_flagcheck(cachep___7, flags___0);
  }
  if (! objp___3) {
    {
    objp___3 = kmem_getpages(cachep___7, local_flags___0, nodeid___2);
    }
  }
  if (! objp___3) {
    goto failed;
  }
  {
  slabp___4 = alloc_slabmgmt(cachep___7, objp___3, (int )offset, local_flags___0 & 4294574079U,
                             nodeid___2);
  }
  if (! slabp___4) {
    goto opps1;
  }
  {
  slab_map_pages(cachep___7, slabp___4, objp___3);
  cache_init_objs(cachep___7, slabp___4);
  }
  if (local_flags___0 & 16U) {
    {
    while (1) {
      while_97_continue: /* CIL Label */ ;
      {
      raw_local_irq_disable();
      }
      {
      while (1) {
        while_98_continue: /* CIL Label */ ;
        goto while_98_break;
      }
      while_98_break: /* CIL Label */ ;
      }
      goto while_97_break;
    }
    while_97_break: /* CIL Label */ ;
    }
  }
  {
  while (1) {
    while_99_continue: /* CIL Label */ ;
    goto while_99_break;
  }
  while_99_break: /* CIL Label */ ;
  }
  {
  _spin_lock(& l3___1->list_lock);
  list_add_tail(& slabp___4->list, & l3___1->slabs_free);
  }
  {
  while (1) {
    while_100_continue: /* CIL Label */ ;
    goto while_100_break;
  }
  while_100_break: /* CIL Label */ ;
  }
  {
  l3___1->free_objects += (unsigned long )cachep___7->num;
  }
  {
  while (1) {
    while_101_continue: /* CIL Label */ ;
    {
    __raw_spin_unlock(& l3___1->list_lock.raw_lock);
    }
    goto while_101_break;
  }
  while_101_break: /* CIL Label */ ;
  }
  return (1);
  opps1: 
  {
  kmem_freepages(cachep___7, objp___3);
  }
  failed: 
  if (local_flags___0 & 16U) {
    {
    while (1) {
      while_102_continue: /* CIL Label */ ;
      {
      raw_local_irq_disable();
      }
      {
      while (1) {
        while_103_continue: /* CIL Label */ ;
        goto while_103_break;
      }
      while_103_break: /* CIL Label */ ;
      }
      goto while_102_break;
    }
    while_102_break: /* CIL Label */ ;
    }
  }
  return (0);
}
}
static void *cache_alloc_refill(struct kmem_cache___0 *cachep___8 , gfp_t flags___1 ) 
{ int batchcount ;
  struct kmem_list3 *l3___2 ;
  struct array_cache *ac___0 ;
  int node___1 ;
  int tmp___17 ;
  long tmp___18 ;
  int tmp___19 ;
  struct list_head *entry ;
  struct slab *slabp___5 ;
  struct list_head  const  *__mptr ;
  int tmp___20 ;
  long tmp___21 ;
  unsigned int tmp___22 ;
  int tmp___23 ;
  int x ;
  long tmp___24 ;

  {
  retry: 
  {
  while (1) {
    while_104_continue: /* CIL Label */ ;
    goto while_104_break;
  }
  while_104_break: /* CIL Label */ ;
  }
  {
  node___1 = 0;
  ac___0 = cpu_cache_get(cachep___8);
  batchcount = (int )ac___0->batchcount;
  }
  if (! ac___0->touched) {
    if (batchcount > 16) {
      {
      batchcount = 16;
      }
    }
  }
  {
  l3___2 = cachep___8->nodelists[node___1];
  }
  {
  while (1) {
    while_105_continue: /* CIL Label */ ;
    if (ac___0->avail > 0U) {
      {
      tmp___17 = 1;
      }
    } else {
      if (! l3___2) {
        {
        tmp___17 = 1;
        }
      } else {
        {
        tmp___17 = 0;
        }
      }
    }
    {
    tmp___18 = __builtin_expect((long )tmp___17, 0L);
    }
    if (tmp___18) {
      {
      while (1) {
        while_106_continue: /* CIL Label */ ;
        {
        __asm__  volatile   ("1:\tud2\n"
                             ".pushsection __bug_table,\"a\"\n"
                             "2:\t.long 1b, %c0\n"
                             "\t.word %c1, 0\n"
                             "\t.org 2b+%c2\n"
                             ".popsection": : "i" ("slab.c"), "i" (2971), "i" (sizeof(struct bug_entry )));
        }
        {
        while (1) {
          while_107_continue: /* CIL Label */ ;
        }
        while_107_break: /* CIL Label */ ;
        }
        goto while_106_break;
      }
      while_106_break: /* CIL Label */ ;
      }
    }
    goto while_105_break;
  }
  while_105_break: /* CIL Label */ ;
  }
  {
  _spin_lock(& l3___2->list_lock);
  }
  if (l3___2->shared) {
    {
    tmp___19 = transfer_objects(ac___0, l3___2->shared, (unsigned int )batchcount);
    }
    if (tmp___19) {
      goto alloc_done;
    }
  }
  {
  while (1) {
    while_108_continue: /* CIL Label */ ;
    if (! (batchcount > 0)) {
      goto while_108_break;
    }
    {
    entry = l3___2->slabs_partial.next;
    }
    if ((unsigned int )entry == (unsigned int )(& l3___2->slabs_partial)) {
      {
      l3___2->free_touched = 1;
      entry = l3___2->slabs_free.next;
      }
      if ((unsigned int )entry == (unsigned int )(& l3___2->slabs_free)) {
        goto must_grow;
      }
    }
    {
    __mptr = (struct list_head  const  *)entry;
    slabp___5 = (struct slab *)((char *)__mptr - (unsigned int )(& ((struct slab *)0)->list));
    }
    {
    while (1) {
      while_109_continue: /* CIL Label */ ;
      goto while_109_break;
    }
    while_109_break: /* CIL Label */ ;
    }
    {
    while (1) {
      while_110_continue: /* CIL Label */ ;
      goto while_110_break;
    }
    while_110_break: /* CIL Label */ ;
    }
    {
    while (1) {
      while_111_continue: /* CIL Label */ ;
      if (slabp___5->inuse < 0U) {
        {
        tmp___20 = 1;
        }
      } else {
        if (slabp___5->inuse >= cachep___8->num) {
          {
          tmp___20 = 1;
          }
        } else {
          {
          tmp___20 = 0;
          }
        }
      }
      {
      tmp___21 = __builtin_expect((long )tmp___20, 0L);
      }
      if (tmp___21) {
        {
        while (1) {
          while_112_continue: /* CIL Label */ ;
          {
          __asm__  volatile   ("1:\tud2\n"
                               ".pushsection __bug_table,\"a\"\n"
                               "2:\t.long 1b, %c0\n"
                               "\t.word %c1, 0\n"
                               "\t.org 2b+%c2\n"
                               ".popsection": : "i" ("slab.c"), "i" (2999), "i" (sizeof(struct bug_entry )));
          }
          {
          while (1) {
            while_113_continue: /* CIL Label */ ;
          }
          while_113_break: /* CIL Label */ ;
          }
          goto while_112_break;
        }
        while_112_break: /* CIL Label */ ;
        }
      }
      goto while_111_break;
    }
    while_111_break: /* CIL Label */ ;
    }
    {
    while (1) {
      while_114_continue: /* CIL Label */ ;
      if (slabp___5->inuse < cachep___8->num) {
        {
        tmp___23 = batchcount;
        batchcount --;
        }
        if (! tmp___23) {
          goto while_114_break;
        }
      } else {
        goto while_114_break;
      }
      {
      while (1) {
        while_115_continue: /* CIL Label */ ;
        goto while_115_break;
      }
      while_115_break: /* CIL Label */ ;
      }
      {
      while (1) {
        while_116_continue: /* CIL Label */ ;
        goto while_116_break;
      }
      while_116_break: /* CIL Label */ ;
      }
      {
      while (1) {
        while_117_continue: /* CIL Label */ ;
        goto while_117_break;
      }
      while_117_break: /* CIL Label */ ;
      }
      {
      tmp___22 = ac___0->avail;
      (ac___0->avail) ++;
      ac___0->entry[tmp___22] = slab_get_obj(cachep___8, slabp___5, node___1);
      }
    }
    while_114_break: /* CIL Label */ ;
    }
    {
    while (1) {
      while_118_continue: /* CIL Label */ ;
      goto while_118_break;
    }
    while_118_break: /* CIL Label */ ;
    }
    {
    list_del(& slabp___5->list);
    }
    if (slabp___5->free == 4294967295U) {
      {
      list_add(& slabp___5->list, & l3___2->slabs_full);
      }
    } else {
      {
      list_add(& slabp___5->list, & l3___2->slabs_partial);
      }
    }
  }
  while_108_break: /* CIL Label */ ;
  }
  must_grow: 
  {
  l3___2->free_objects -= (unsigned long )ac___0->avail;
  }
  alloc_done: 
  {
  while (1) {
    while_119_continue: /* CIL Label */ ;
    {
    __raw_spin_unlock(& l3___2->list_lock.raw_lock);
    }
    goto while_119_break;
  }
  while_119_break: /* CIL Label */ ;
  }
  {
  tmp___24 = __builtin_expect((long )(! (! (! ac___0->avail))), 0L);
  }
  if (tmp___24) {
    {
    x = cache_grow(cachep___8, flags___1, node___1, (void *)0);
    ac___0 = cpu_cache_get(cachep___8);
    }
    if (! x) {
      if (ac___0->avail == 0U) {
        return ((void *)0);
      }
    }
    if (! ac___0->avail) {
      goto retry;
    }
  }
  {
  ac___0->touched = 1U;
  (ac___0->avail) --;
  }
  return (ac___0->entry[ac___0->avail]);
}
}
__inline static void cache_alloc_debugcheck_before(struct kmem_cache___0 *cachep___9 ,
                                                   gfp_t flags___2 ) 
{ 

  {
  {
  while (1) {
    while_120_continue: /* CIL Label */ ;
    if (flags___2 & 16U) {
      {
      while (1) {
        while_121_continue: /* CIL Label */ ;
        {
        _cond_resched();
        }
        goto while_121_break;
      }
      while_121_break: /* CIL Label */ ;
      }
    }
    goto while_120_break;
  }
  while_120_break: /* CIL Label */ ;
  }
  return;
}
}
__inline static int should_failslab(struct kmem_cache___0 *cachep___10 , gfp_t flags___3 ) 
{ 

  {
  return (0);
}
}
__inline static void *____cache_alloc(struct kmem_cache___0 *cachep___11 , gfp_t flags___4 ) 
{ void *objp___4 ;
  struct array_cache *ac___1 ;
  long tmp___25 ;

  {
  {
  while (1) {
    while_122_continue: /* CIL Label */ ;
    goto while_122_break;
  }
  while_122_break: /* CIL Label */ ;
  }
  {
  ac___1 = cpu_cache_get(cachep___11);
  tmp___25 = __builtin_expect((long )(! (! ac___1->avail)), 1L);
  }
  if (tmp___25) {
    {
    while (1) {
      while_123_continue: /* CIL Label */ ;
      goto while_123_break;
    }
    while_123_break: /* CIL Label */ ;
    }
    {
    ac___1->touched = 1U;
    (ac___1->avail) --;
    objp___4 = ac___1->entry[ac___1->avail];
    }
  } else {
    {
    while (1) {
      while_124_continue: /* CIL Label */ ;
      goto while_124_break;
    }
    while_124_break: /* CIL Label */ ;
    }
    {
    objp___4 = cache_alloc_refill(cachep___11, flags___4);
    }
  }
  return (objp___4);
}
}
__inline static void *( __attribute__((__always_inline__)) __do_cache_alloc)(struct kmem_cache___0 *cachep___12 ,
                                                                             gfp_t flags___5 ) 
{ void *tmp___26 ;

  {
  {
  tmp___26 = ____cache_alloc(cachep___12, flags___5);
  }
  return (tmp___26);
}
}
__inline static void *( __attribute__((__always_inline__)) __cache_alloc)(struct kmem_cache___0 *cachep___13 ,
                                                                          gfp_t flags___6 ,
                                                                          void *caller ) 
{ unsigned long save_flags ;
  void *objp___5 ;
  int tmp___27 ;
  int tmp___28 ;
  int tmp___29 ;
  long tmp___30 ;

  {
  {
  tmp___27 = should_failslab(cachep___13, flags___6);
  }
  if (tmp___27) {
    return ((void *)0);
  }
  {
  cache_alloc_debugcheck_before(cachep___13, flags___6);
  }
  {
  while (1) {
    while_125_continue: /* CIL Label */ ;
    {
    while (1) {
      while_126_continue: /* CIL Label */ ;
      {
      save_flags = __raw_local_irq_save();
      }
      goto while_126_break;
    }
    while_126_break: /* CIL Label */ ;
    }
    {
    while (1) {
      while_127_continue: /* CIL Label */ ;
      goto while_127_break;
    }
    while_127_break: /* CIL Label */ ;
    }
    goto while_125_break;
  }
  while_125_break: /* CIL Label */ ;
  }
  {
  objp___5 = __do_cache_alloc(cachep___13, flags___6);
  }
  {
  while (1) {
    while_128_continue: /* CIL Label */ ;
    {
    tmp___28 = raw_irqs_disabled_flags(save_flags);
    }
    if (tmp___28) {
      {
      raw_local_irq_restore(save_flags);
      }
      {
      while (1) {
        while_129_continue: /* CIL Label */ ;
        goto while_129_break;
      }
      while_129_break: /* CIL Label */ ;
      }
    } else {
      {
      while (1) {
        while_130_continue: /* CIL Label */ ;
        goto while_130_break;
      }
      while_130_break: /* CIL Label */ ;
      }
      {
      raw_local_irq_restore(save_flags);
      }
    }
    goto while_128_break;
  }
  while_128_break: /* CIL Label */ ;
  }
  {
  objp___5 = objp___5;
  prefetchw((void const   *)objp___5);
  }
  if (flags___6 & 32768U) {
    if (objp___5) {
      {
      tmp___29 = 1;
      }
    } else {
      {
      tmp___29 = 0;
      }
    }
  } else {
    {
    tmp___29 = 0;
    }
  }
  {
  tmp___30 = __builtin_expect((long )tmp___29, 0L);
  }
  if (tmp___30) {
    {
    __constant_c_memset(objp___5, 0UL, cachep___13->buffer_size);
    }
  }
  return (objp___5);
}
}
__asm__("error in function free_block");
static void cache_flusharray(struct kmem_cache___0 *cachep___14 , struct array_cache *ac___2 ) 
{ int batchcount___0 ;
  struct kmem_list3 *l3___3 ;
  int node___2 ;
  struct array_cache *shared_array ;
  int max ;

  {
  {
  node___2 = 0;
  batchcount___0 = (int )ac___2->batchcount;
  }
  {
  while (1) {
    while_131_continue: /* CIL Label */ ;
    goto while_131_break;
  }
  while_131_break: /* CIL Label */ ;
  }
  {
  l3___3 = cachep___14->nodelists[node___2];
  _spin_lock(& l3___3->list_lock);
  }
  if (l3___3->shared) {
    {
    shared_array = l3___3->shared;
    max = (int )(shared_array->limit - shared_array->avail);
    }
    if (max) {
      if (batchcount___0 > max) {
        {
        batchcount___0 = max;
        }
      }
      {
      __memcpy((void *)(& shared_array->entry[shared_array->avail]), (void const   *)(ac___2->entry),
               sizeof(void *) * (unsigned int )batchcount___0);
      shared_array->avail += (unsigned int )batchcount___0;
      }
      goto free_done;
    }
  }
  {
  free_block((struct kmem_cache *)cachep___14, ac___2->entry, batchcount___0, node___2);
  }
  free_done: 
  {
  while (1) {
    while_132_continue: /* CIL Label */ ;
    {
    __raw_spin_unlock(& l3___3->list_lock.raw_lock);
    }
    goto while_132_break;
  }
  while_132_break: /* CIL Label */ ;
  }
  {
  ac___2->avail -= (unsigned int )batchcount___0;
  memmove((void *)(ac___2->entry), (void const   *)(& ac___2->entry[batchcount___0]),
          sizeof(void *) * ac___2->avail);
  }
  return;
}
}
__inline static void __cache_free(struct kmem_cache___0 *cachep___15 , void *objp___6 ) 
{ struct array_cache *ac___3 ;
  struct array_cache *tmp___31 ;
  int tmp___32 ;
  unsigned int tmp___33 ;
  unsigned int tmp___34 ;
  long tmp___35 ;

  {
  {
  tmp___31 = cpu_cache_get(cachep___15);
  ac___3 = tmp___31;
  }
  {
  while (1) {
    while_133_continue: /* CIL Label */ ;
    goto while_133_break;
  }
  while_133_break: /* CIL Label */ ;
  }
  {
  objp___6 = objp___6;
  }
  if (numa_platform) {
    {
    tmp___32 = cache_free_alien(cachep___15, objp___6);
    }
    if (tmp___32) {
      return;
    }
  }
  {
  tmp___35 = __builtin_expect((long )(! (! (ac___3->avail < ac___3->limit))), 1L);
  }
  if (tmp___35) {
    {
    while (1) {
      while_134_continue: /* CIL Label */ ;
      goto while_134_break;
    }
    while_134_break: /* CIL Label */ ;
    }
    {
    tmp___33 = ac___3->avail;
    (ac___3->avail) ++;
    ac___3->entry[tmp___33] = objp___6;
    }
    return;
  } else {
    {
    while (1) {
      while_135_continue: /* CIL Label */ ;
      goto while_135_break;
    }
    while_135_break: /* CIL Label */ ;
    }
    {
    cache_flusharray(cachep___15, ac___3);
    tmp___34 = ac___3->avail;
    (ac___3->avail) ++;
    ac___3->entry[tmp___34] = objp___6;
    }
  }
  return;
}
}
__asm__("error in function kmem_cache_alloc");
extern void *__crc_kmem_cache_alloc  __attribute__((__weak__)) ;
static unsigned long const   __kcrctab_kmem_cache_alloc  __attribute__((__used__,
__unused__, __section__("__kcrctab")))  =    (unsigned long const   )((unsigned long )(& __crc_kmem_cache_alloc));
static char const   __kstrtab_kmem_cache_alloc[17]  __attribute__((__section__("__ksymtab_strings"),
__aligned__(1)))  = 
  {      (char const   )'k',      (char const   )'m',      (char const   )'e',      (char const   )'m', 
        (char const   )'_',      (char const   )'c',      (char const   )'a',      (char const   )'c', 
        (char const   )'h',      (char const   )'e',      (char const   )'_',      (char const   )'a', 
        (char const   )'l',      (char const   )'l',      (char const   )'o',      (char const   )'c', 
        (char const   )'\000'};
static struct kernel_symbol  const  __ksymtab_kmem_cache_alloc  __attribute__((__used__,
__unused__, __section__("__ksymtab")))  =    {(unsigned long )(& kmem_cache_alloc), __kstrtab_kmem_cache_alloc};
__asm__("error in function kmem_ptr_validate");
__inline static void *( __attribute__((__always_inline__)) __do_kmalloc)(size_t size ,
                                                                         gfp_t flags___7 ,
                                                                         void *caller___0 ) 
{ struct kmem_cache___0 *cachep___16 ;
  long tmp___36 ;
  void *tmp___37 ;

  {
  {
  cachep___16 = __find_general_cachep(size, flags___7);
  tmp___36 = __builtin_expect((long )(! (! ((unsigned long )cachep___16 <= (unsigned long )((void *)16)))),
                              0L);
  }
  if (tmp___36) {
    return ((void *)cachep___16);
  }
  {
  tmp___37 = __cache_alloc(cachep___16, flags___7, caller___0);
  }
  return (tmp___37);
}
}
void *__kmalloc(size_t size___0 , gfp_t flags___8 ) 
{ void *tmp___38 ;

  {
  {
  tmp___38 = __do_kmalloc(size___0, flags___8, (void *)0);
  }
  return (tmp___38);
}
}
extern void *__crc___kmalloc  __attribute__((__weak__)) ;
static unsigned long const   __kcrctab___kmalloc  __attribute__((__used__, __unused__,
__section__("__kcrctab")))  =    (unsigned long const   )((unsigned long )(& __crc___kmalloc));
static char const   __kstrtab___kmalloc[10]  __attribute__((__section__("__ksymtab_strings"),
__aligned__(1)))  = 
  {      (char const   )'_',      (char const   )'_',      (char const   )'k',      (char const   )'m', 
        (char const   )'a',      (char const   )'l',      (char const   )'l',      (char const   )'o', 
        (char const   )'c',      (char const   )'\000'};
static struct kernel_symbol  const  __ksymtab___kmalloc  __attribute__((__used__,
__unused__, __section__("__ksymtab")))  =    {(unsigned long )(& __kmalloc), __kstrtab___kmalloc};
__asm__("error in function kmem_cache_free");
extern void *__crc_kmem_cache_free  __attribute__((__weak__)) ;
static unsigned long const   __kcrctab_kmem_cache_free  __attribute__((__used__, __unused__,
__section__("__kcrctab")))  =    (unsigned long const   )((unsigned long )(& __crc_kmem_cache_free));
static char const   __kstrtab_kmem_cache_free[16]  __attribute__((__section__("__ksymtab_strings"),
__aligned__(1)))  = 
  {      (char const   )'k',      (char const   )'m',      (char const   )'e',      (char const   )'m', 
        (char const   )'_',      (char const   )'c',      (char const   )'a',      (char const   )'c', 
        (char const   )'h',      (char const   )'e',      (char const   )'_',      (char const   )'f', 
        (char const   )'r',      (char const   )'e',      (char const   )'e',      (char const   )'\000'};
static struct kernel_symbol  const  __ksymtab_kmem_cache_free  __attribute__((__used__,
__unused__, __section__("__ksymtab")))  =    {(unsigned long )(& kmem_cache_free), __kstrtab_kmem_cache_free};
void kfree(void const   *objp___7 ) 
{ struct kmem_cache___0 *c ;
  unsigned long flags___9 ;
  long tmp___39 ;
  int tmp___40 ;

  {
  {
  tmp___39 = __builtin_expect((long )(! (! ((unsigned long )objp___7 <= (unsigned long )((void *)16)))),
                              0L);
  }
  if (tmp___39) {
    return;
  }
  {
  while (1) {
    while_136_continue: /* CIL Label */ ;
    {
    while (1) {
      while_137_continue: /* CIL Label */ ;
      {
      flags___9 = __raw_local_irq_save();
      }
      goto while_137_break;
    }
    while_137_break: /* CIL Label */ ;
    }
    {
    while (1) {
      while_138_continue: /* CIL Label */ ;
      goto while_138_break;
    }
    while_138_break: /* CIL Label */ ;
    }
    goto while_136_break;
  }
  while_136_break: /* CIL Label */ ;
  }
  {
  while (1) {
    while_139_continue: /* CIL Label */ ;
    goto while_139_break;
  }
  while_139_break: /* CIL Label */ ;
  }
  {
  c = virt_to_cache(objp___7);
  debug_check_no_locks_freed(objp___7, (unsigned long )c->buffer_size);
  debug_check_no_obj_freed(objp___7, (unsigned long )c->buffer_size);
  __cache_free(c, (void *)objp___7);
  }
  {
  while (1) {
    while_140_continue: /* CIL Label */ ;
    {
    tmp___40 = raw_irqs_disabled_flags(flags___9);
    }
    if (tmp___40) {
      {
      raw_local_irq_restore(flags___9);
      }
      {
      while (1) {
        while_141_continue: /* CIL Label */ ;
        goto while_141_break;
      }
      while_141_break: /* CIL Label */ ;
      }
    } else {
      {
      while (1) {
        while_142_continue: /* CIL Label */ ;
        goto while_142_break;
      }
      while_142_break: /* CIL Label */ ;
      }
      {
      raw_local_irq_restore(flags___9);
      }
    }
    goto while_140_break;
  }
  while_140_break: /* CIL Label */ ;
  }
  return;
}
}
extern void *__crc_kfree  __attribute__((__weak__)) ;
static unsigned long const   __kcrctab_kfree  __attribute__((__used__, __unused__,
__section__("__kcrctab")))  =    (unsigned long const   )((unsigned long )(& __crc_kfree));
static char const   __kstrtab_kfree[6]  __attribute__((__section__("__ksymtab_strings"),
__aligned__(1)))  = {      (char const   )'k',      (char const   )'f',      (char const   )'r',      (char const   )'e', 
        (char const   )'e',      (char const   )'\000'};
static struct kernel_symbol  const  __ksymtab_kfree  __attribute__((__used__, __unused__,
__section__("__ksymtab")))  =    {(unsigned long )(& kfree), __kstrtab_kfree};
__asm__("error in function kmem_cache_size");
extern void *__crc_kmem_cache_size  __attribute__((__weak__)) ;
static unsigned long const   __kcrctab_kmem_cache_size  __attribute__((__used__, __unused__,
__section__("__kcrctab")))  =    (unsigned long const   )((unsigned long )(& __crc_kmem_cache_size));
static char const   __kstrtab_kmem_cache_size[16]  __attribute__((__section__("__ksymtab_strings"),
__aligned__(1)))  = 
  {      (char const   )'k',      (char const   )'m',      (char const   )'e',      (char const   )'m', 
        (char const   )'_',      (char const   )'c',      (char const   )'a',      (char const   )'c', 
        (char const   )'h',      (char const   )'e',      (char const   )'_',      (char const   )'s', 
        (char const   )'i',      (char const   )'z',      (char const   )'e',      (char const   )'\000'};
static struct kernel_symbol  const  __ksymtab_kmem_cache_size  __attribute__((__used__,
__unused__, __section__("__ksymtab")))  =    {(unsigned long )(& kmem_cache_size), __kstrtab_kmem_cache_size};
__asm__("error in function kmem_cache_name");
extern void *__crc_kmem_cache_name  __attribute__((__weak__)) ;
static unsigned long const   __kcrctab_kmem_cache_name  __attribute__((__used__, __unused__,
__section__("__kcrctab_gpl")))  =    (unsigned long const   )((unsigned long )(& __crc_kmem_cache_name));
static char const   __kstrtab_kmem_cache_name[16]  __attribute__((__section__("__ksymtab_strings"),
__aligned__(1)))  = 
  {      (char const   )'k',      (char const   )'m',      (char const   )'e',      (char const   )'m', 
        (char const   )'_',      (char const   )'c',      (char const   )'a',      (char const   )'c', 
        (char const   )'h',      (char const   )'e',      (char const   )'_',      (char const   )'n', 
        (char const   )'a',      (char const   )'m',      (char const   )'e',      (char const   )'\000'};
static struct kernel_symbol  const  __ksymtab_kmem_cache_name  __attribute__((__used__,
__unused__, __section__("__ksymtab_gpl")))  =    {(unsigned long )(& kmem_cache_name), __kstrtab_kmem_cache_name};
static int alloc_kmemlist(struct kmem_cache___0 *cachep___17 ) 
{ int node___3 ;
  struct kmem_list3 *l3___4 ;
  struct array_cache *new_shared ;
  struct array_cache **new_alien ;
  struct array_cache *shared ;
  cpumask_t const   *__tmp__ ;
  cpumask_t const   *tmp___41 ;
  int tmp___42 ;
  void *tmp___43 ;
  cpumask_t const   *__tmp_____0 ;
  cpumask_t const   *tmp___44 ;
  int tmp___45 ;

  {
  {
  new_alien = (struct array_cache **)((void *)0);
  node___3 = 0;
  }
  {
  while (1) {
    while_143_continue: /* CIL Label */ ;
    if (! (node___3 == 0)) {
      goto while_143_break;
    }
    if (use_alien_caches) {
      {
      new_alien = alloc_alien_cache(node___3, (int )cachep___17->limit);
      }
      if (! new_alien) {
        goto fail;
      }
    }
    {
    new_shared = (struct array_cache *)((void *)0);
    }
    if (cachep___17->shared) {
      {
      new_shared = alloc_arraycache(node___3, (int )(cachep___17->shared * cachep___17->batchcount),
                                    -1163005939);
      }
      if (! new_shared) {
        {
        free_alien_cache(new_alien);
        }
        goto fail;
      }
    }
    {
    l3___4 = cachep___17->nodelists[node___3];
    }
    if (l3___4) {
      {
      shared = l3___4->shared;
      _spin_lock_irq(& l3___4->list_lock);
      }
      if (shared) {
        {
        free_block((struct kmem_cache *)cachep___17, shared->entry, (int )shared->avail,
                   node___3);
        }
      }
      {
      l3___4->shared = new_shared;
      }
      if (! l3___4->alien) {
        {
        l3___4->alien = new_alien;
        new_alien = (struct array_cache **)((void *)0);
        }
      }
      {
      tmp___41 = _node_to_cpumask_ptr(node___3);
      __tmp__ = tmp___41;
      tmp___42 = __cpus_weight(__tmp__, 64);
      l3___4->free_limit = (unsigned int )(1 + tmp___42) * cachep___17->batchcount + cachep___17->num;
      }
      {
      while (1) {
        while_144_continue: /* CIL Label */ ;
        {
        __raw_spin_unlock(& l3___4->list_lock.raw_lock);
        }
        {
        while (1) {
          while_145_continue: /* CIL Label */ ;
          {
          while (1) {
            while_146_continue: /* CIL Label */ ;
            goto while_146_break;
          }
          while_146_break: /* CIL Label */ ;
          }
          {
          raw_local_irq_enable();
          }
          goto while_145_break;
        }
        while_145_break: /* CIL Label */ ;
        }
        goto while_144_break;
      }
      while_144_break: /* CIL Label */ ;
      }
      {
      kfree((void const   *)shared);
      free_alien_cache(new_alien);
      }
      goto __Cont___0;
    }
    {
    tmp___43 = kmalloc_node(sizeof(struct kmem_list3 ), 208U, node___3);
    l3___4 = (struct kmem_list3 *)tmp___43;
    }
    if (! l3___4) {
      {
      free_alien_cache(new_alien);
      kfree((void const   *)new_shared);
      }
      goto fail;
    }
    {
    kmem_list3_init(l3___4);
    l3___4->next_reap = (unsigned long )((jiffies + (unsigned long volatile   )1000) + (unsigned long volatile   )((unsigned long )cachep___17 % 1000UL));
    l3___4->shared = new_shared;
    l3___4->alien = new_alien;
    tmp___44 = _node_to_cpumask_ptr(node___3);
    __tmp_____0 = tmp___44;
    tmp___45 = __cpus_weight(__tmp_____0, 64);
    l3___4->free_limit = (unsigned int )(1 + tmp___45) * cachep___17->batchcount + cachep___17->num;
    cachep___17->nodelists[node___3] = l3___4;
    }
    __Cont___0: /* CIL Label */ 
    {
    node___3 = 1;
    }
  }
  while_143_break: /* CIL Label */ ;
  }
  return (0);
  fail: 
  if (! cachep___17->next.next) {
    {
    node___3 --;
    }
    {
    while (1) {
      while_147_continue: /* CIL Label */ ;
      if (! (node___3 >= 0)) {
        goto while_147_break;
      }
      if (cachep___17->nodelists[node___3]) {
        {
        l3___4 = cachep___17->nodelists[node___3];
        kfree((void const   *)l3___4->shared);
        free_alien_cache(l3___4->alien);
        kfree((void const   *)l3___4);
        cachep___17->nodelists[node___3] = (struct kmem_list3 *)((void *)0);
        }
      }
      {
      node___3 --;
      }
    }
    while_147_break: /* CIL Label */ ;
    }
  }
  return (-12);
}
}
static void do_ccupdate_local(void *info ) 
{ struct ccupdate_struct *new ;
  struct array_cache *old ;
  int ret__ ;
  int ret_____0 ;
  int ret_____1 ;

  {
  {
  new = (struct ccupdate_struct *)info;
  }
  {
  while (1) {
    while_148_continue: /* CIL Label */ ;
    goto while_148_break;
  }
  while_148_break: /* CIL Label */ ;
  }
  {
  old = cpu_cache_get(new->cachep);
  }
  if ((int )sizeof(per_cpu__cpu_number) == 1) {
    goto switch_149_1;
  } else {
    if ((int )sizeof(per_cpu__cpu_number) == 2) {
      goto switch_149_2;
    } else {
      if ((int )sizeof(per_cpu__cpu_number) == 4) {
        goto switch_149_4;
      } else {
        {
        goto switch_149_default;
        if (0) {
          switch_149_1: /* CIL Label */ 
          {
          __asm__  ("mov"
                    "b "
                    "%%fs:"
                    "%1,%0": "=r" (ret__): "m" (per_cpu__cpu_number));
          }
          goto switch_149_break;
          switch_149_2: /* CIL Label */ 
          {
          __asm__  ("mov"
                    "w "
                    "%%fs:"
                    "%1,%0": "=r" (ret__): "m" (per_cpu__cpu_number));
          }
          goto switch_149_break;
          switch_149_4: /* CIL Label */ 
          {
          __asm__  ("mov"
                    "l "
                    "%%fs:"
                    "%1,%0": "=r" (ret__): "m" (per_cpu__cpu_number));
          }
          goto switch_149_break;
          switch_149_default: /* CIL Label */ 
          {
          __bad_percpu_size();
          }
        } else {
          switch_149_break: /* CIL Label */ ;
        }
        }
      }
    }
  }
  if ((int )sizeof(per_cpu__cpu_number) == 1) {
    goto switch_150_1;
  } else {
    if ((int )sizeof(per_cpu__cpu_number) == 2) {
      goto switch_150_2;
    } else {
      if ((int )sizeof(per_cpu__cpu_number) == 4) {
        goto switch_150_4;
      } else {
        {
        goto switch_150_default;
        if (0) {
          switch_150_1: /* CIL Label */ 
          {
          __asm__  ("mov"
                    "b "
                    "%%fs:"
                    "%1,%0": "=r" (ret_____0): "m" (per_cpu__cpu_number));
          }
          goto switch_150_break;
          switch_150_2: /* CIL Label */ 
          {
          __asm__  ("mov"
                    "w "
                    "%%fs:"
                    "%1,%0": "=r" (ret_____0): "m" (per_cpu__cpu_number));
          }
          goto switch_150_break;
          switch_150_4: /* CIL Label */ 
          {
          __asm__  ("mov"
                    "l "
                    "%%fs:"
                    "%1,%0": "=r" (ret_____0): "m" (per_cpu__cpu_number));
          }
          goto switch_150_break;
          switch_150_default: /* CIL Label */ 
          {
          __bad_percpu_size();
          }
        } else {
          switch_150_break: /* CIL Label */ ;
        }
        }
      }
    }
  }
  {
  (new->cachep)->array[ret__] = new->new[ret_____0];
  }
  if ((int )sizeof(per_cpu__cpu_number) == 1) {
    goto switch_151_1;
  } else {
    if ((int )sizeof(per_cpu__cpu_number) == 2) {
      goto switch_151_2;
    } else {
      if ((int )sizeof(per_cpu__cpu_number) == 4) {
        goto switch_151_4;
      } else {
        {
        goto switch_151_default;
        if (0) {
          switch_151_1: /* CIL Label */ 
          {
          __asm__  ("mov"
                    "b "
                    "%%fs:"
                    "%1,%0": "=r" (ret_____1): "m" (per_cpu__cpu_number));
          }
          goto switch_151_break;
          switch_151_2: /* CIL Label */ 
          {
          __asm__  ("mov"
                    "w "
                    "%%fs:"
                    "%1,%0": "=r" (ret_____1): "m" (per_cpu__cpu_number));
          }
          goto switch_151_break;
          switch_151_4: /* CIL Label */ 
          {
          __asm__  ("mov"
                    "l "
                    "%%fs:"
                    "%1,%0": "=r" (ret_____1): "m" (per_cpu__cpu_number));
          }
          goto switch_151_break;
          switch_151_default: /* CIL Label */ 
          {
          __bad_percpu_size();
          }
        } else {
          switch_151_break: /* CIL Label */ ;
        }
        }
      }
    }
  }
  {
  new->new[ret_____1] = old;
  }
  return;
}
}
static int do_tune_cpucache(struct kmem_cache___0 *cachep___18 , int limit , int batchcount___1 ,
                            int shared___0 ) 
{ struct ccupdate_struct *new___0 ;
  int i___1 ;
  void *tmp___46 ;
  struct array_cache *ccold ;
  int tmp___47 ;

  {
  {
  tmp___46 = kzalloc(sizeof(*new___0), 208U);
  new___0 = (struct ccupdate_struct *)tmp___46;
  }
  if (! new___0) {
    return (-12);
  }
  {
  i___1 = -1;
  }
  {
  while (1) {
    while_152_continue: /* CIL Label */ ;
    {
    i___1 = __next_cpu(i___1, (cpumask_t const   *)(& cpu_online_map));
    }
    if (! (i___1 < 64)) {
      goto while_152_break;
    }
    {
    new___0->new[i___1] = alloc_arraycache(0, limit, batchcount___1);
    }
    if (! new___0->new[i___1]) {
      {
      i___1 --;
      }
      {
      while (1) {
        while_153_continue: /* CIL Label */ ;
        if (! (i___1 >= 0)) {
          goto while_153_break;
        }
        {
        kfree((void const   *)new___0->new[i___1]);
        i___1 --;
        }
      }
      while_153_break: /* CIL Label */ ;
      }
      {
      kfree((void const   *)new___0);
      }
      return (-12);
    }
  }
  while_152_break: /* CIL Label */ ;
  }
  {
  new___0->cachep = cachep___18;
  on_each_cpu(& do_ccupdate_local, (void *)new___0, 1);
  }
  {
  while (1) {
    while_154_continue: /* CIL Label */ ;
    goto while_154_break;
  }
  while_154_break: /* CIL Label */ ;
  }
  {
  cachep___18->batchcount = (unsigned int )batchcount___1;
  cachep___18->limit = (unsigned int )limit;
  cachep___18->shared = (unsigned int )shared___0;
  i___1 = -1;
  }
  {
  while (1) {
    while_155_continue: /* CIL Label */ ;
    {
    i___1 = __next_cpu(i___1, (cpumask_t const   *)(& cpu_online_map));
    }
    if (! (i___1 < 64)) {
      goto while_155_break;
    }
    {
    ccold = new___0->new[i___1];
    }
    if (! ccold) {
      goto __Cont___1;
    }
    {
    _spin_lock_irq(& (cachep___18->nodelists[0])->list_lock);
    free_block((struct kmem_cache *)cachep___18, ccold->entry, (int )ccold->avail,
               0);
    }
    {
    while (1) {
      while_156_continue: /* CIL Label */ ;
      {
      __raw_spin_unlock(& (cachep___18->nodelists[0])->list_lock.raw_lock);
      }
      {
      while (1) {
        while_157_continue: /* CIL Label */ ;
        {
        while (1) {
          while_158_continue: /* CIL Label */ ;
          goto while_158_break;
        }
        while_158_break: /* CIL Label */ ;
        }
        {
        raw_local_irq_enable();
        }
        goto while_157_break;
      }
      while_157_break: /* CIL Label */ ;
      }
      goto while_156_break;
    }
    while_156_break: /* CIL Label */ ;
    }
    {
    kfree((void const   *)ccold);
    }
    __Cont___1: /* CIL Label */ 
    {

    }
  }
  while_155_break: /* CIL Label */ ;
  }
  {
  kfree((void const   *)new___0);
  tmp___47 = alloc_kmemlist(cachep___18);
  }
  return (tmp___47);
}
}
__asm__("error in function enable_cpucache");
static void drain_array(struct kmem_cache___0 *cachep___19 , struct kmem_list3 *l3___5 ,
                        struct array_cache *ac___4 , int force , int node___4 ) 
{ int tofree ;

  {
  if (! ac___4) {
    return;
  } else {
    if (! ac___4->avail) {
      return;
    }
  }
  if (ac___4->touched) {
    if (! force) {
      {
      ac___4->touched = 0U;
      }
    } else {
      goto _L;
    }
  } else {
    _L: /* CIL Label */ 
    {
    _spin_lock_irq(& l3___5->list_lock);
    }
    if (ac___4->avail) {
      if (force) {
        {
        tofree = (int )ac___4->avail;
        }
      } else {
        {
        tofree = (int )((ac___4->limit + 4U) / 5U);
        }
      }
      if ((unsigned int )tofree > ac___4->avail) {
        {
        tofree = (int )((ac___4->avail + 1U) / 2U);
        }
      }
      {
      free_block((struct kmem_cache *)cachep___19, ac___4->entry, tofree, node___4);
      ac___4->avail -= (unsigned int )tofree;
      memmove((void *)(ac___4->entry), (void const   *)(& ac___4->entry[tofree]),
              sizeof(void *) * ac___4->avail);
      }
    }
    {
    while (1) {
      while_159_continue: /* CIL Label */ ;
      {
      __raw_spin_unlock(& l3___5->list_lock.raw_lock);
      }
      {
      while (1) {
        while_160_continue: /* CIL Label */ ;
        {
        while (1) {
          while_161_continue: /* CIL Label */ ;
          goto while_161_break;
        }
        while_161_break: /* CIL Label */ ;
        }
        {
        raw_local_irq_enable();
        }
        goto while_160_break;
      }
      while_160_break: /* CIL Label */ ;
      }
      goto while_159_break;
    }
    while_159_break: /* CIL Label */ ;
    }
  }
  return;
}
}
static void cache_reap(struct work_struct *w ) 
{ struct kmem_cache___0 *searchp ;
  struct kmem_list3 *l3___6 ;
  int node___5 ;
  struct delayed_work *work ;
  struct work_struct  const  *__mptr___0 ;
  int tmp___48 ;
  struct list_head  const  *__mptr___1 ;
  struct list_head  const  *__mptr___2 ;
  struct array_cache *tmp___49 ;
  int freed ;
  unsigned long tmp___50 ;

  {
  {
  node___5 = 0;
  __mptr___0 = (struct work_struct  const  *)w;
  work = (struct delayed_work *)((char *)__mptr___0 - (unsigned int )(& ((struct delayed_work *)0)->work));
  tmp___48 = mutex_trylock(& cache_chain_mutex);
  }
  if (! tmp___48) {
    goto out;
  }
  {
  __mptr___1 = (struct list_head  const  *)cache_chain.next;
  searchp = (struct kmem_cache___0 *)((char *)__mptr___1 - (unsigned int )(& ((struct kmem_cache___0 *)0)->next));
  }
  {
  while (1) {
    while_162_continue: /* CIL Label */ ;
    {
    prefetch((void const   *)searchp->next.next);
    }
    if (! ((unsigned int )(& searchp->next) != (unsigned int )(& cache_chain))) {
      goto while_162_break;
    }
    {
    while (1) {
      while_163_continue: /* CIL Label */ ;
      goto while_163_break;
    }
    while_163_break: /* CIL Label */ ;
    }
    {
    l3___6 = searchp->nodelists[node___5];
    }
    {
    while (1) {
      while_164_continue: /* CIL Label */ ;
      goto while_164_break;
    }
    while_164_break: /* CIL Label */ ;
    }
    {
    tmp___49 = cpu_cache_get(searchp);
    drain_array(searchp, l3___6, tmp___49, 0, node___5);
    }
    if ((long )jiffies - (long )l3___6->next_reap < 0L) {
      goto next;
    }
    {
    l3___6->next_reap = (unsigned long )(jiffies + (unsigned long volatile   )1000);
    drain_array(searchp, l3___6, l3___6->shared, 0, node___5);
    }
    if (l3___6->free_touched) {
      {
      l3___6->free_touched = 0;
      }
    } else {
      {
      freed = drain_freelist((struct kmem_cache *)searchp, l3___6, (int )(((l3___6->free_limit + 5U * searchp->num) - 1U) / (5U * searchp->num)));
      }
      {
      while (1) {
        while_165_continue: /* CIL Label */ ;
        goto while_165_break;
      }
      while_165_break: /* CIL Label */ ;
      }
    }
    next: 
    {
    cond_resched();
    __mptr___2 = (struct list_head  const  *)searchp->next.next;
    searchp = (struct kmem_cache___0 *)((char *)__mptr___2 - (unsigned int )(& ((struct kmem_cache___0 *)0)->next));
    }
  }
  while_162_break: /* CIL Label */ ;
  }
  {
  while (1) {
    while_166_continue: /* CIL Label */ ;
    goto while_166_break;
  }
  while_166_break: /* CIL Label */ ;
  }
  {
  mutex_unlock(& cache_chain_mutex);
  }
  {
  while (1) {
    while_167_continue: /* CIL Label */ ;
    goto while_167_break;
  }
  while_167_break: /* CIL Label */ ;
  }
  out: 
  {
  tmp___50 = round_jiffies_relative(500UL);
  schedule_delayed_work(work, tmp___50);
  }
  return;
}
}
static void print_slabinfo_header(struct seq_file *m ) 
{ 

  {
  {
  seq_puts(m, "slabinfo - version: 2.1\n");
  seq_puts(m, "# name            <active_objs> <num_objs> <objsize> <objperslab> <pagesperslab>");
  seq_puts(m, " : tunables <limit> <batchcount> <sharedfactor>");
  seq_puts(m, " : slabdata <active_slabs> <num_slabs> <sharedavail>");
  seq_putc(m, (char )'\n');
  }
  return;
}
}
static void *s_start(struct seq_file *m___0 , loff_t *pos ) 
{ loff_t n ;
  struct list_head *tmp___51 ;

  {
  {
  n = *pos;
  mutex_lock(& cache_chain_mutex);
  }
  if (! n) {
    {
    print_slabinfo_header(m___0);
    }
  }
  {
  tmp___51 = seq_list_start(& cache_chain, *pos);
  }
  return ((void *)tmp___51);
}
}
static void *s_next(struct seq_file *m___1 , void *p , loff_t *pos___0 ) 
{ struct list_head *tmp___52 ;

  {
  {
  tmp___52 = seq_list_next(p, & cache_chain, pos___0);
  }
  return ((void *)tmp___52);
}
}
static void s_stop(struct seq_file *m___2 , void *p___0 ) 
{ 

  {
  {
  mutex_unlock(& cache_chain_mutex);
  }
  return;
}
}
static int s_show(struct seq_file *m___3 , void *p___1 ) 
{ struct kmem_cache___0 *cachep___20 ;
  struct list_head  const  *__mptr___3 ;
  struct slab *slabp___6 ;
  unsigned long active_objs ;
  unsigned long num_objs ;
  unsigned long active_slabs ;
  unsigned long num_slabs ;
  unsigned long free_objects ;
  unsigned long shared_avail ;
  char const   *name ;
  char *error ;
  int node___6 ;
  struct kmem_list3 *l3___7 ;
  struct list_head  const  *__mptr___4 ;
  struct list_head  const  *__mptr___5 ;
  struct list_head  const  *__mptr___6 ;
  struct list_head  const  *__mptr___7 ;
  struct list_head  const  *__mptr___8 ;
  struct list_head  const  *__mptr___9 ;

  {
  {
  __mptr___3 = (struct list_head  const  *)p___1;
  cachep___20 = (struct kmem_cache___0 *)((char *)__mptr___3 - (unsigned int )(& ((struct kmem_cache___0 *)0)->next));
  active_slabs = 0UL;
  free_objects = 0UL;
  shared_avail = 0UL;
  error = (char *)((void *)0);
  active_objs = 0UL;
  num_slabs = 0UL;
  node___6 = 0;
  }
  {
  while (1) {
    while_168_continue: /* CIL Label */ ;
    if (! (node___6 == 0)) {
      goto while_168_break;
    }
    {
    l3___7 = cachep___20->nodelists[node___6];
    }
    if (! l3___7) {
      goto __Cont___2;
    }
    {
    while (1) {
      while_169_continue: /* CIL Label */ ;
      goto while_169_break;
    }
    while_169_break: /* CIL Label */ ;
    }
    {
    _spin_lock_irq(& l3___7->list_lock);
    __mptr___4 = (struct list_head  const  *)l3___7->slabs_full.next;
    slabp___6 = (struct slab *)((char *)__mptr___4 - (unsigned int )(& ((struct slab *)0)->list));
    }
    {
    while (1) {
      while_170_continue: /* CIL Label */ ;
      {
      prefetch((void const   *)slabp___6->list.next);
      }
      if (! ((unsigned int )(& slabp___6->list) != (unsigned int )(& l3___7->slabs_full))) {
        goto while_170_break;
      }
      if (slabp___6->inuse != cachep___20->num) {
        if (! error) {
          {
          error = (char *)"slabs_full accounting error";
          }
        }
      }
      {
      active_objs += (unsigned long )cachep___20->num;
      active_slabs ++;
      __mptr___5 = (struct list_head  const  *)slabp___6->list.next;
      slabp___6 = (struct slab *)((char *)__mptr___5 - (unsigned int )(& ((struct slab *)0)->list));
      }
    }
    while_170_break: /* CIL Label */ ;
    }
    {
    __mptr___6 = (struct list_head  const  *)l3___7->slabs_partial.next;
    slabp___6 = (struct slab *)((char *)__mptr___6 - (unsigned int )(& ((struct slab *)0)->list));
    }
    {
    while (1) {
      while_171_continue: /* CIL Label */ ;
      {
      prefetch((void const   *)slabp___6->list.next);
      }
      if (! ((unsigned int )(& slabp___6->list) != (unsigned int )(& l3___7->slabs_partial))) {
        goto while_171_break;
      }
      if (slabp___6->inuse == cachep___20->num) {
        if (! error) {
          {
          error = (char *)"slabs_partial inuse accounting error";
          }
        }
      }
      if (! slabp___6->inuse) {
        if (! error) {
          {
          error = (char *)"slabs_partial/inuse accounting error";
          }
        }
      }
      {
      active_objs += (unsigned long )slabp___6->inuse;
      active_slabs ++;
      __mptr___7 = (struct list_head  const  *)slabp___6->list.next;
      slabp___6 = (struct slab *)((char *)__mptr___7 - (unsigned int )(& ((struct slab *)0)->list));
      }
    }
    while_171_break: /* CIL Label */ ;
    }
    {
    __mptr___8 = (struct list_head  const  *)l3___7->slabs_free.next;
    slabp___6 = (struct slab *)((char *)__mptr___8 - (unsigned int )(& ((struct slab *)0)->list));
    }
    {
    while (1) {
      while_172_continue: /* CIL Label */ ;
      {
      prefetch((void const   *)slabp___6->list.next);
      }
      if (! ((unsigned int )(& slabp___6->list) != (unsigned int )(& l3___7->slabs_free))) {
        goto while_172_break;
      }
      if (slabp___6->inuse) {
        if (! error) {
          {
          error = (char *)"slabs_free/inuse accounting error";
          }
        }
      }
      {
      num_slabs ++;
      __mptr___9 = (struct list_head  const  *)slabp___6->list.next;
      slabp___6 = (struct slab *)((char *)__mptr___9 - (unsigned int )(& ((struct slab *)0)->list));
      }
    }
    while_172_break: /* CIL Label */ ;
    }
    {
    free_objects += l3___7->free_objects;
    }
    if (l3___7->shared) {
      {
      shared_avail += (unsigned long )(l3___7->shared)->avail;
      }
    }
    {
    while (1) {
      while_173_continue: /* CIL Label */ ;
      {
      __raw_spin_unlock(& l3___7->list_lock.raw_lock);
      }
      {
      while (1) {
        while_174_continue: /* CIL Label */ ;
        {
        while (1) {
          while_175_continue: /* CIL Label */ ;
          goto while_175_break;
        }
        while_175_break: /* CIL Label */ ;
        }
        {
        raw_local_irq_enable();
        }
        goto while_174_break;
      }
      while_174_break: /* CIL Label */ ;
      }
      goto while_173_break;
    }
    while_173_break: /* CIL Label */ ;
    }
    __Cont___2: /* CIL Label */ 
    {
    node___6 = 1;
    }
  }
  while_168_break: /* CIL Label */ ;
  }
  {
  num_slabs += active_slabs;
  num_objs = num_slabs * (unsigned long )cachep___20->num;
  }
  if (num_objs - active_objs != free_objects) {
    if (! error) {
      {
      error = (char *)"free_objects accounting error";
      }
    }
  }
  {
  name = cachep___20->name;
  }
  if (error) {
    {
    printk("<3>slab: cache %s error: %s\n", name, error);
    }
  }
  {
  seq_printf(m___3, "%-17s %6lu %6lu %6u %4u %4d", name, active_objs, num_objs, cachep___20->buffer_size,
             cachep___20->num, 1 << cachep___20->gfporder);
  seq_printf(m___3, " : tunables %4u %4u %4u", cachep___20->limit, cachep___20->batchcount,
             cachep___20->shared);
  seq_printf(m___3, " : slabdata %6lu %6lu %6lu", active_slabs, num_slabs, shared_avail);
  seq_putc(m___3, (char )'\n');
  }
  return (0);
}
}
struct seq_operations  const  slabinfo_op  =    {& s_start, & s_stop, & s_next, & s_show};
ssize_t slabinfo_write(struct file *file , char const   *buffer , size_t count , loff_t *ppos ) 
{ char kbuf[129] ;
  char *tmp___53 ;
  int limit___0 ;
  int batchcount___2 ;
  int shared___1 ;
  int res ;
  struct kmem_cache___0 *cachep___21 ;
  unsigned long tmp___54 ;
  int tmp___55 ;
  struct list_head  const  *__mptr___10 ;
  struct list_head  const  *__mptr___11 ;
  int tmp___56 ;

  {
  if (count > 128U) {
    return (-22);
  }
  {
  tmp___54 = copy_from_user((void *)(& kbuf), (void const   *)buffer, (unsigned long )count);
  }
  if (tmp___54) {
    return (-14);
  }
  {
  kbuf[128] = (char )'\000';
  tmp___53 = strchr((char const   *)(kbuf), ' ');
  }
  if (! tmp___53) {
    return (-22);
  }
  {
  *tmp___53 = (char )'\000';
  tmp___53 ++;
  tmp___55 = sscanf((char const   *)tmp___53, " %d %d %d", & limit___0, & batchcount___2,
                    & shared___1);
  }
  if (tmp___55 != 3) {
    return (-22);
  }
  {
  mutex_lock(& cache_chain_mutex);
  res = -22;
  __mptr___10 = (struct list_head  const  *)cache_chain.next;
  cachep___21 = (struct kmem_cache___0 *)((char *)__mptr___10 - (unsigned int )(& ((struct kmem_cache___0 *)0)->next));
  }
  {
  while (1) {
    while_176_continue: /* CIL Label */ ;
    {
    prefetch((void const   *)cachep___21->next.next);
    }
    if (! ((unsigned int )(& cachep___21->next) != (unsigned int )(& cache_chain))) {
      goto while_176_break;
    }
    {
    tmp___56 = strcmp(cachep___21->name, (char const   *)(kbuf));
    }
    if (! tmp___56) {
      if (limit___0 < 1) {
        {
        res = 0;
        }
      } else {
        if (batchcount___2 < 1) {
          {
          res = 0;
          }
        } else {
          if (batchcount___2 > limit___0) {
            {
            res = 0;
            }
          } else {
            if (shared___1 < 0) {
              {
              res = 0;
              }
            } else {
              {
              res = do_tune_cpucache(cachep___21, limit___0, batchcount___2, shared___1);
              }
            }
          }
        }
      }
      goto while_176_break;
    }
    {
    __mptr___11 = (struct list_head  const  *)cachep___21->next.next;
    cachep___21 = (struct kmem_cache___0 *)((char *)__mptr___11 - (unsigned int )(& ((struct kmem_cache___0 *)0)->next));
    }
  }
  while_176_break: /* CIL Label */ ;
  }
  {
  mutex_unlock(& cache_chain_mutex);
  }
  if (res >= 0) {
    {
    res = (int )count;
    }
  }
  return (res);
}
}
size_t ksize(void const   *objp___8 ) 
{ long tmp___57 ;
  long tmp___58 ;
  struct kmem_cache___0 *tmp___59 ;

  {
  {
  while (1) {
    while_177_continue: /* CIL Label */ ;
    {
    tmp___57 = __builtin_expect((long )(! (! (! objp___8))), 0L);
    }
    if (tmp___57) {
      {
      while (1) {
        while_178_continue: /* CIL Label */ ;
        {
        __asm__  volatile   ("1:\tud2\n"
                             ".pushsection __bug_table,\"a\"\n"
                             "2:\t.long 1b, %c0\n"
                             "\t.word %c1, 0\n"
                             "\t.org 2b+%c2\n"
                             ".popsection": : "i" ("slab.c"), "i" (4469), "i" (sizeof(struct bug_entry )));
        }
        {
        while (1) {
          while_179_continue: /* CIL Label */ ;
        }
        while_179_break: /* CIL Label */ ;
        }
        goto while_178_break;
      }
      while_178_break: /* CIL Label */ ;
      }
    }
    goto while_177_break;
  }
  while_177_break: /* CIL Label */ ;
  }
  {
  tmp___58 = __builtin_expect((long )(! (! ((unsigned int )objp___8 == (unsigned int )((void *)16)))),
                              0L);
  }
  if (tmp___58) {
    return (0U);
  }
  {
  tmp___59 = virt_to_cache(objp___8);
  }
  return (tmp___59->buffer_size);
}
}
